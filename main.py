"""
Deep Agents Code Analysis - Main Entry Point

LangChain Deep Agents ê¸°ë°˜ Git ì½”ë“œ ë¶„ì„ ì‹œìŠ¤í…œ
"""

import asyncio
import logging
import sys
from pathlib import Path
from argparse import ArgumentParser
from dotenv import load_dotenv
import os
import uuid

from langchain_aws import ChatBedrockConverse

from core.orchestrator.orchestrator import DeepAgentOrchestrator
from core.state import AgentState
from agents.repo_synthesizer import RepoSynthesizerAgent, RepoSynthesizerContext
from shared.storage import ResultStore

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("logs/deep_agents.log", encoding="utf-8"),
    ],
)

logger = logging.getLogger(__name__)


def load_environment():
    """
    í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ê²€ì¦
    """
    load_dotenv()

    # TOKENIZERS_PARALLELISM ì„¤ì • (fork ê²½ê³  ë°©ì§€)
    os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
    logger.debug(f"ğŸ”§ TOKENIZERS_PARALLELISM={os.getenv('TOKENIZERS_PARALLELISM')}")

    required_vars = [
        "AWS_BEDROCK_MODEL_ID_SONNET",
        "AWS_BEDROCK_MODEL_ID_HAIKU",
    ]

    missing_vars = [var for var in required_vars if not os.getenv(var)]

    if missing_vars:
        logger.error(f"âŒ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing_vars)}")
        logger.error("   .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš” (.env.example ì°¸ê³ )")
        sys.exit(1)

    logger.info("âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")


def create_llms() -> tuple[ChatBedrockConverse, ChatBedrockConverse]:
    """
    AWS Bedrock LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    Returns:
        (sonnet_llm, haiku_llm)
    """
    # Bedrockì€ us-east-1 ë¦¬ì „ ì‚¬ìš© (ëª¨ë¸ ì§€ì›ì´ ê°€ì¥ ë§ìŒ)
    bedrock_region = os.getenv("AWS_BEDROCK_REGION", "us-east-1")
    sonnet_model_id = os.getenv("AWS_BEDROCK_MODEL_ID_SONNET")
    haiku_model_id = os.getenv("AWS_BEDROCK_MODEL_ID_HAIKU")

    logger.info(f"ğŸ¤– LLM ì´ˆê¸°í™”")
    logger.info(f"   Bedrock Region: {bedrock_region}")
    logger.info(f"   Sonnet: {sonnet_model_id}")
    logger.info(f"   Haiku: {haiku_model_id}")

    sonnet_llm = ChatBedrockConverse(
        model=sonnet_model_id,
        region_name=bedrock_region,
        temperature=0.0,
        max_tokens=4096,
        # timeout íŒŒë¼ë¯¸í„°ëŠ” Bedrock Converse APIì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
    )

    haiku_llm = ChatBedrockConverse(
        model=haiku_model_id,
        region_name=bedrock_region,
        temperature=0.0,
        max_tokens=4096,
        # timeout íŒŒë¼ë¯¸í„°ëŠ” Bedrock Converse APIì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
    )

    return sonnet_llm, haiku_llm


async def analyze_multiple_repos(
    orchestrator: DeepAgentOrchestrator,
    git_urls: list[str],
    target_user: str | None,
    data_dir: Path,
) -> dict:
    """
    ì—¬ëŸ¬ ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ + ì¢…í•© (ì˜µì…˜ 1: ìµœìƒìœ„ ë ˆë²¨ ë°˜ë³µ)

    Args:
        orchestrator: DeepAgentOrchestrator ì¸ìŠ¤í„´ìŠ¤
        git_urls: Git ë ˆí¬ì§€í† ë¦¬ URL ë¦¬ìŠ¤íŠ¸
        target_user: íŠ¹ì • ìœ ì € ì´ë©”ì¼
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬

    Returns:
        ì¢…í•© ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("=" * 60)
    is_single = len(git_urls) == 1
    logger.info(f"ğŸš€ {'Single' if is_single else 'Multi'}-Repository Analysis")
    logger.info("=" * 60)
    logger.info(f"   ë ˆí¬ì§€í† ë¦¬ ìˆ˜: {len(git_urls)}ê°œ")
    logger.info(f"   Target User: {target_user if target_user else 'ì „ì²´ ìœ ì €'}")
    logger.info("")

    # ë©”ì¸ task UUID ìƒì„± (ì¢…í•© ê²°ê³¼ìš©)
    import uuid
    from shared.storage import create_storage_backend
    from shared.config import settings
    
    main_task_uuid = str(uuid.uuid4())
    
    # shared/storageë¥¼ í†µí•´ ë©”ì¸ ê²½ë¡œ ìƒì„±
    if settings.STORAGE_BACKEND.value == "local":
        main_base_path = data_dir / "analyze_multi" / main_task_uuid
        main_base_path.mkdir(parents=True, exist_ok=True)
    else:  # S3
        # S3 í™˜ê²½: ë¬¸ìì—´ ê²½ë¡œë§Œ ê´€ë¦¬
        main_base_path = f"analyze_multi/{main_task_uuid}"

    logger.info(f"ğŸ“‚ ì¢…í•© ê²°ê³¼ ê²½ë¡œ: {main_base_path}")
    logger.info("")

    # 1. ê° ë ˆí¬ì§€í† ë¦¬ ë³‘ë ¬ ë¶„ì„ (ê°ê° setup â†’ plan â†’ execute â†’ finalize)
    logger.info(f"ğŸ“¦ {len(git_urls)}ê°œ ë ˆí¬ì§€í† ë¦¬ ë³‘ë ¬ ë¶„ì„ ì‹œì‘...")
    logger.info("")

    # ë©€í‹° ë¶„ì„ ëª¨ë“œ: ê° ë ˆí¬ ê²°ê³¼ë¥¼ analyze_multi/{main_task_uuid}/repos/{repo_task_uuid}/ì— ì €ì¥
    repo_results = await asyncio.gather(
        *[
            orchestrator.run(
                git_url, 
                target_user,
                main_task_uuid=main_task_uuid,
                main_base_path=main_base_path
            ) 
            for git_url in git_urls
        ],
        return_exceptions=True
    )

    # ê²°ê³¼ ì •ë¦¬
    successful_results = []
    failed_results = []

    for i, result in enumerate(repo_results):
        git_url = git_urls[i]

        if isinstance(result, Exception):
            logger.error(f"âŒ {git_url}: {result}")
            failed_results.append({
                "git_url": git_url,
                "error_message": str(result),
            })
        else:
            if result.get("error_message"):
                logger.error(f"âŒ {git_url}: {result.get('error_message')}")
                failed_results.append({
                    "git_url": git_url,
                    "error_message": result.get("error_message"),
                })
            else:
                logger.info(f"âœ… {git_url}: ë¶„ì„ ì™„ë£Œ")
                successful_results.append(result)

    logger.info("")
    logger.info(f"ğŸ“Š ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ì™„ë£Œ: ì„±ê³µ {len(successful_results)}ê°œ, ì‹¤íŒ¨ {len(failed_results)}ê°œ")
    logger.info("")

    # 2. ì¢…í•© agent ì‹¤í–‰
    if successful_results:
        logger.info("ğŸ”¬ ì¢…í•© ë¶„ì„ ì‹œì‘...")

        synthesizer = RepoSynthesizerAgent()
        synthesis_context = RepoSynthesizerContext(
            task_uuid=main_task_uuid,
            main_task_uuid=main_task_uuid,
            main_base_path=str(main_base_path),
            repo_results=successful_results,
            target_user=target_user,
        )

        synthesis_response = await synthesizer.run(synthesis_context)

        logger.info("âœ… ì¢…í•© ë¶„ì„ ì™„ë£Œ")
        logger.info(f"   ì¢…í•© ë¦¬í¬íŠ¸: {synthesis_response.synthesis_report_path}")

        store = ResultStore(main_task_uuid, main_base_path)
        store.save_result("repo_synthesizer", synthesis_response)

        # ì¢…í•© ë¶„ì„ ê²°ê³¼ DB ì €ì¥
        if orchestrator.db_writer and orchestrator.user_id:
            try:
                from shared.graph_db import AnalysisStatus
                import uuid as uuid_module

                # ëŒ€í‘œ ë ˆí¬ì§€í† ë¦¬ URL (ì²« ë²ˆì§¸ ì„±ê³µí•œ ë ˆí¬)
                representative_url = (
                    successful_results[0].get("git_url") 
                    if successful_results and successful_results[0].get("git_url")
                    else git_urls[0]
                )

                await orchestrator.db_writer.save_final_analysis(
                    user_id=orchestrator.user_id,
                    repository_url=representative_url,
                    result=synthesis_response.model_dump(),  # RepoSynthesizerResponse
                    main_task_uuid=uuid_module.UUID(main_task_uuid),
                    status=AnalysisStatus.COMPLETED,
                    error_message=None
                )
                logger.info(f"ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ: {main_task_uuid}")
            except Exception as e:
                logger.warning(f"âš ï¸ ì¢…í•© ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: {e}")

        return {
            "main_task_uuid": main_task_uuid,
            "main_base_path": str(main_base_path),
            "total_repos": len(git_urls),
            "successful_repos": len(successful_results),
            "failed_repos": len(failed_results),
            "repo_results": successful_results,
            "failed_results": failed_results,
            "synthesis": synthesis_response.model_dump(),
        }
    else:
        logger.error("âŒ ë¶„ì„ ì„±ê³µí•œ ë ˆí¬ì§€í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "main_task_uuid": main_task_uuid,
            "main_base_path": str(main_base_path),
            "total_repos": len(git_urls),
            "successful_repos": 0,
            "failed_repos": len(failed_results),
            "failed_results": failed_results,
            "error_message": "ëª¨ë“  ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ì‹¤íŒ¨",
        }


async def main_async(args):
    """
    ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜
    """
    logger.info("=" * 60)
    logger.info("ğŸš€ Deep Agents Code Analysis")
    logger.info("=" * 60)

    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_environment()

    # LLM ìƒì„±
    sonnet_llm, haiku_llm = create_llms()

    # ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
    data_dir = Path(os.getenv("DATA_DIR", "./data"))
    data_dir.mkdir(parents=True, exist_ok=True)

    # Neo4j ì„¤ì • (Settingsë¥¼ í†µí•´ ë™ì  IP ì„¤ì • ì ìš©)
    from shared.config import settings
    neo4j_uri = os.getenv("NEO4J_URI") or settings.NEO4J_URI
    neo4j_user = os.getenv("NEO4J_USER", settings.NEO4J_USER)
    neo4j_password = os.getenv("NEO4J_PASSWORD", settings.NEO4J_PASSWORD)

    # Orchestrator ìƒì„±
    orchestrator = DeepAgentOrchestrator(
        sonnet_llm=sonnet_llm,
        haiku_llm=haiku_llm,
        data_dir=data_dir,
        neo4j_uri=neo4j_uri,
        neo4j_user=neo4j_user,
        neo4j_password=neo4j_password,
    )

    # ë‹¨ì¼/ë‹¤ì¤‘ ë ˆí¬ ì²˜ë¦¬ (ëª¨ë‘ ì¢…í•© ê²°ê³¼ ìƒì„±)
    git_urls = args.git_urls if hasattr(args, 'git_urls') and args.git_urls else [args.git_url]

    # ëª¨ë“  ê²½ìš°ì— analyze_multiple_repos ì‚¬ìš© (1ê°œë“  Nê°œë“  ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
    final_result = await analyze_multiple_repos(
        orchestrator=orchestrator,
        git_urls=git_urls,
        target_user=args.target_user,
        data_dir=data_dir,
    )

    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š {'Single' if len(git_urls) == 1 else 'Multi'}-Repository ë¶„ì„ ì™„ë£Œ")
    logger.info("=" * 60)

    if final_result.get("error_message"):
        logger.error(f"âŒ ì—ëŸ¬: {final_result['error_message']}")
        sys.exit(1)
    else:
        logger.info(f"âœ… ë©”ì¸ Task UUID: {final_result['main_task_uuid']}")
        logger.info(f"ğŸ“‚ ì¢…í•© ê²°ê³¼ ê²½ë¡œ: {final_result['main_base_path']}")
        logger.info(f"ğŸ“¦ ë ˆí¬ì§€í† ë¦¬: ì„±ê³µ {final_result['successful_repos']}ê°œ / ì‹¤íŒ¨ {final_result['failed_repos']}ê°œ")

        if final_result.get("synthesis"):
            synthesis = final_result["synthesis"]
            logger.info(f"ğŸ“Š ì´ ì»¤ë°‹: {synthesis.get('total_commits', 0):,}ê°œ")
            logger.info(f"ğŸ“Š ì´ íŒŒì¼: {synthesis.get('total_files', 0):,}ê°œ")
            logger.info(f"ğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸: {synthesis.get('synthesis_report_path')}")

    logger.info("=" * 60)


async def main_batch_mode():
    """
    AWS Batch ëª¨ë“œ ë©”ì¸ í•¨ìˆ˜

    í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì •ì„ ì½ì–´ ë‹¨ì¼/ë‹¤ì¤‘ ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ì‹¤í–‰
    - USER_ID: ì‚¬ìš©ì UUID (í•„ìˆ˜)
    - GIT_URLS: Git ë ˆí¬ì§€í† ë¦¬ URL (í•„ìˆ˜, ì‰¼í‘œ êµ¬ë¶„ìœ¼ë¡œ ë‹¤ì¤‘ ë ˆí¬ ì§€ì›)
      ì˜ˆ: "https://github.com/user/repo1" (ë‹¨ì¼)
      ì˜ˆ: "https://github.com/user/repo1,https://github.com/user/repo2" (ë‹¤ì¤‘)
    - TARGET_USER: íŠ¹ì • ìœ ì € ì´ë©”ì¼ (ì˜µì…”ë„)
    """
    logger.info("==" * 30)
    logger.info("ğŸš€ Deep Agents Batch Mode")
    logger.info("==" * 30)

    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_environment()

    # í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
    user_id_str = os.getenv("USER_ID")
    git_urls_str = os.getenv("GIT_URLS")

    if not user_id_str:
        logger.error("âŒ USER_ID í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        sys.exit(1)

    if not git_urls_str:
        logger.error("âŒ GIT_URLS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        sys.exit(1)

    # UUID ë³€í™˜
    try:
        user_id = uuid.UUID(user_id_str)
    except ValueError as e:
        logger.error(f"âŒ USER_ID í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {user_id_str}")
        sys.exit(1)

    # Git URLs íŒŒì‹± (ì‰¼í‘œë¡œ êµ¬ë¶„)
    git_urls = [url.strip() for url in git_urls_str.split(",") if url.strip()]

    if not git_urls:
        logger.error("âŒ GIT_URLSê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        sys.exit(1)

    # ì˜µì…”ë„ í™˜ê²½ ë³€ìˆ˜
    target_user = os.getenv("TARGET_USER")
    is_multi_repo = len(git_urls) > 1

    logger.info(f"ğŸ“‹ Batch ì„¤ì •:")
    logger.info(f"   USER_ID: {user_id}")
    logger.info(f"   ëª¨ë“œ: {'ë‹¤ì¤‘ ë ˆí¬ì§€í† ë¦¬' if is_multi_repo else 'ë‹¨ì¼ ë ˆí¬ì§€í† ë¦¬'}")
    logger.info(f"   ë ˆí¬ì§€í† ë¦¬ ìˆ˜: {len(git_urls)}ê°œ")
    for i, url in enumerate(git_urls, 1):
        logger.info(f"   [{i}] {url}")
    logger.info(f"   TARGET_USER: {target_user if target_user else 'ì „ì²´ ìœ ì €'}")
    logger.info("")

    # LLM ìƒì„±
    sonnet_llm, haiku_llm = create_llms()

    # ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
    data_dir = Path(os.getenv("DATA_DIR", "./data"))
    data_dir.mkdir(parents=True, exist_ok=True)

    # Neo4j ì„¤ì • (Settingsë¥¼ í†µí•´ ë™ì  IP ì„¤ì • ì ìš©)
    from shared.config import settings
    neo4j_uri = os.getenv("NEO4J_URI") or settings.NEO4J_URI
    neo4j_user = os.getenv("NEO4J_USER", settings.NEO4J_USER)
    neo4j_password = os.getenv("NEO4J_PASSWORD", settings.NEO4J_PASSWORD)

    # AnalysisDBWriter ì´ˆê¸°í™”
    logger.info("ğŸ”§ AnalysisDBWriter ì´ˆê¸°í™” ì¤‘...")
    try:
        from shared.graph_db import AnalysisDBWriter

        db_writer = await AnalysisDBWriter.initialize(
            echo=False,
            create_tables=False  # í”„ë¡œë•ì…˜ì—ì„œëŠ” í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬
        )
        logger.info("âœ… DB Writer ì´ˆê¸°í™” ì™„ë£Œ\n")
    except Exception as e:
        logger.error(f"âŒ DB Writer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # Orchestrator ìƒì„± (DB Writer í¬í•¨)
    orchestrator = DeepAgentOrchestrator(
        sonnet_llm=sonnet_llm,
        haiku_llm=haiku_llm,
        data_dir=data_dir,
        neo4j_uri=neo4j_uri,
        neo4j_user=neo4j_user,
        neo4j_password=neo4j_password,
        user_id=user_id,
        db_writer=db_writer,
    )

    # ë‹¨ì¼/ë‹¤ì¤‘ ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ì‹¤í–‰ (ëª¨ë‘ analyze_multiple_reposë¡œ í†µí•©)
    try:
        logger.info(f"ğŸš€ ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ì‹œì‘: {len(git_urls)}ê°œ")
        final_result = await analyze_multiple_repos(
            orchestrator=orchestrator,
            git_urls=git_urls,
            target_user=target_user,
            data_dir=data_dir,
        )

        # ê²°ê³¼ ì¶œë ¥
        logger.info("==" * 30)
        logger.info("ğŸ“Š Batch ë¶„ì„ ì™„ë£Œ")
        logger.info("==" * 30)

        if final_result.get("error_message"):
            logger.error(f"âŒ ì—ëŸ¬: {final_result['error_message']}")
            sys.exit(1)
        else:
            # í†µí•©ëœ ê²°ê³¼ ì¶œë ¥ (ë‹¨ì¼/ë‹¤ì¤‘ ëª¨ë‘ ë™ì¼í•œ í˜•ì‹)
            logger.info(f"âœ… Main Task UUID: {final_result.get('main_task_uuid')}")
            logger.info(f"ğŸ“‚ Main Base Path: {final_result.get('main_base_path')}")
            logger.info(f"ğŸ“¦ ì„±ê³µ: {final_result.get('successful_repos', 0)}ê°œ / ì‹¤íŒ¨: {final_result.get('failed_repos', 0)}ê°œ")
            if final_result.get("synthesis"):
                synthesis = final_result["synthesis"]
                logger.info(f"ğŸ“Š ì´ ì»¤ë°‹: {synthesis.get('total_commits', 0):,}ê°œ")
                logger.info(f"ğŸ“Š ì´ íŒŒì¼: {synthesis.get('total_files', 0):,}ê°œ")
            logger.info("==" * 30)

            # ì„±ê³µ ì™„ë£Œ ì‹œ ëª…ì‹œì  ì¢…ë£Œ
            logger.info("âœ… Batch ì‘ì—… ì •ìƒ ì™„ë£Œ")
            sys.exit(0)

    except Exception as e:
        logger.exception(f"âŒ Batch ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        sys.exit(1)

    finally:
        # DB Writer ì¢…ë£Œ
        await AnalysisDBWriter.close()


def main():
    """
    ë™ê¸° ë©”ì¸ í•¨ìˆ˜ (CLI ì§„ì…ì )
    """
    parser = ArgumentParser(description="Deep Agents Code Analysis")

    # Batch ëª¨ë“œ í”Œë˜ê·¸ ì¶”ê°€
    parser.add_argument(
        "--batch-mode",
        action="store_true",
        help="AWS Batch ëª¨ë“œë¡œ ì‹¤í–‰ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°)",
    )

    # ë‹¨ì¼ ë ˆí¬ ë˜ëŠ” ë‹¤ì¤‘ ë ˆí¬ ì§€ì› (ìƒí˜¸ ë°°íƒ€ì , batch-modeê°€ ì•„ë‹ ë•Œë§Œ í•„ìˆ˜)
    repo_group = parser.add_mutually_exclusive_group(required=False)
    repo_group.add_argument(
        "--git-url",
        type=str,
        help="ë¶„ì„í•  ë‹¨ì¼ Git ë ˆí¬ì§€í† ë¦¬ URL (ì˜ˆ: https://github.com/user/repo.git)",
    )
    repo_group.add_argument(
        "--git-urls",
        type=str,
        nargs="+",
        help="ë¶„ì„í•  ì—¬ëŸ¬ Git ë ˆí¬ì§€í† ë¦¬ URL (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„, ì˜ˆ: https://github.com/user/repo1.git https://github.com/user/repo2.git)",
    )

    parser.add_argument(
        "--target-user",
        type=str,
        default=None,
        help="ë¶„ì„ ëŒ€ìƒ íŠ¹ì • ìœ ì € ì´ë©”ì¼ (ì˜ˆ: user@example.com). ë¯¸ì§€ì • ì‹œ ì „ì²´ ìœ ì € ë¶„ì„",
    )

    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="ë¡œê·¸ ë ˆë²¨ ì„¤ì •",
    )

    args = parser.parse_args()

    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(getattr(logging, args.log_level))

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    Path("logs").mkdir(exist_ok=True)

    # Batch ëª¨ë“œ ë¶„ê¸°
    if args.batch_mode:
        logger.info("ğŸ”„ Batch ëª¨ë“œë¡œ ì‹¤í–‰")
        try:
            asyncio.run(main_batch_mode())
        except KeyboardInterrupt:
            logger.info("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            sys.exit(0)
        except Exception as e:
            logger.exception(f"âŒ Batch ëª¨ë“œ ì˜ˆì™¸ ë°œìƒ: {e}")
            sys.exit(1)
    else:
        # ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” git-url ë˜ëŠ” git-urlsê°€ í•„ìˆ˜
        if not args.git_url and not args.git_urls:
            parser.error("--git-url ë˜ëŠ” --git-urls ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤ (--batch-modeê°€ ì•„ë‹Œ ê²½ìš°)")

        # ë¹„ë™ê¸° ì‹¤í–‰
        try:
            asyncio.run(main_async(args))
        except KeyboardInterrupt:
            logger.info("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            sys.exit(0)
        except Exception as e:
            logger.exception(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
            sys.exit(1)


if __name__ == "__main__":
    main()
