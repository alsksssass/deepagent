#!/usr/bin/env python3
"""
AWS Batch ì‘ì—… ë¡œê·¸ ê°€ì ¸ì˜¤ê¸° ìŠ¤í¬ë¦½íŠ¸

Usage:
    # Job IDë¡œ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
    python scripts/fetch_batch_logs.py --job-id 35eaf1c8-99c7-4602-b8b8-635c1140338e

    # Log stream nameìœ¼ë¡œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
    python scripts/fetch_batch_logs.py --log-stream deep-agents/default/0a89ffc969184b39b1425ff883757e16

    # ì¶œë ¥ íŒŒì¼ ì§€ì •
    python scripts/fetch_batch_logs.py --job-id <JOB_ID> --output logs/batch_job.log

    # ìµœê·¼ Nê°œ ì´ë²¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
    python scripts/fetch_batch_logs.py --job-id <JOB_ID> --limit 500

    # íŠ¹ì • í‚¤ì›Œë“œ í•„í„°ë§
    python scripts/fetch_batch_logs.py --job-id <JOB_ID> --filter "validation errors"
"""

import argparse
import json
import sys
import re
from datetime import datetime
from pathlib import Path
import subprocess


def run_aws_command(cmd: list) -> dict:
    """AWS CLI ëª…ë ¹ ì‹¤í–‰"""
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        return json.loads(result.stdout) if result.stdout else {}
    except subprocess.CalledProcessError as e:
        print(f"âŒ AWS CLI ëª…ë ¹ ì‹¤íŒ¨: {e.stderr}", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}", file=sys.stderr)
        sys.exit(1)


def get_log_stream_from_job_id(job_id: str, region: str) -> tuple[str, str]:
    """Job IDë¡œë¶€í„° log stream nameê³¼ status ê°€ì ¸ì˜¤ê¸°"""
    print(f"ğŸ” Job IDë¡œ ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ ì¡°íšŒ ì¤‘: {job_id}")

    cmd = [
        "aws", "batch", "describe-jobs",
        "--jobs", job_id,
        "--region", region,
        "--output", "json"
    ]

    result = run_aws_command(cmd)

    if not result.get("jobs"):
        print(f"âŒ Jobì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}", file=sys.stderr)
        sys.exit(1)

    job = result["jobs"][0]
    log_stream = job.get("container", {}).get("logStreamName")
    status = job.get("status", "UNKNOWN")

    if not log_stream:
        print(f"âŒ ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Job ìƒíƒœ: {status}", file=sys.stderr)
        sys.exit(1)

    print(f"âœ… ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ ë°œê²¬: {log_stream}")
    print(f"ğŸ“Š Job ìƒíƒœ: {status}")

    return log_stream, status


def fetch_logs(log_stream: str, region: str, log_group: str = None, limit: int = None) -> list[dict]:
    """CloudWatch Logsì—ì„œ ë¡œê·¸ ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    print(f"ğŸ“¥ ë¡œê·¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘... (limit: {limit or 'unlimited'})")
    
    # ë¡œê·¸ ê·¸ë£¹ ìë™ ê°ì§€: log-stream ì´ë¦„ìœ¼ë¡œ íŒë‹¨
    if not log_group:
        if log_stream.startswith("deep-agents/"):
            log_group = "/aws/batch/deep-agents"
        else:
            log_group = "/aws/batch/job"  # ê¸°ë³¸ê°’
    
    print(f"ğŸ“‚ ë¡œê·¸ ê·¸ë£¹: {log_group}")

    cmd = [
        "aws", "logs", "get-log-events",
        "--log-group-name", log_group,
        "--log-stream-name", log_stream,
        "--region", region,
        "--output", "json"
    ]

    if limit:
        cmd.extend(["--limit", str(limit)])

    result = run_aws_command(cmd)
    events = result.get("events", [])

    print(f"âœ… {len(events)}ê°œ ë¡œê·¸ ì´ë²¤íŠ¸ ê°€ì ¸ì˜´")
    return events


def format_log_event(event: dict) -> str:
    """ë¡œê·¸ ì´ë²¤íŠ¸ë¥¼ ì½ê¸° ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    timestamp = datetime.fromtimestamp(event["timestamp"] / 1000)
    time_str = timestamp.strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    message = event["message"].rstrip()

    return f"[{time_str}] {message}"


def should_include_log(message: str, errors_only: bool = False, min_level: str = None, 
                       filter_keyword: str = None, exclude_patterns: list = None) -> bool:
    """ë¡œê·¸ ë©”ì‹œì§€ê°€ í•„í„° ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸"""
    message_lower = message.lower()
    
    # ì˜¤ë¥˜ë§Œ í•„í„°ë§
    if errors_only:
        error_patterns = [
            r'\bERROR\b',
            r'\bWARNING\b',
            r'\bException\b',
            r'\bTraceback\b',
            r'\bFailed\b',
            r'\bfailed\b',
            r'\bì‹¤íŒ¨\b',
            r'âš ï¸',
            r'âŒ',
            r'validation error',
            r'íŒŒì‹± ì‹¤íŒ¨',
            r'ë¶„ì„ ì‹¤íŒ¨',
            r'ì²˜ë¦¬ ì‹¤íŒ¨',
        ]
        if not any(re.search(pattern, message, re.IGNORECASE) for pattern in error_patterns):
            return False
    
    # ìµœì†Œ ë¡œê·¸ ë ˆë²¨ í•„í„°ë§
    if min_level:
        level_order = {'DEBUG': 0, 'INFO': 1, 'WARNING': 2, 'ERROR': 3, 'CRITICAL': 4}
        message_level = None
        for level in ['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG']:
            if f' - {level} - ' in message or f' - {level} ' in message:
                message_level = level
                break
        
        if message_level:
            if level_order.get(message_level, 0) < level_order.get(min_level, 0):
                return False
    
    # í‚¤ì›Œë“œ í•„í„°ë§
    if filter_keyword:
        if filter_keyword.lower() not in message_lower:
            return False
    
    # ì œì™¸ íŒ¨í„´ í•„í„°ë§
    if exclude_patterns:
        for pattern in exclude_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return False
    
    return True


def save_logs(events: list[dict], output_path: Path, filter_keyword: str = None,
              errors_only: bool = False, min_level: str = None, exclude_patterns: list = None):
    """ë¡œê·¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    filtered_count = 0
    total_count = len(events)
    excluded_count = 0

    with open(output_path, 'w', encoding='utf-8') as f:
        for event in events:
            formatted = format_log_event(event)
            message = event.get("message", "")

            # í•„í„°ë§ ì ìš©
            if should_include_log(message, errors_only, min_level, filter_keyword, exclude_patterns):
                f.write(formatted + '\n')
                filtered_count += 1
            else:
                excluded_count += 1

    # í†µê³„ ì¶œë ¥
    stats = []
    if errors_only:
        stats.append("ì˜¤ë¥˜/ê²½ê³ ë§Œ")
    if min_level:
        stats.append(f"ìµœì†Œ ë ˆë²¨: {min_level}")
    if filter_keyword:
        stats.append(f"í‚¤ì›Œë“œ: '{filter_keyword}'")
    if exclude_patterns:
        stats.append(f"ì œì™¸ íŒ¨í„´: {len(exclude_patterns)}ê°œ")
    
    stats_str = f" ({', '.join(stats)})" if stats else ""
    print(f"ğŸ“ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"   ì „ì²´: {total_count}ê°œ â†’ í•„í„°ë§: {filtered_count}ê°œ (ì œì™¸: {excluded_count}ê°œ){stats_str}")


def print_logs(events: list[dict], filter_keyword: str = None, tail: int = None,
               errors_only: bool = False, min_level: str = None, exclude_patterns: list = None):
    """ë¡œê·¸ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
    filtered_events = []

    for event in events:
        formatted = format_log_event(event)
        message = event.get("message", "")

        if should_include_log(message, errors_only, min_level, filter_keyword, exclude_patterns):
            filtered_events.append(formatted)

    # tail ì˜µì…˜ ì ìš©
    if tail and len(filtered_events) > tail:
        print(f"\n... ({len(filtered_events) - tail}ê°œ ì´ë²¤íŠ¸ ìƒëµ) ...\n")
        filtered_events = filtered_events[-tail:]

    for line in filtered_events:
        print(line)

    stats = []
    if errors_only:
        stats.append("ì˜¤ë¥˜/ê²½ê³ ë§Œ")
    if min_level:
        stats.append(f"ìµœì†Œ ë ˆë²¨: {min_level}")
    if filter_keyword:
        stats.append(f"í‚¤ì›Œë“œ: '{filter_keyword}'")
    
    stats_str = f" ({', '.join(stats)})" if stats else ""
    print(f"\nğŸ“Š ì´ {len(filtered_events)}/{len(events)} ì´ë²¤íŠ¸{stats_str}")


def main():
    parser = argparse.ArgumentParser(
        description="AWS Batch ì‘ì—… ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    # ì…ë ¥ ì˜µì…˜
    input_group = parser.add_mutually_exclusive_group(required=False)
    input_group.add_argument(
        "--job-id",
        help="AWS Batch Job ID"
    )
    input_group.add_argument(
        "--log-stream",
        help="CloudWatch Log Stream ì´ë¦„"
    )
    
    # ìœ„ì¹˜ ì¸ìë¡œ log-stream ë°›ê¸°
    parser.add_argument(
        "log_stream_positional",
        nargs="?",
        help="CloudWatch Log Stream ì´ë¦„ (ìœ„ì¹˜ ì¸ì)"
    )

    # ì¶œë ¥ ì˜µì…˜
    parser.add_argument(
        "--output", "-o",
        type=Path,
        help="ë¡œê·¸ ì €ì¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: logs/batch_<timestamp>.log)"
    )

    # í•„í„°ë§ ì˜µì…˜
    parser.add_argument(
        "--limit",
        type=int,
        help="ê°€ì ¸ì˜¬ ìµœëŒ€ ì´ë²¤íŠ¸ ìˆ˜"
    )
    parser.add_argument(
        "--filter", "-f",
        help="í•„í„°ë§í•  í‚¤ì›Œë“œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨)"
    )
    parser.add_argument(
        "--tail", "-t",
        type=int,
        help="ë§ˆì§€ë§‰ Nê°œ ì´ë²¤íŠ¸ë§Œ ì¶œë ¥ (ì½˜ì†” ì¶œë ¥ ì‹œ)"
    )
    parser.add_argument(
        "--errors-only", "-e",
        action="store_true",
        help="ì˜¤ë¥˜/ê²½ê³ ë§Œ í•„í„°ë§ (ERROR, WARNING, Exception, Traceback ë“±, ê¸°ë³¸ê°’)"
    )
    parser.add_argument(
        "--min-level",
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help="ìµœì†Œ ë¡œê·¸ ë ˆë²¨ (ì´ ë ˆë²¨ ì´ìƒë§Œ í¬í•¨)"
    )
    parser.add_argument(
        "--exclude",
        action="append",
        help="ì œì™¸í•  íŒ¨í„´ (ì •ê·œì‹, ì—¬ëŸ¬ ë²ˆ ì‚¬ìš© ê°€ëŠ¥)"
    )

    # AWS ì„¤ì •
    parser.add_argument(
        "--region",
        default="ap-northeast-2",
        help="AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)"
    )
    parser.add_argument(
        "--log-group",
        help="CloudWatch Log Group ì´ë¦„ (ê¸°ë³¸ê°’: ìë™ ê°ì§€)"
    )

    # ì¶œë ¥ ëª¨ë“œ
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="íŒŒì¼ë¡œ ì €ì¥í•˜ì§€ ì•Šê³  ì½˜ì†”ì—ë§Œ ì¶œë ¥"
    )
    
    # ê¸°ë³¸ê°’: ì˜¤ë¥˜ë§Œ í•„í„°ë§ (í¸ì˜ì„±)
    parser.add_argument(
        "--all",
        action="store_true",
        help="ëª¨ë“  ë¡œê·¸ í¬í•¨ (ê¸°ë³¸ê°’: --errors-onlyê°€ í™œì„±í™”ëœ ê²½ìš° ë¹„í™œì„±í™”)"
    )

    args = parser.parse_args()
    
    # ê¸°ë³¸ê°’: ì˜¤ë¥˜ë§Œ í•„í„°ë§ (--allì´ ì—†ê³  ë‹¤ë¥¸ í•„í„°ë„ ì—†ìœ¼ë©´)
    if not args.all and not args.filter and not args.min_level:
        args.errors_only = True
        print("â„¹ï¸  ê¸°ë³¸ê°’: ì˜¤ë¥˜/ê²½ê³ ë§Œ í•„í„°ë§ (--allë¡œ ëª¨ë“  ë¡œê·¸ í¬í•¨ ê°€ëŠ¥)")
    elif args.all:
        # --allì´ ì§€ì •ë˜ë©´ errors_only ë¹„í™œì„±í™”
        args.errors_only = False

    # Log stream ì´ë¦„ í™•ì¸
    if args.job_id:
        log_stream, status = get_log_stream_from_job_id(args.job_id, args.region)
    elif args.log_stream:
        log_stream = args.log_stream
        print(f"ğŸ” ë¡œê·¸ ìŠ¤íŠ¸ë¦¼: {log_stream}")
    elif args.log_stream_positional:
        log_stream = args.log_stream_positional
        print(f"ğŸ” ë¡œê·¸ ìŠ¤íŠ¸ë¦¼: {log_stream}")
    else:
        parser.error("--job-id, --log-stream ë˜ëŠ” ìœ„ì¹˜ ì¸ìë¡œ log-streamì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")

    # ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
    events = fetch_logs(log_stream, args.region, args.log_group, args.limit)

    if not events:
        print("âš ï¸ ë¡œê·¸ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“‹ ë¡œê·¸ ë‚´ìš©")
    print("="*80 + "\n")
    print_logs(events, args.filter, args.tail, args.errors_only, args.min_level, args.exclude)

    # íŒŒì¼ ì €ì¥
    if not args.no_save:
        if args.output:
            output_path = args.output
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            job_suffix = args.job_id[:8] if args.job_id else "stream"
            suffix = "_errors" if args.errors_only else ""
            output_path = Path(f"logs/batch_{job_suffix}_{timestamp}{suffix}.log")

        save_logs(events, output_path, args.filter, args.errors_only, args.min_level, args.exclude)
        print(f"\nğŸ’¾ ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜: {output_path.absolute()}")


if __name__ == "__main__":
    main()
