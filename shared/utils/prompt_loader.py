"""
YAML ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ë¡œë”

ì—ì´ì „íŠ¸ë³„ í”„ë¡¬í”„íŠ¸ë¥¼ YAML íŒŒì¼ë¡œ ê´€ë¦¬í•˜ê³  ìºì‹±í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
"""

import yaml
import importlib
import inspect
import os
from pathlib import Path
from functools import lru_cache
from typing import Dict, Any, Optional, Type
import logging

from pydantic import BaseModel
from langchain_aws import ChatBedrockConverse
from shared.schemas.common import BaseResponse
from .schema_prompt_generator import SchemaPromptGenerator

logger = logging.getLogger(__name__)


class PromptLoader:
    """
    YAML í”„ë¡¬í”„íŠ¸ ë¡œë” (ìºì‹± ì§€ì›)

    ì‚¬ìš© ì˜ˆì‹œ:
        prompts = PromptLoader.load("commit_evaluator")
        system_prompt = prompts["system_prompt"]
        user_template = PromptLoader.format(
            prompts["user_template"],
            commit_hash="abc123",
            message="Add feature"
        )
    """

    @staticmethod
    @lru_cache(maxsize=32)
    def load(agent_name: str) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ë³„ í”„ë¡¬í”„íŠ¸ YAML ë¡œë“œ

        Args:
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„ (ì˜ˆ: "commit_evaluator", "planner")

        Returns:
            prompts: {
                "system_prompt": "...",
                "user_template": "...",
                "model": "...",
                ...
            }

        Raises:
            FileNotFoundError: prompts.yaml íŒŒì¼ì´ ì—†ì„ ë•Œ
            yaml.YAMLError: YAML íŒŒì‹± ì‹¤íŒ¨ ì‹œ
        """
        base_path = Path(__file__).parent.parent.parent

        # plannerëŠ” core/planner/prompts.yamlì— ìˆìŒ
        if agent_name == "planner":
            prompt_path = base_path / "core" / "planner" / "prompts.yaml"
        else:
            # agents/{agent_name}/prompts.yaml ê²½ë¡œ
            prompt_path = base_path / "agents" / agent_name / "prompts.yaml"

        if not prompt_path.exists():
            expected_location = (
                f"core/planner/prompts.yaml" if agent_name == "planner"
                else f"agents/{agent_name}/prompts.yaml"
            )
            raise FileNotFoundError(
                f"Prompt file not found: {prompt_path}\n"
                f"Expected location: {expected_location}"
            )

        try:
            with open(prompt_path, "r", encoding="utf-8") as f:
                prompts = yaml.safe_load(f)

            logger.debug(f"âœ… Loaded prompts for {agent_name} (cached)")
            return prompts

        except yaml.YAMLError as e:
            logger.error(f"âŒ YAML parsing error in {prompt_path}: {e}")
            raise

    @staticmethod
    def format(template: str, **kwargs) -> str:
        """
        í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë³€ìˆ˜ ì¹˜í™˜

        Args:
            template: í…œí”Œë¦¿ ë¬¸ìì—´ (ì˜ˆ: "Hash: {commit_hash}")
            **kwargs: ì¹˜í™˜í•  ë³€ìˆ˜ë“¤

        Returns:
            ì¹˜í™˜ëœ ë¬¸ìì—´

        Example:
            >>> template = "Commit {hash} by {author}"
            >>> PromptLoader.format(template, hash="abc123", author="John")
            "Commit abc123 by John"
        """
        try:
            return template.format(**kwargs)
        except KeyError as e:
            logger.warning(f"âš ï¸ Missing template variable: {e}")
            return template

    @staticmethod
    def get_model(agent_name: str) -> str:
        """
        ì—ì´ì „íŠ¸ì˜ ê¸°ë³¸ ëª¨ë¸ ID ë°˜í™˜

        Args:
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„

        Returns:
            model_id: Bedrock ëª¨ë¸ ID (ê¸°ë³¸ê°’: claude-3-5-sonnet)
        """
        prompts = PromptLoader.load(agent_name)
        return prompts.get("model", "us.anthropic.claude-3-5-sonnet-20241022-v2:0")

    @staticmethod
    @lru_cache(maxsize=16)
    def get_llm(agent_name: str) -> ChatBedrockConverse:
        """
        ì—ì´ì „íŠ¸ì˜ YAML ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ChatBedrockConverse ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë°˜í™˜
        
        YAMLì—ì„œ ëª¨ë¸ IDì™€ ì„¤ì •ì„ ë¡œë“œí•˜ì—¬ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ê°™ì€ ì—ì´ì „íŠ¸ì— ëŒ€í•´ì„œëŠ” ìºì‹±ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
            
        Returns:
            ChatBedrockConverse ì¸ìŠ¤í„´ìŠ¤
            
        Example:
            >>> llm = PromptLoader.get_llm("security_agent")
            >>> response = await llm.ainvoke([SystemMessage(...), HumanMessage(...)])
        """
        prompts = PromptLoader.load(agent_name)
        model_id = prompts.get("model", "anthropic.claude-3-5-sonnet-20241022-v2:0")
        
        # YAMLì—ì„œ ëª¨ë¸ ì„¤ì • ë¡œë“œ (ì„ íƒì )
        model_config = prompts.get("model_config", {})
        
        # ìš°ì„ ìˆœìœ„: í™˜ê²½ ë³€ìˆ˜ > YAML model_config > ê¸°ë³¸ê°’
        region = (
            os.getenv("AWS_DEFAULT_REGION") or 
            model_config.get("region") or 
            "us-east-1"
        )
        temperature = model_config.get("temperature", 0.0)
        max_tokens = model_config.get("max_tokens", 4096)
        
        logger.debug(
            f"âœ… LLM ìƒì„±: {agent_name} - model={model_id}, "
            f"region={region}, temperature={temperature}, max_tokens={max_tokens}"
        )
        
        return ChatBedrockConverse(
            model=model_id,
            region_name=region,
            temperature=temperature,
            max_tokens=max_tokens,
        )

    @staticmethod
    def clear_cache():
        """
        ìºì‹œ ì´ˆê¸°í™” (í…ŒìŠ¤íŠ¸ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ë³€ê²½ ì‹œ)

        ì‚¬ìš© ì˜ˆì‹œ:
            PromptLoader.clear_cache()  # ìºì‹œ ì‚­ì œ
            prompts = PromptLoader.load("commit_evaluator")  # ì¬ë¡œë“œ
            llm = PromptLoader.get_llm("security_agent")  # ì¬ìƒì„±
        """
        PromptLoader.load.cache_clear()
        PromptLoader.get_llm.cache_clear()
        logger.info("ğŸ”„ Prompt and LLM cache cleared")

    @staticmethod
    def validate_prompts(agent_name: str, required_keys: list[str]) -> bool:
        """
        í”„ë¡¬í”„íŠ¸ YAMLì˜ í•„ìˆ˜ í‚¤ ê²€ì¦

        Args:
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
            required_keys: í•„ìˆ˜ í‚¤ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["system_prompt", "user_template"])

        Returns:
            True if all required keys exist

        Raises:
            ValueError: í•„ìˆ˜ í‚¤ ëˆ„ë½ ì‹œ
        """
        prompts = PromptLoader.load(agent_name)
        missing_keys = [key for key in required_keys if key not in prompts]

        if missing_keys:
            raise ValueError(
                f"Missing required prompt keys for {agent_name}: {missing_keys}"
            )

        logger.debug(f"âœ… Prompt validation passed for {agent_name}")
        return True

    @staticmethod
    def load_with_schema(
        agent_name: str,
        response_schema_class: Optional[Type[BaseModel]] = None,
        schema_key: str = "json_schema"
    ) -> Dict[str, Any]:
        """
        í”„ë¡¬í”„íŠ¸ ë¡œë“œ + ìŠ¤í‚¤ë§ˆ ìë™ ì£¼ì… (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
        
        í•˜ì´ë¸Œë¦¬ë“œ ë™ì‘:
        1. ì»¤ìŠ¤í„°ë§ˆì´ì§•ëœ ìŠ¤í‚¤ë§ˆê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (custom_json_schema)
        2. response_schema_classê°€ ì œê³µë˜ë©´ ìë™ ìƒì„±
        3. auto_detect_schemaê°€ trueì´ë©´ ìë™ ê°ì§€ ì‹œë„
        4. ì—†ìœ¼ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë§Œ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
        
        Args:
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
            response_schema_class: Response ìŠ¤í‚¤ë§ˆ í´ë˜ìŠ¤ (Noneì´ë©´ ìë™ ê°ì§€ ì‹œë„)
            schema_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì—ì„œ ì‚¬ìš©í•  ë³€ìˆ˜ëª… (ê¸°ë³¸: "json_schema")
            
        Returns:
            í”„ë¡¬í”„íŠ¸ dict (json_schema ë³€ìˆ˜ í¬í•¨)
            
        Example:
            >>> from agents.security_agent.schemas import SecurityAnalysis
            >>> prompts = PromptLoader.load_with_schema(
            ...     "security_agent",
            ...     response_schema_class=SecurityAnalysis
            ... )
            >>> system_prompt = PromptLoader.format(
            ...     prompts["system_prompt"],
            ...     json_schema=prompts["json_schema"]
            ... )
        """
        # 1. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        prompts = PromptLoader.load(agent_name)
        
        # 2. ì»¤ìŠ¤í„°ë§ˆì´ì§•ëœ ìŠ¤í‚¤ë§ˆê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if "custom_json_schema" in prompts:
            prompts[schema_key] = prompts["custom_json_schema"]
            logger.debug(f"âœ… Using custom JSON schema for {agent_name}")
            return prompts
        
        # 3. ìŠ¤í‚¤ë§ˆ í´ë˜ìŠ¤ê°€ ì œê³µë˜ë©´ ìë™ ìƒì„±
        if response_schema_class:
            try:
                prompts[schema_key] = SchemaPromptGenerator.generate_json_schema_example(
                    response_schema_class
                )
                logger.debug(f"âœ… Auto-generated JSON schema for {agent_name} from {response_schema_class.__name__}")
                return prompts
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to generate schema for {agent_name}: {e}")
        
        # 4. ìë™ ê°ì§€ ì‹œë„ (ì„ íƒì )
        if prompts.get("auto_detect_schema", False):
            schema_class = PromptLoader._detect_response_schema(agent_name)
            if schema_class:
                try:
                    prompts[schema_key] = SchemaPromptGenerator.generate_json_schema_example(
                        schema_class
                    )
                    logger.debug(f"âœ… Auto-detected and generated JSON schema for {agent_name}")
                    return prompts
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to generate schema from auto-detection: {e}")
        
        # 5. ìŠ¤í‚¤ë§ˆê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ (í•˜ìœ„ í˜¸í™˜ì„±)
        prompts[schema_key] = ""
        logger.debug(f"âš ï¸ No JSON schema generated for {agent_name} (using empty string)")
        return prompts

    @staticmethod
    def _detect_response_schema(agent_name: str) -> Optional[Type[BaseModel]]:
        """
        ì—ì´ì „íŠ¸ ì´ë¦„ì—ì„œ Response ìŠ¤í‚¤ë§ˆ í´ë˜ìŠ¤ ìë™ ê°ì§€
        
        ì˜ˆ: "security_agent" â†’ SecurityAgentResponse
            "commit_evaluator" â†’ CommitEvaluatorResponse
        
        Args:
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
            
        Returns:
            Response ìŠ¤í‚¤ë§ˆ í´ë˜ìŠ¤ ë˜ëŠ” None
        """
        try:
            # agents/{agent_name}/schemas.py ëª¨ë“ˆ ë¡œë“œ
            module_name = f"agents.{agent_name}.schemas"
            module = importlib.import_module(module_name)
            
            # Responseë¡œ ëë‚˜ëŠ” í´ë˜ìŠ¤ ì°¾ê¸° (BaseResponse ì œì™¸)
            for name, obj in inspect.getmembers(module):
                if (inspect.isclass(obj) and 
                    name.endswith("Response") and 
                    name != "BaseResponse" and
                    issubclass(obj, BaseResponse) and
                    obj != BaseResponse):
                    logger.debug(f"âœ… Auto-detected schema: {name} for {agent_name}")
                    return obj
            
            # Responseê°€ ì—†ìœ¼ë©´ Analysisë¡œ ëë‚˜ëŠ” í´ë˜ìŠ¤ ì°¾ê¸°
            for name, obj in inspect.getmembers(module):
                if (inspect.isclass(obj) and 
                    name.endswith("Analysis") and 
                    issubclass(obj, BaseModel) and
                    obj != BaseModel):
                    logger.debug(f"âœ… Auto-detected schema: {name} for {agent_name}")
                    return obj
                    
        except ImportError as e:
            logger.debug(f"âš ï¸ Could not import schemas for {agent_name}: {e}")
        except Exception as e:
            logger.warning(f"âš ï¸ Error detecting schema for {agent_name}: {e}")
        
        return None
