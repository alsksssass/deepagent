"""
S3StorageBackend - AWS S3 ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€

AWS í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ S3ì— ê²°ê³¼ë¥¼ ì €ì¥/ë¡œë“œ
"""

import json
import logging
from typing import Type, TypeVar, Optional, List, Any
from datetime import datetime
from pathlib import Path
try:
    import boto3
    from botocore.exceptions import ClientError
except ImportError:
    boto3 = None
    ClientError = Exception

from shared.storage.base import StorageBackend
from shared.schemas.common import BaseResponse
from shared.config import settings

logger = logging.getLogger(__name__)

T = TypeVar("T", bound=BaseResponse)


class S3StorageBackend(StorageBackend):
    """
    AWS S3 ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ

    êµ¬ì¡°:
        s3://bucket-name/analyze_multi/{main_task_uuid}/repos/{task_uuid}/
        â”œâ”€â”€ results/
        â”‚   â”œâ”€â”€ repo_cloner.json
        â”‚   â”œâ”€â”€ static_analyzer.json
        â”‚   â”œâ”€â”€ commit_evaluator/
        â”‚   â”‚   â”œâ”€â”€ batch_0000.json
        â”‚   â”‚   â””â”€â”€ batch_0001.json
        â”‚   â””â”€â”€ reporter.json
        â””â”€â”€ metadata.json
    """

    def __init__(self, task_uuid: str, base_path: str):
        """
        S3StorageBackend ì´ˆê¸°í™”

        Args:
            task_uuid: ì‘ì—… ê³ ìœ  UUID
            base_path: S3 ê¸°ë³¸ ê²½ë¡œ (ì˜ˆ: "analyze_multi/{main_task_uuid}/repos/{task_uuid}")

        Raises:
            ImportError: boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
        """
        if boto3 is None:
            raise ImportError(
                "boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install boto3'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
            )

        super().__init__(task_uuid, base_path)

        # S3 ì„¤ì •
        # ARN í˜•ì‹ì¸ ê²½ìš° bucket nameë§Œ ì¶”ì¶œ
        bucket_name_raw = settings.S3_BUCKET_NAME
        if bucket_name_raw.startswith("arn:aws:s3:::"):
            # arn:aws:s3:::bucket-name í˜•ì‹ì—ì„œ bucket name ì¶”ì¶œ
            self.bucket_name = bucket_name_raw.split(":::")[-1]
        else:
            self.bucket_name = bucket_name_raw
            
        self.region = settings.S3_REGION
        self.base_prefix = base_path.strip("/")
        self.results_prefix = f"{self.base_prefix}/results"

        # S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        self.s3_client = boto3.client(
            "s3",
            region_name=self.region,
            aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
            aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
        )

        logger.debug(f"ğŸ“¦ S3StorageBackend ì´ˆê¸°í™”: s3://{self.bucket_name}/{self.base_prefix}")

    def _get_s3_key(self, *parts: str) -> str:
        """S3 í‚¤ ìƒì„± í—¬í¼"""
        return "/".join(str(p).strip("/") for p in parts if p)

    def _upload_json(self, key: str, data: dict | str) -> str:
        """JSON ë°ì´í„°ë¥¼ S3ì— ì—…ë¡œë“œ"""
        try:
            if isinstance(data, str):
                json_content = data
            else:
                json_content = json.dumps(data, indent=2, ensure_ascii=False, default=str)

            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=key,
                Body=json_content.encode("utf-8"),
                ContentType="application/json",
                Metadata={"uploaded_at": datetime.now().isoformat()},
            )

            s3_path = f"s3://{self.bucket_name}/{key}"
            logger.debug(f"ğŸ’¾ S3 ì—…ë¡œë“œ: {s3_path}")
            return s3_path

        except ClientError as e:
            logger.error(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨ ({key}): {e}")
            raise

    def _download_json(self, key: str) -> dict:
        """S3ì—ì„œ JSON ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)
            json_content = response["Body"].read().decode("utf-8")
            return json.loads(json_content)

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                raise FileNotFoundError(f"S3 ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: s3://{self.bucket_name}/{key}")
            logger.error(f"âŒ S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({key}): {e}")
            raise

    def _list_objects(self, prefix: str, suffix: str = "") -> List[str]:
        """S3 ê°ì²´ ëª©ë¡ ì¡°íšŒ"""
        try:
            paginator = self.s3_client.get_paginator("list_objects_v2")
            pages = paginator.paginate(Bucket=self.bucket_name, Prefix=prefix)

            keys = []
            for page in pages:
                if "Contents" in page:
                    for obj in page["Contents"]:
                        key = obj["Key"]
                        if suffix and not key.endswith(suffix):
                            continue
                        keys.append(key)

            return sorted(keys)

        except ClientError as e:
            logger.error(f"âŒ S3 ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ({prefix}): {e}")
            return []

    def save_result(self, agent_name: str, result: BaseResponse) -> str:
        """ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ S3ì— ì €ì¥"""
        key = self._get_s3_key(self.results_prefix, f"{agent_name}.json")

        try:
            json_content = result.model_dump_json(indent=2, ensure_ascii=False)
            s3_path = self._upload_json(key, json_content)

            logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ (S3): {agent_name} â†’ {s3_path}")
            return s3_path

        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ ({agent_name}): {e}")
            raise

    def load_result(self, agent_name: str, result_class: Type[T]) -> T:
        """S3ì—ì„œ ì €ì¥ëœ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ë¡œë“œ"""
        key = self._get_s3_key(self.results_prefix, f"{agent_name}.json")

        try:
            data = self._download_json(key)
            result = result_class(**data)

            logger.debug(f"ğŸ“‚ ê²°ê³¼ ë¡œë“œ (S3): {agent_name} â† s3://{self.bucket_name}/{key}")
            return result

        except FileNotFoundError:
            raise FileNotFoundError(f"ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {agent_name}")
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨ ({agent_name}): {e}")
            raise

    def save_batched_result(
        self,
        agent_name: str,
        batch_id: int,
        result: BaseResponse | List[BaseResponse] | dict[str, Any],
    ) -> str:
        """ëŒ€ìš©ëŸ‰ ê²°ê³¼ë¥¼ ë°°ì¹˜ë³„ë¡œ S3ì— ì €ì¥"""
        key = self._get_s3_key(
            self.results_prefix, agent_name, f"batch_{batch_id:04d}.json"
        )

        try:
            # ê²°ê³¼ íƒ€ì…ì— ë”°ë¼ ì§ë ¬í™”
            if isinstance(result, BaseResponse):
                data = result.model_dump()
            elif isinstance(result, list) and result and isinstance(result[0], BaseResponse):
                data = [r.model_dump() for r in result]
            else:
                data = result

            s3_path = self._upload_json(key, data)

            logger.info(f"ğŸ’¾ ë°°ì¹˜ ê²°ê³¼ ì €ì¥ (S3): {agent_name}/batch_{batch_id:04d} â†’ {s3_path}")
            return s3_path

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ ({agent_name}/batch_{batch_id}): {e}")
            raise

    def load_batched_results(
        self,
        agent_name: str,
        result_class: Optional[Type[T]] = None,
    ) -> List[dict[str, Any]] | List[T]:
        """S3ì—ì„œ ë°°ì¹˜ ê²°ê³¼ ì „ì²´ë¥¼ ë¡œë“œ"""
        prefix = self._get_s3_key(self.results_prefix, agent_name, "batch_")
        batch_keys = self._list_objects(prefix, suffix=".json")

        if not batch_keys:
            logger.warning(f"âš ï¸  ë°°ì¹˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {agent_name}")
            return []

        results = []

        for key in batch_keys:
            try:
                data = self._download_json(key)

                if result_class:
                    if isinstance(data, list):
                        results.extend([result_class(**item) for item in data])
                    else:
                        results.append(result_class(**data))
                else:
                    if isinstance(data, list):
                        results.extend(data)
                    else:
                        results.append(data)

            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({key}): {e}")
                continue

        logger.debug(f"ğŸ“‚ ë°°ì¹˜ ê²°ê³¼ ë¡œë“œ (S3): {agent_name} - {len(results)}ê°œ í•­ëª©")
        return results

    def save_metadata(self, metadata: dict[str, Any]) -> str:
        """ì‘ì—… ë©”íƒ€ë°ì´í„°ë¥¼ S3ì— ì €ì¥"""
        key = self._get_s3_key(self.base_prefix, "metadata.json")

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        metadata["updated_at"] = datetime.now().isoformat()

        try:
            s3_path = self._upload_json(key, metadata)
            logger.debug(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ (S3): {s3_path}")
            return s3_path

        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def load_metadata(self) -> dict[str, Any]:
        """S3ì—ì„œ ì‘ì—… ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œ"""
        key = self._get_s3_key(self.base_prefix, "metadata.json")

        try:
            return self._download_json(key)
        except FileNotFoundError:
            return {}
        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}

    def list_available_results(self) -> List[str]:
        """S3ì—ì„œ ì €ì¥ëœ ì—ì´ì „íŠ¸ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ"""
        prefix = self._get_s3_key(self.results_prefix, "")
        keys = self._list_objects(prefix, suffix=".json")

        # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ë””ë ‰í† ë¦¬ ì œì™¸)
        result_files = []
        for key in keys:
            parts = key.split("/")
            if len(parts) > 0:
                filename = parts[-1]
                # batch_ íŒŒì¼ ì œì™¸, .json íŒŒì¼ë§Œ
                if not filename.startswith("batch_") and filename.endswith(".json"):
                    result_files.append(filename.replace(".json", ""))

        return sorted(set(result_files))

    def list_batched_agents(self) -> List[str]:
        """S3ì—ì„œ ë°°ì¹˜ ì €ì¥ëœ ì—ì´ì „íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        prefix = self._get_s3_key(self.results_prefix, "")
        keys = self._list_objects(prefix)

        # batch_ íŒŒì¼ì´ ìˆëŠ” ì—ì´ì „íŠ¸ ì¶”ì¶œ
        batched_agents = set()
        for key in keys:
            parts = key.split("/")
            if len(parts) >= 2:
                filename = parts[-1]
                if filename.startswith("batch_"):
                    agent_name = parts[-2]
                    batched_agents.add(agent_name)

        return sorted(batched_agents)

    def get_result_path(self, agent_name: str) -> str:
        """ì—ì´ì „íŠ¸ ê²°ê³¼ S3 ê²½ë¡œ ë°˜í™˜"""
        key = self._get_s3_key(self.results_prefix, f"{agent_name}.json")
        return f"s3://{self.bucket_name}/{key}"

    def get_batch_dir(self, agent_name: str) -> str:
        """ë°°ì¹˜ ê²°ê³¼ S3 ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
        prefix = self._get_s3_key(self.results_prefix, agent_name, "")
        return f"s3://{self.bucket_name}/{prefix}"

    def save_report(self, report_name: str, content: str) -> str:
        """ë¦¬í¬íŠ¸ íŒŒì¼ì„ S3ì— ì €ì¥"""
        key = self._get_s3_key(self.base_prefix, "reports", report_name)
        
        try:
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=key,
                Body=content.encode("utf-8"),
                ContentType="text/markdown",
                Metadata={"uploaded_at": datetime.now().isoformat()},
            )
            
            s3_path = f"s3://{self.bucket_name}/{key}"
            logger.info(f"ğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ (S3): {s3_path}")
            return s3_path
            
        except ClientError as e:
            logger.error(f"âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨ ({report_name}): {e}")
            raise

    def load_report(self, report_name: str) -> str:
        """S3ì—ì„œ ë¦¬í¬íŠ¸ íŒŒì¼ ë¡œë“œ"""
        key = self._get_s3_key(self.base_prefix, "reports", report_name)
        
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)
            content = response["Body"].read().decode("utf-8")
            logger.debug(f"ğŸ“‚ ë¦¬í¬íŠ¸ ë¡œë“œ (S3): {report_name}")
            return content
            
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                raise FileNotFoundError(f"ë¦¬í¬íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {report_name}")
            logger.error(f"âŒ ë¦¬í¬íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ({report_name}): {e}")
            raise

    def save_log(self, log_name: str, content: str) -> str:
        """ë¡œê·¸ íŒŒì¼ì„ S3ì— ì €ì¥"""
        key = self._get_s3_key(self.base_prefix, "logs", log_name)
        
        try:
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=key,
                Body=content.encode("utf-8"),
                ContentType="text/plain",
                Metadata={"uploaded_at": datetime.now().isoformat()},
            )
            
            s3_path = f"s3://{self.bucket_name}/{key}"
            logger.info(f"ğŸ’¾ ë¡œê·¸ ì €ì¥ (S3): {s3_path}")
            return s3_path
            
        except ClientError as e:
            logger.error(f"âŒ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨ ({log_name}): {e}")
            raise

    def upload_log_directory(self, local_log_dir: Path, remote_subdir: str = None) -> List[str]:
        """
        ë¡œê·¸ ë””ë ‰í† ë¦¬ ì „ì²´ë¥¼ S3ì— ì—…ë¡œë“œ
        
        Args:
            local_log_dir: ë¡œì»¬ ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            remote_subdir: S3ì— ì €ì¥í•  í•˜ìœ„ ë””ë ‰í† ë¦¬ (ì˜ˆ: "debug" â†’ logs/debug/)
        """
        if not local_log_dir.exists():
            logger.warning(f"âš ï¸ ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {local_log_dir}")
            return []
        
        uploaded_paths = []
        
        try:
            for log_file in local_log_dir.rglob("*"):
                if log_file.is_file():
                    # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
                    relative_path = log_file.relative_to(local_log_dir)
                    
                    # S3 í‚¤ ìƒì„±
                    if remote_subdir:
                        key = self._get_s3_key(self.base_prefix, "logs", remote_subdir, str(relative_path))
                    else:
                        key = self._get_s3_key(self.base_prefix, "logs", str(relative_path))
                    
                    # íŒŒì¼ ì—…ë¡œë“œ
                    self.s3_client.upload_file(
                        str(log_file),
                        self.bucket_name,
                        key,
                        ExtraArgs={
                            "ContentType": "text/plain",
                            "Metadata": {"uploaded_at": datetime.now().isoformat()},
                        }
                    )
                    
                    s3_path = f"s3://{self.bucket_name}/{key}"
                    uploaded_paths.append(s3_path)
                    logger.debug(f"ğŸ’¾ ë¡œê·¸ íŒŒì¼ ì—…ë¡œë“œ: {log_file.name} â†’ {s3_path}")
            
            logger.info(f"âœ… ë¡œê·¸ ë””ë ‰í† ë¦¬ ì—…ë¡œë“œ ì™„ë£Œ: {len(uploaded_paths)}ê°œ íŒŒì¼")
            return uploaded_paths
            
        except ClientError as e:
            logger.error(f"âŒ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def save_debug_file(self, relative_path: str, content: str | bytes) -> str:
        """ë””ë²„ê·¸ íŒŒì¼ì„ S3ì— ì €ì¥"""
        key = self._get_s3_key(self.base_prefix, relative_path)
        
        try:
            if isinstance(content, bytes):
                body = content
                content_type = "application/octet-stream"
            else:
                body = content.encode("utf-8")
                content_type = "application/json" if relative_path.endswith(".json") else "text/plain"
            
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=key,
                Body=body,
                ContentType=content_type,
                Metadata={"uploaded_at": datetime.now().isoformat()},
            )
            
            s3_path = f"s3://{self.bucket_name}/{key}"
            logger.debug(f"ğŸ’¾ ë””ë²„ê·¸ íŒŒì¼ ì €ì¥ (S3): {relative_path} â†’ {s3_path}")
            return s3_path
            
        except ClientError as e:
            logger.error(f"âŒ ë””ë²„ê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({relative_path}): {e}")
            raise

    def load_debug_file(self, relative_path: str) -> str:
        """ë””ë²„ê·¸ íŒŒì¼ì„ S3ì—ì„œ ë¡œë“œ"""
        key = self._get_s3_key(self.base_prefix, relative_path)
        
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)
            content = response["Body"].read().decode("utf-8")
            logger.debug(f"ğŸ“‚ ë””ë²„ê·¸ íŒŒì¼ ë¡œë“œ (S3): {relative_path} â†’ s3://{self.bucket_name}/{key}")
            return content
            
        except ClientError as e:
            error_code = e.response.get("Error", {}).get("Code", "")
            if error_code == "NoSuchKey":
                raise FileNotFoundError(f"ë””ë²„ê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {relative_path} (s3://{self.bucket_name}/{key})")
            logger.error(f"âŒ ë””ë²„ê·¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({relative_path}): {e}")
            raise
