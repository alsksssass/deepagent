"""
Deep Agents Orchestrator

ì „ì²´ ì›Œí¬í”Œë¡œìš° ì¡°ìœ¨ ë° ì—ì´ì „íŠ¸ ì‹¤í–‰ (Pydantic ê¸°ë°˜)
"""

import logging
import asyncio
import os
from typing import Any
from datetime import datetime
from pathlib import Path
import uuid

from langchain_aws import ChatBedrockConverse
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from core.state import AgentState
# from core.planner.agent import PlannerAgent
from shared.storage import ResultStore
from shared.utils.token_tracker import TokenTracker
from .config_loader import OrchestratorConfig

# Agents (ìƒˆ ì•„í‚¤í…ì²˜)
from agents.repo_cloner import RepoClonerAgent, RepoClonerContext
from agents.static_analyzer import StaticAnalyzerAgent, StaticAnalyzerContext
from agents.commit_analyzer import CommitAnalyzerAgent, CommitAnalyzerContext
from agents.commit_evaluator import CommitEvaluatorAgent, CommitEvaluatorContext
from agents.user_aggregator import UserAggregatorAgent, UserAggregatorContext, UserAggregatorResponse
from agents.reporter import ReporterAgent, ReporterContext

# Agents (Phase 5 ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ)
from agents.code_rag_builder import CodeRAGBuilderAgent, CodeRAGBuilderContext
from agents.user_skill_profiler import UserSkillProfilerAgent, UserSkillProfilerContext

# Tools (for CommitEvaluator)
from shared.tools.neo4j_tools import get_user_commits

logger = logging.getLogger(__name__)


class DeepAgentOrchestrator:
    """
    Deep Agents ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

    LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ê³  ì—ì´ì „íŠ¸ë¥¼ ì¡°ìœ¨ (Pydantic ê¸°ë°˜)
    """

    def __init__(
        self,
        sonnet_llm: ChatBedrockConverse,
        haiku_llm: ChatBedrockConverse,
        data_dir: Path,
        neo4j_uri: str | None = None,
        neo4j_user: str | None = None,
        neo4j_password: str | None = None,
        config_path: Path | None = None,
        user_id: uuid.UUID | None = None,
        db_writer: Any | None = None,
    ):
        self.sonnet_llm = sonnet_llm
        self.haiku_llm = haiku_llm
        self.data_dir = data_dir

        # Neo4j ì„¤ì •: íŒŒë¼ë¯¸í„° ìš°ì„ , í™˜ê²½ ë³€ìˆ˜, Settings ìˆœì„œ
        from shared.config import settings
        self.neo4j_uri = neo4j_uri or os.getenv("NEO4J_URI") or settings.NEO4J_URI
        self.neo4j_user = neo4j_user or os.getenv("NEO4J_USER", settings.NEO4J_USER)
        self.neo4j_password = neo4j_password or os.getenv("NEO4J_PASSWORD", settings.NEO4J_PASSWORD)

        # DB Writer (ì˜µì…”ë„: Batch ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)
        self.user_id = user_id
        self.db_writer = db_writer

        # Orchestrator ì„¤ì • ë¡œë“œ
        self.config = OrchestratorConfig(config_path)

        # Planner
        # self.planner = PlannerAgent(llm=sonnet_llm)

        # LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
        self.workflow = self._create_workflow()
        self.app = self.workflow.compile(checkpointer=MemorySaver())

    def _create_workflow(self) -> StateGraph:
        """
        LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±

        ë…¸ë“œ:
        1. setup: ì‘ì—… ì´ˆê¸°í™”
        2. plan: ë™ì  ê³„íš ìƒì„± (Planner)
        3. execute: ì—ì´ì „íŠ¸ ì‹¤í–‰
        4. finalize: ì‘ì—… ì™„ë£Œ ì²˜ë¦¬

        Returns:
            StateGraph: LangGraph ì›Œí¬í”Œë¡œìš°
        """
        workflow = StateGraph(AgentState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("setup", self._setup_node)
        # workflow.add_node("plan", self._plan_node)
        workflow.add_node("execute", self._execute_node)
        workflow.add_node("finalize", self._finalize_node)

        # ì—£ì§€ ì¶”ê°€
        workflow.set_entry_point("setup")
        # workflow.add_edge("setup", "plan")
        # workflow.add_edge("plan", "execute")
        workflow.add_edge("setup", "execute")
        workflow.add_edge("execute", "finalize")
        workflow.add_edge("finalize", END)

        return workflow

    async def run(
        self,
        git_url: str,
        target_user: str | None = None,
        main_task_uuid: str | None = None,
        main_base_path: str | Path | None = None,
    ) -> AgentState:
        """
        ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            git_url: Git ë ˆí¬ì§€í† ë¦¬ URL
            target_user: íŠ¹ì • ìœ ì € ì´ë©”ì¼ (Noneì´ë©´ ì „ì²´ ë¶„ì„)
            main_task_uuid: ë©€í‹° ë¶„ì„ ì‹œ ë©”ì¸ task UUID (Noneì´ë©´ ë‹¨ì¼ ë¶„ì„)
            main_base_path: ë©€í‹° ë¶„ì„ ì‹œ ë©”ì¸ base path (Noneì´ë©´ ë‹¨ì¼ ë¶„ì„)

        Returns:
            AgentState: ìµœì¢… ìƒíƒœ
        """
        logger.info("ğŸš€ Deep Agents ë¶„ì„ ì‹œì‘ (Pydantic ê¸°ë°˜)")
        logger.info(f"   Git URL: {git_url}")
        logger.info(f"   Target User: {target_user if target_user else 'ì „ì²´ ìœ ì €'}")
        if main_task_uuid:
            logger.info(f"   ë©€í‹° ë¶„ì„ ëª¨ë“œ: {main_task_uuid}")

        # ì´ˆê¸° ìƒíƒœ
        initial_state: AgentState = {
            "task_uuid": str(uuid.uuid4()),
            "git_url": git_url,
            "target_user": target_user,
            "base_path": "",
            "repo_path": None,
            "static_analysis": None,
            "neo4j_ready": False,
            "chromadb_ready": False,
            "todo_list": None,
            "subagent_results": {},
            "final_report_path": None,
            "final_report": None,
            "error_message": None,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "total_commits": 0,
            "total_files": 0,
            "elapsed_time": 0.0,
        }
        
        # ë©€í‹° ë¶„ì„ ëª¨ë“œ ì •ë³´ë¥¼ ìƒíƒœì— ì¶”ê°€ (ë‚´ë¶€ ì‚¬ìš©)
        if main_task_uuid and main_base_path:
            initial_state["_main_task_uuid"] = main_task_uuid
            initial_state["_main_base_path"] = str(main_base_path)

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        config = {"configurable": {"thread_id": initial_state["task_uuid"]}}

        try:
            final_state = await self.app.ainvoke(initial_state, config=config)
            logger.info("âœ… Deep Agents ë¶„ì„ ì™„ë£Œ")
            return final_state
        except Exception as e:
            # ì›Œí¬í”Œë¡œìš° ì™¸ë¶€ì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ DB FAILED ì—…ë°ì´íŠ¸
            logger.exception(f"âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

            if self.db_writer and self.user_id:
                try:
                    from shared.graph_db import AnalysisStatus

                    task_uuid_obj = uuid.UUID(initial_state["task_uuid"])

                    # ê¸°ì¡´ ë ˆì½”ë“œê°€ ìˆìœ¼ë©´ FAILEDë¡œ ì—…ë°ì´íŠ¸
                    existing = await self.db_writer.get_repository_analysis(task_uuid_obj)
                    if existing:
                        await self.db_writer.update_repository_result(
                            task_uuid=task_uuid_obj,
                            result={},
                            status=AnalysisStatus.FAILED,
                            error_message=str(e)
                        )
                        logger.info(f"ğŸ“Š DB FAILED ì—…ë°ì´íŠ¸ ì™„ë£Œ: {initial_state['task_uuid']}")
                    else:
                        # ë ˆì½”ë“œê°€ ì—†ìœ¼ë©´ ìƒì„± (setup_node ì‹¤íŒ¨ ì‹œ)
                        await self.db_writer.save_repository_analysis(
                            user_id=self.user_id,
                            repository_url=git_url,
                            result={},
                            task_uuid=task_uuid_obj,
                            status=AnalysisStatus.FAILED,
                            error_message=str(e)
                        )
                        logger.info(f"ğŸ“Š DB FAILED ë ˆì½”ë“œ ìƒì„± ì™„ë£Œ: {initial_state['task_uuid']}")
                except Exception as db_err:
                    logger.warning(f"âš ï¸ DB FAILED ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {db_err}")

            # ì—ëŸ¬ ì •ë³´ë¥¼ í¬í•¨í•œ ìƒíƒœ ë°˜í™˜
            initial_state["error_message"] = str(e)
            return initial_state

    async def _setup_node(self, state: AgentState) -> dict[str, Any]:
        """
        ì‘ì—… ì´ˆê¸°í™” ë…¸ë“œ

        ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
        Taskë³„ ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        DB ë ˆì½”ë“œ ìƒì„± (PROCESSING ìƒíƒœ)
        """
        logger.info("âš™ï¸  Setup: ì‘ì—… ì´ˆê¸°í™”")

        task_uuid = state["task_uuid"]
        
        # ë©€í‹° ë¶„ì„ ëª¨ë“œì¸ì§€ í™•ì¸
        main_task_uuid = state.get("_main_task_uuid")
        main_base_path = state.get("_main_base_path")
        is_multi_analysis = bool(main_task_uuid and main_base_path)
        
        # shared/storageì˜ create_storage_backendë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ë¡œ ìƒì„±
        from shared.storage import create_storage_backend
        from shared.storage.local_store import LocalStorageBackend
        
        backend = create_storage_backend(
            task_uuid=task_uuid,
            base_path=None,  # ìë™ ìƒì„±
            is_multi_analysis=is_multi_analysis,
            main_task_uuid=main_task_uuid if is_multi_analysis else None,
        )
        
        # base_path ì¶”ì¶œ (ë¡œì»¬/S3 í™˜ê²½ì— ë§ê²Œ)
        if isinstance(backend, LocalStorageBackend):
            base_path = Path(backend.base_path)
            base_path.mkdir(parents=True, exist_ok=True)
        else:
            # S3 í™˜ê²½: base_pathëŠ” ë¬¸ìì—´ë¡œ ê´€ë¦¬
            base_path = backend.base_path
            logger.info(f"   S3 ê²½ë¡œ: s3://{backend.bucket_name}/{base_path}")
        
        if is_multi_analysis:
            logger.info(f"   ë©€í‹° ë¶„ì„ ëª¨ë“œ: {main_task_uuid}")

        # Taskë³„ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
        if isinstance(backend, LocalStorageBackend):
            log_dir = base_path / "logs"
            log_dir.mkdir(exist_ok=True)
            task_log_file = log_dir / "combined.log"
        else:
            # S3 í™˜ê²½: ë¡œì»¬ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
            import tempfile
            log_dir = Path(tempfile.mkdtemp(prefix=f"deep-agents-logs-{task_uuid}-"))
            task_log_file = log_dir / "combined.log"
        
        # Taskë³„ í†µí•© ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        task_handler = logging.FileHandler(task_log_file, encoding="utf-8")
        task_handler.setFormatter(
            logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        )
        task_handler.setLevel(logging.INFO)

        # ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ ì¶”ê°€
        root_logger = logging.getLogger()
        root_logger.addHandler(task_handler)

        # Task UUIDë¥¼ í•¸ë“¤ëŸ¬ì— ì €ì¥ (ë‚˜ì¤‘ì— ì œê±°í•˜ê¸° ìœ„í•´)
        task_handler.task_uuid = task_uuid

        logger.info(f"   ì‘ì—… ê²½ë¡œ: {base_path}")
        logger.info(f"   ë¡œê·¸ íŒŒì¼: {task_log_file}")

        # DB Writerê°€ ìˆìœ¼ë©´ ë¹ˆ ë ˆì½”ë“œ ìƒì„± (PROCESSING ìƒíƒœ)
        if self.db_writer and self.user_id:
            try:
                from shared.graph_db import AnalysisStatus

                git_url = state["git_url"]
                task_uuid_obj = uuid.UUID(task_uuid)
                
                # main_task_uuid ì¶”ì¶œ (ë©€í‹° ë¶„ì„ ì‹œ)
                main_task_uuid_obj = None
                if state.get("_main_task_uuid"):
                    main_task_uuid_obj = uuid.UUID(state["_main_task_uuid"])

                # ê¸°ì¡´ ë ˆì½”ë“œ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                existing = await self.db_writer.get_repository_analysis(task_uuid_obj)
                if existing:
                    logger.info(f"ğŸ“Š ê¸°ì¡´ DB ë ˆì½”ë“œ ì¬ì‚¬ìš©: {task_uuid} (ìƒíƒœ: {existing.status.value})")
                else:
                    # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                    await self.db_writer.save_repository_analysis(
                        user_id=self.user_id,
                        repository_url=git_url,
                        result={},  # ë¹ˆ ê²°ê³¼
                        task_uuid=task_uuid_obj,
                        main_task_uuid=main_task_uuid_obj,  # ë©€í‹° ë¶„ì„ ì‹œ ì¢…í•© ë¶„ì„ê³¼ ì—°ê²°
                        status=AnalysisStatus.PROCESSING,
                        error_message=None
                    )
                    logger.info(f"ğŸ“Š DB ë ˆì½”ë“œ ìƒì„± ì™„ë£Œ: {task_uuid} (PROCESSING, main_task: {main_task_uuid_obj})")
            except Exception as e:
                logger.warning(f"âš ï¸ DB ë ˆì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}")

        return {
            "base_path": str(base_path),
            "updated_at": datetime.now().isoformat(),
        }

    async def _plan_node(self, state: AgentState) -> dict[str, Any]:
        """
        ê³„íš ìƒì„± ë…¸ë“œ

        Plannerë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  TodoList ìƒì„±
        """
        logger.info("ğŸ“‹ Plan: ì‘ì—… ê³„íš ìƒì„±")

        # Planner ì‹¤í–‰
        plan_result = await self.planner.create_plan(state)

        return plan_result

    async def _execute_node(self, state: AgentState) -> dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ ì‹¤í–‰ ë…¸ë“œ (Pydantic ê¸°ë°˜)

        Level 1 ë³‘ë ¬ ì²˜ë¦¬: ë…ë¦½ì ì¸ ì—ì´ì „íŠ¸ë¥¼ ë³‘ë ¬ ì‹¤í–‰
        """
        logger.info("âš¡ Execute: ì—ì´ì „íŠ¸ ì‹¤í–‰ (Pydantic)")

        try:
            task_uuid = state["task_uuid"]
            base_path = Path(state["base_path"])
            git_url = state["git_url"]
            target_user = state.get("target_user")

            # ResultStore ì´ˆê¸°í™”
            store = ResultStore(task_uuid, base_path)

            # Level 1-1: RepoCloner (ìˆœì°¨)
            logger.info("ğŸ“¥ Level 1-1: RepoCloner ì‹¤í–‰")
            repo_cloner = RepoClonerAgent()
            repo_ctx = RepoClonerContext(
                task_uuid=task_uuid,
                git_url=git_url,
                base_path=str(base_path),
                result_store_path=str(store.results_dir),
                user_id=str(self.user_id) if self.user_id else None,
                db_writer=self.db_writer,
            )
            repo_response = await repo_cloner.run(repo_ctx)

            if repo_response.status != "success":
                return {
                    "error_message": f"RepoCloner ì‹¤íŒ¨: {repo_response.error}",
                    "updated_at": datetime.now().isoformat(),
                }

            # ResultStoreì— ì €ì¥
            store.save_result("repo_cloner", repo_response)

            repo_path = repo_response.repo_path

            # Level 1-2: ë³‘ë ¬ ì‹¤í–‰ (StaticAnalyzer, CommitAnalyzer, CodeRAGBuilder)
            logger.info("ğŸ“Š Level 1-2: ë³‘ë ¬ ë¶„ì„ ì‹œì‘")

            static_analyzer = StaticAnalyzerAgent()
            commit_analyzer = CommitAnalyzerAgent(
                neo4j_uri=self.neo4j_uri,
                neo4j_user=self.neo4j_user,
                neo4j_password=self.neo4j_password,
            )
            code_rag_builder = CodeRAGBuilderAgent()

            # Pydantic Context ìƒì„±
            static_ctx = StaticAnalyzerContext(
                task_uuid=task_uuid,
                repo_path=repo_path,
                result_store_path=str(store.results_dir),
            )
            commit_ctx = CommitAnalyzerContext(
                task_uuid=task_uuid,
                repo_path=repo_path,
                git_url=git_url,  # Repository Isolationìš©
                target_user=target_user,
                result_store_path=str(store.results_dir),
            )
            # ChromaDB persist ë””ë ‰í† ë¦¬: í™˜ê²½ ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ data_dir/chroma_db
            chromadb_persist_dir = os.getenv(
                "CHROMADB_PERSIST_DIR", str(self.data_dir / "chroma_db")
            )

            code_rag_ctx = CodeRAGBuilderContext(
                task_uuid=task_uuid,
                repo_path=repo_path,
                persist_dir=chromadb_persist_dir,
                result_store_path=str(store.results_dir),
            )

            static_response, commit_response, rag_response = await asyncio.gather(
                static_analyzer.run(static_ctx),
                commit_analyzer.run(commit_ctx),
                code_rag_builder.run(code_rag_ctx),
            )

            # ResultStoreì— ì €ì¥
            store.save_result("static_analyzer", static_response)
            store.save_result("commit_analyzer", commit_response)
            store.save_result("code_rag_builder", rag_response)

            # Pydantic Response â†’ dict ë³€í™˜ (ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
            static_result = static_response.model_dump()
            commit_result = commit_response.model_dump()
            rag_result = rag_response.model_dump()

            # CommitAnalyzer ì‹¤íŒ¨ ì‹œ ì‘ì—… ì¢…ë£Œ
            if commit_response.status != "success":
                error_msg = f"CommitAnalyzer ì‹¤íŒ¨: {commit_result.get('error', 'Unknown error')}"
                logger.error(f"âŒ {error_msg}")
                logger.error("âŒ ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return {
                    "error_message": error_msg,
                    "static_analysis": static_result,
                    "neo4j_ready": False,
                    "chromadb_ready": rag_result.get("status") == "success",
                    "total_commits": 0,
                    "total_files": static_result.get("loc_stats", {}).get("total_files", 0),
                    "subagent_results": {
                        "repo_cloner": {
                            "status": "success",
                            "path": "results/repo_cloner.json",
                        },
                        "static_analyzer": {
                            "status": static_response.status,
                            "path": "results/static_analyzer.json",
                        },
                        "commit_analyzer": {
                            "status": commit_response.status,
                            "path": "results/commit_analyzer.json",
                        },
                        "code_rag_builder": {
                            "status": rag_response.status,
                            "path": "results/code_rag_builder.json",
                        },
                    },
                    "updated_at": datetime.now().isoformat(),
                }

            # Level 1-3: CommitEvaluator (ë³‘ë ¬) - CommitAnalyzer ì„±ê³µ ì‹œì—ë§Œ ì‹¤í–‰
            logger.info("ğŸ“ Level 1-3: CommitEvaluator ì‹¤í–‰")
            commit_evaluations = []
            
            # Neo4jì—ì„œ ìœ ì € ì»¤ë°‹ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            if target_user:
                    # Repository ID ìƒì„± (ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ í•„ìˆ˜)
                    from shared.utils.repo_utils import generate_repo_id

                    repo_id = generate_repo_id(git_url)

                    user_commits = await get_user_commits.ainvoke(
                        {
                            "user_email": target_user,
                            "repo_id": repo_id,  # ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ í•„ìˆ˜
                            "limit": 100,
                            "neo4j_uri": self.neo4j_uri,
                            "neo4j_user": self.neo4j_user,
                            "neo4j_password": self.neo4j_password,
                        }
                    )
                    # None ì²´í¬
                    if user_commits is None:
                        user_commits = []
                        logger.warning(f"âš ï¸ íƒ€ê²Ÿ ìœ ì € {target_user}ì˜ ì»¤ë°‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    logger.info(f"ğŸ” íƒ€ê²Ÿ ìœ ì € {target_user}: {len(user_commits)}ê°œ ì»¤ë°‹")
            else:
                # ì „ì²´ ìœ ì €ì˜ ê²½ìš°: ëª¨ë“  ìœ ì €ì˜ ìµœê·¼ ì»¤ë°‹ ìƒ˜í”Œë§
                from shared.tools.neo4j_tools import query_graph
                from shared.utils.repo_utils import generate_repo_id

                # Repository ID ìƒì„± (ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ í•„ìˆ˜)
                repo_id = generate_repo_id(git_url)

                # 1. ëª¨ë“  ìœ ì € ì´ë©”ì¼ ê°€ì ¸ì˜¤ê¸° (repo_id í•„í„°ë§)
                all_users_query = f"""
                MATCH (u:User)-[:COMMITTED]->(c:Commit)
                WHERE u.repo_id = $repo_id AND c.repo_id = $repo_id
                RETURN DISTINCT u.email AS email, count(c) AS commit_count
                ORDER BY commit_count DESC
                """
                all_users = await query_graph.ainvoke(
                    {
                        "cypher_query": all_users_query,
                        "parameters": {"repo_id": repo_id},
                        "repo_id": repo_id,
                        "neo4j_uri": self.neo4j_uri,
                        "neo4j_user": self.neo4j_user,
                        "neo4j_password": self.neo4j_password,
                    }
                )

                # None ì²´í¬
                if all_users is None:
                    all_users = []
                    logger.warning("âš ï¸ ìœ ì € ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                logger.info(f"ğŸ” ì „ì²´ {len(all_users)}ëª…ì˜ ìœ ì € ë°œê²¬")

                # 2. ê° ìœ ì €ì˜ ìµœê·¼ ì»¤ë°‹ ìƒ˜í”Œë§ (ìœ ì €ë‹¹ ìµœëŒ€ 20ê°œ)
                user_commits = []
                for user_info in all_users:
                    user_email = user_info["email"]
                    user_sample = await get_user_commits.ainvoke(
                        {
                            "user_email": user_email,
                            "repo_id": repo_id,  # ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ í•„ìˆ˜
                            "limit": 20,
                            "neo4j_uri": self.neo4j_uri,
                            "neo4j_user": self.neo4j_user,
                            "neo4j_password": self.neo4j_password,
                        }
                    )
                    # None ì²´í¬
                    if user_sample is None:
                        user_sample = []
                    # ê° ì»¤ë°‹ì— author_email ì¶”ê°€
                    for commit in user_sample:
                        commit["author_email"] = user_email
                    user_commits.extend(user_sample)

                logger.info(f"ğŸ” ì „ì²´ ìƒ˜í”Œë§: {len(user_commits)}ê°œ ì»¤ë°‹ (ìœ ì €ë‹¹ ìµœëŒ€ 20ê°œ)")

            # CommitEvaluator ë³‘ë ¬ ì‹¤í–‰ (ì„¤ì •ì—ì„œ ë°°ì¹˜ í¬ê¸° ê°€ì ¸ì˜¤ê¸°) - Pydantic ê¸°ë°˜
            commit_evaluator = CommitEvaluatorAgent(llm=self.haiku_llm)
            total_evaluated = 0  # í†µê³„ìš© ì¹´ìš´í„°ë§Œ ìœ ì§€

            batch_size = self.config.commit_evaluator_batch_size
            for i in range(0, len(user_commits), batch_size):
                    batch = user_commits[i : i + batch_size]

                    # Pydantic Context ìƒì„±
                    batch_contexts = [
                        CommitEvaluatorContext(
                            task_uuid=task_uuid,
                            commit_hash=commit["hash"],
                            user=target_user if target_user else commit.get("author_email", ""),
                            git_url=git_url,  # Repository Isolationìš©
                            neo4j_uri=self.neo4j_uri,
                            neo4j_user=self.neo4j_user,
                            neo4j_password=self.neo4j_password,
                        )
                        for commit in batch
                    ]

                    batch_responses = await asyncio.gather(
                        *[commit_evaluator.run(ctx) for ctx in batch_contexts]
                    )

                    # ë°°ì¹˜ ê²°ê³¼ë¥¼ ResultStoreì— ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: ì¦‰ì‹œ ì €ì¥)
                    batch_id = i // batch_size
                    store.save_batched_result(
                        "commit_evaluator",
                        batch_id,
                        [resp.model_dump() for resp in batch_responses],
                    )

                    # ë©”ëª¨ë¦¬ í•´ì œ: batch_responsesëŠ” ë” ì´ìƒ í•„ìš” ì—†ìŒ
                    total_evaluated += len(batch_responses)
                    del batch_responses

                    logger.info(
                        f"   {i + len(batch)}/{len(user_commits)} ì»¤ë°‹ í‰ê°€ ì™„ë£Œ (ë°°ì¹˜ {batch_id} ì €ì¥ë¨)"
                    )

            # Level 1-4: UserAggregator - Pydantic ê¸°ë°˜ (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)
            logger.info("ğŸ‘¤ Level 1-4: UserAggregator ì‹¤í–‰")

            # CommitEvaluator ë°°ì¹˜ê°€ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
            batched_agents = store.list_batched_agents()
            has_commit_evaluations = "commit_evaluator" in batched_agents

            if has_commit_evaluations:
                user_aggregator = UserAggregatorAgent()
                # UserAggregatorê°€ ResultStoreì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¡œë“œí•˜ë¯€ë¡œ commit_evaluations ì „ë‹¬ ë¶ˆí•„ìš”
                user_agg_ctx = UserAggregatorContext(
                    task_uuid=task_uuid,
                    user=target_user,  # Noneì´ë©´ ì „ì²´ ìœ ì € (validatorì—ì„œ í—ˆìš©)
                    commit_evaluations=None,  # ResultStoreì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ
                    result_store_path=str(store.results_dir),
                )
                user_agg_response = await user_aggregator.run(user_agg_ctx)
                store.save_result("user_aggregator", user_agg_response)
                user_agg_result = user_agg_response.model_dump()
            else:
                user_agg_result = {
                    "status": "failed",
                    "user": target_user if target_user else None,
                    "aggregate_stats": {},
                    "error": "ì»¤ë°‹ í‰ê°€ ê²°ê³¼ ì—†ìŒ",
                }

            # Level 1-4.5: UserSkillProfiler - Pydantic ê¸°ë°˜
            logger.info("ğŸ¯ Level 1-4.5: UserSkillProfiler ì‹¤í–‰")

            # â„¹ï¸ skill_chartsëŠ” ë…ë¦½ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸(server/skill_charts_builder.py)ë¡œ ì‚¬ì „ êµ¬ì¶•ë¨
            # â„¹ï¸ get_skill_chroma_client()ëŠ” ì›ê²© ChromaDB(CHROMADB_HOST)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ persist_dir ë¶ˆí•„ìš”
            if rag_result["status"] == "success":
                logger.info(f"âœ… ì½”ë“œ RAG êµ¬ì¶• ì™„ë£Œ: {rag_result['total_chunks']} chunks")

                # ChromaDB persist ë””ë ‰í† ë¦¬ (ì½”ë“œ ì»¬ë ‰ì…˜ìš©)
                chromadb_persist_dir = os.getenv(
                    "CHROMADB_PERSIST_DIR", str(self.data_dir / "chroma_db")
                )

                # target_userê°€ Noneì´ë©´ "ALL_USERS"ë¡œ ì²˜ë¦¬ (UserAggregatorì™€ ë™ì¼)
                user_for_skill_profiler = target_user if target_user else "ALL_USERS"

                user_skill_profiler = UserSkillProfilerAgent()
                skill_profile_ctx = UserSkillProfilerContext(
                    task_uuid=task_uuid,
                    user=user_for_skill_profiler,
                    # persist_dirëŠ” ê¸°ë³¸ê°’ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ì›ê²© ChromaDB ì‚¬ìš©ìœ¼ë¡œ ë¬´ì‹œë¨)
                    code_persist_dir=chromadb_persist_dir,  # ì½”ë“œ ì»¬ë ‰ì…˜ìš© ë””ë ‰í† ë¦¬
                    result_store_path=str(store.results_dir),
                )
                skill_profile_response = await user_skill_profiler.run(skill_profile_ctx)
                store.save_result("user_skill_profiler", skill_profile_response)
                skill_profile_result = skill_profile_response.model_dump()
            else:
                skill_profile_result = {
                    "status": "skipped",
                    "user": target_user if target_user else "ALL_USERS",
                    "skill_profile": {},
                    "error": "RAG not ready",
                }

            # Level 1-5: Reporter - Pydantic ê¸°ë°˜
            logger.info("ğŸ“ Level 1-5: Reporter ì‹¤í–‰")

            reporter = ReporterAgent(llm=self.sonnet_llm)
            # ReporterëŠ” ResultStoreì—ì„œ ì§ì ‘ ë¡œë“œí•˜ë¯€ë¡œ dict ì „ë‹¬ ë¶ˆí•„ìš” (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹ˆ dict ì „ë‹¬)
            reporter_ctx = ReporterContext(
                task_uuid=task_uuid,
                base_path=str(base_path),
                git_url=git_url,
                static_analysis={},  # ResultStoreì—ì„œ ë¡œë“œí•˜ë¯€ë¡œ ë¹ˆ dict
                user_aggregate={},  # ResultStoreì—ì„œ ë¡œë“œí•˜ë¯€ë¡œ ë¹ˆ dict
                result_store_path=str(store.results_dir),
            )
            report_response = await reporter.run(reporter_ctx)
            store.save_result("reporter", report_response)
            report_result = report_response.model_dump()

            # ìµœì¢… ê²°ê³¼ ë°˜í™˜ (ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ)
            return {
                "repo_path": repo_path,
                "static_analysis": static_result,  # Reporter í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
                "neo4j_ready": commit_response.status == "success",
                "chromadb_ready": (
                    rag_result["status"] == "success"
                ),  # skill_chartsëŠ” ë…ë¦½ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‚¬ì „ êµ¬ì¶•
                "total_commits": commit_result.get("total_commits", 0),
                "total_files": static_result.get("loc_stats", {}).get("total_files", 0),
                "subagent_results": {
                    "repo_cloner": {
                        "status": repo_response.status,
                        "path": "results/repo_cloner.json",
                    },
                    "static_analyzer": {
                        "status": static_response.status,
                        "path": "results/static_analyzer.json",
                    },
                    "commit_analyzer": {
                        "status": commit_response.status,
                        "path": "results/commit_analyzer.json",
                    },
                    "code_rag_builder": {
                        "status": rag_response.status,
                        "path": "results/code_rag_builder.json",
                    },
                    # skill_charts_rag_builderëŠ” ë…ë¦½ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë¶„ë¦¬ë¨
                    "user_skill_profiler": {
                        "status": skill_profile_result.get("status", "skipped"),
                        "path": "results/user_skill_profiler.json",
                    },
                    "user_aggregator": {
                        "status": user_agg_result.get("status", "failed"),
                        "path": "results/user_aggregator.json",
                    },
                    "reporter": {"status": report_response.status, "path": "results/reporter.json"},
                },
                "final_report_path": report_result.get("report_path"),
                "updated_at": datetime.now().isoformat(),
                "error_message": None,
            }

        except Exception as e:
            logger.error(f"âŒ Execute ë…¸ë“œ ì—ëŸ¬: {e}")
            import traceback

            logger.error(f"ìƒì„¸ Traceback:\n{traceback.format_exc()}")
            return {
                "error_message": str(e),
                "updated_at": datetime.now().isoformat(),
            }

    async def _finalize_node(self, state: AgentState) -> dict[str, Any]:
        """
        ìµœì¢… ì²˜ë¦¬ ë…¸ë“œ

        ê²°ê³¼ ì €ì¥ ë° ë¦¬í¬íŠ¸ ìƒì„±
        Taskë³„ ë¡œê·¸ í•¸ë“¤ëŸ¬ ì œê±°
        DB ê²°ê³¼ ì—…ë°ì´íŠ¸ (COMPLETED ë˜ëŠ” FAILED)
        """
        logger.info("ğŸ‰ Finalize: ì‘ì—… ì™„ë£Œ ì²˜ë¦¬")

        task_uuid = state["task_uuid"]
        base_path = Path(state["base_path"])

        # ì„ì‹œ ë¦¬í¬íŠ¸
        report_content = f"""# ì½”ë“œ ë¶„ì„ ë¦¬í¬íŠ¸ (Pydantic ê¸°ë°˜)

**Task UUID**: {state['task_uuid']}
**Git URL**: {state['git_url']}
**Target User**: {state.get('target_user', 'ì „ì²´ ìœ ì €')}

## ì‹¤í–‰ ê²°ê³¼

ì„œë¸Œì—ì´ì „íŠ¸ ê²°ê³¼: {state.get('subagent_results', {})}

**ìƒì„± ì‹œê°„**: {datetime.now().isoformat()}
"""

        # ResultStoreë¥¼ í†µí•´ ë¦¬í¬íŠ¸ ì €ì¥ (S3 ë˜ëŠ” ë¡œì»¬)
        try:
            from shared.storage import ResultStore

            store = ResultStore(task_uuid, base_path)
            report_path = store.save_report("final_report.md", report_content)
            logger.info(f"   ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ ResultStore ì €ì¥ ì‹¤íŒ¨, ë¡œì»¬ì— ì €ì¥: {e}")
            # Fallback: ë¡œì»¬ì— ì €ì¥
            report_path = base_path / "final_report.md"
            report_path.write_text(report_content, encoding="utf-8")
            logger.info(f"   ë¦¬í¬íŠ¸ ì €ì¥ (ë¡œì»¬): {report_path}")

        # ë¡œê·¸ íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ (ì‘ì—… ì™„ë£Œ ì‹œ)
        log_dir = base_path / "logs"
        if log_dir.exists():
            try:
                from shared.storage import ResultStore

                store = ResultStore(task_uuid, base_path)
                uploaded_logs = store.upload_log_directory(log_dir)
                if uploaded_logs:
                    logger.info(f"   ë¡œê·¸ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {len(uploaded_logs)}ê°œ íŒŒì¼")
            except Exception as e:
                logger.warning(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ë””ë²„ê·¸ ë¡œê·¸ ë””ë ‰í† ë¦¬ë„ S3ì— ì—…ë¡œë“œ
        debug_dir = base_path / "debug"
        if debug_dir.exists():
            try:
                from shared.storage import ResultStore

                store = ResultStore(task_uuid, base_path)
                # debug ë””ë ‰í† ë¦¬ë¥¼ logs/debug/ ì•„ë˜ì— ì—…ë¡œë“œ
                uploaded_debug = store.upload_log_directory(debug_dir, remote_subdir="debug")
                if uploaded_debug:
                    logger.info(f"   ë””ë²„ê·¸ ë¡œê·¸ ì—…ë¡œë“œ ì™„ë£Œ: {len(uploaded_debug)}ê°œ íŒŒì¼")
            except Exception as e:
                logger.warning(f"âš ï¸ ë””ë²„ê·¸ ë¡œê·¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        # í† í° ì‚¬ìš©ëŸ‰ ì „ì²´ ì§‘ê³„ ì¶œë ¥
        logger.info("")
        TokenTracker.print_summary()

        # Taskë³„ ë¡œê·¸ í•¸ë“¤ëŸ¬ ì œê±° (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        root_logger = logging.getLogger()
        handlers_to_remove = [
            h for h in root_logger.handlers if hasattr(h, 'task_uuid') and h.task_uuid == task_uuid
        ]
        for handler in handlers_to_remove:
            handler.close()
            root_logger.removeHandler(handler)
            logger.debug(f"   ë¡œê·¸ í•¸ë“¤ëŸ¬ ì œê±°: {task_uuid}")

        # DB Writerê°€ ìˆìœ¼ë©´ ê²°ê³¼ ì—…ë°ì´íŠ¸
        if self.db_writer and self.user_id:
            try:
                from shared.graph_db import AnalysisStatus
                from shared.storage import ResultStore

                # ResultStoreì—ì„œ user_aggregator ê²°ê³¼ ë¡œë“œ
                store = ResultStore(task_uuid, base_path)
                user_agg_result = store.load_result("user_aggregator", UserAggregatorResponse)

                # ì—ëŸ¬ ì—¬ë¶€ í™•ì¸
                has_error = state.get("error_message") is not None
                status = AnalysisStatus.FAILED if has_error else AnalysisStatus.COMPLETED
                error_message = state.get("error_message")

                task_uuid_obj = uuid.UUID(task_uuid)
                
                # main_task_uuid ì¶”ì¶œ (ë©€í‹° ë¶„ì„ ì‹œ)
                main_task_uuid_obj = None
                if state.get("_main_task_uuid"):
                    main_task_uuid_obj = uuid.UUID(state["_main_task_uuid"])

                # DB ì—…ë°ì´íŠ¸
                await self.db_writer.update_repository_result(
                    task_uuid=task_uuid_obj,
                    result=user_agg_result.model_dump() if user_agg_result else {},
                    main_task_uuid=main_task_uuid_obj,  # ë©€í‹° ë¶„ì„ ì‹œ ì¢…í•© ë¶„ì„ê³¼ ì—°ê²°
                    status=status,
                    error_message=error_message
                )
                logger.info(f"ğŸ“Š DB ê²°ê³¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {task_uuid} ({status.value}, main_task: {main_task_uuid_obj})")
            except Exception as e:
                logger.warning(f"âš ï¸ DB ê²°ê³¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

        return {
            "final_report_path": str(report_path),
            "final_report": report_content,
            "updated_at": datetime.now().isoformat(),
        }
