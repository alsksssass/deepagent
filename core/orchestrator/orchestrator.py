"""
Deep Agents Orchestrator

ì „ì²´ ì›Œí¬í”Œë¡œìš° ì¡°ìœ¨ ë° ì—ì´ì „íŠ¸ ì‹¤í–‰ (Pydantic ê¸°ë°˜)
"""

import logging
import asyncio
import os
from typing import Any
from datetime import datetime
from pathlib import Path
import uuid

from langchain_aws import ChatBedrockConverse
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from core.state import AgentState
from core.planner.agent import PlannerAgent
from shared.storage import ResultStore
from shared.utils.token_tracker import TokenTracker
from .config_loader import OrchestratorConfig

# Agents (ìƒˆ ì•„í‚¤í…ì²˜)
from agents.repo_cloner import RepoClonerAgent, RepoClonerContext
from agents.static_analyzer import StaticAnalyzerAgent, StaticAnalyzerContext
from agents.commit_analyzer import CommitAnalyzerAgent, CommitAnalyzerContext
from agents.commit_evaluator import CommitEvaluatorAgent, CommitEvaluatorContext
from agents.user_aggregator import UserAggregatorAgent, UserAggregatorContext
from agents.reporter import ReporterAgent, ReporterContext

# Agents (Phase 5 ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ)
from agents.code_rag_builder import CodeRAGBuilderAgent, CodeRAGBuilderContext
from agents.user_skill_profiler import UserSkillProfilerAgent, UserSkillProfilerContext

# Tools (for CommitEvaluator)
from shared.tools.neo4j_tools import get_user_commits

logger = logging.getLogger(__name__)


class DeepAgentOrchestrator:
    """
    Deep Agents ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

    LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ê³  ì—ì´ì „íŠ¸ë¥¼ ì¡°ìœ¨ (Pydantic ê¸°ë°˜)
    """

    def __init__(
        self,
        sonnet_llm: ChatBedrockConverse,
        haiku_llm: ChatBedrockConverse,
        data_dir: Path,
        neo4j_uri: str | None = None,
        neo4j_user: str | None = None,
        neo4j_password: str | None = None,
        config_path: Path | None = None,
    ):
        self.sonnet_llm = sonnet_llm
        self.haiku_llm = haiku_llm
        self.data_dir = data_dir
        
        # Neo4j ì„¤ì •: í™˜ê²½ ë³€ìˆ˜ ìš°ì„ , íŒŒë¼ë¯¸í„° ì „ë‹¬ ì‹œ ì˜¤ë²„ë¼ì´ë“œ
        self.neo4j_uri = neo4j_uri or os.getenv("NEO4J_URI", "bolt://localhost:7687")
        self.neo4j_user = neo4j_user or os.getenv("NEO4J_USER", "neo4j")
        self.neo4j_password = neo4j_password or os.getenv("NEO4J_PASSWORD", "password")

        # Orchestrator ì„¤ì • ë¡œë“œ
        self.config = OrchestratorConfig(config_path)

        # Planner
        self.planner = PlannerAgent(llm=sonnet_llm)

        # LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
        self.workflow = self._create_workflow()
        self.app = self.workflow.compile(checkpointer=MemorySaver())

    def _create_workflow(self) -> StateGraph:
        """
        LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±

        ë…¸ë“œ:
        1. setup: ì‘ì—… ì´ˆê¸°í™”
        2. plan: ë™ì  ê³„íš ìƒì„± (Planner)
        3. execute: ì—ì´ì „íŠ¸ ì‹¤í–‰
        4. finalize: ì‘ì—… ì™„ë£Œ ì²˜ë¦¬

        Returns:
            StateGraph: LangGraph ì›Œí¬í”Œë¡œìš°
        """
        workflow = StateGraph(AgentState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("setup", self._setup_node)
        workflow.add_node("plan", self._plan_node)
        workflow.add_node("execute", self._execute_node)
        workflow.add_node("finalize", self._finalize_node)

        # ì—£ì§€ ì¶”ê°€
        workflow.set_entry_point("setup")
        workflow.add_edge("setup", "plan")
        workflow.add_edge("plan", "execute")
        workflow.add_edge("execute", "finalize")
        workflow.add_edge("finalize", END)

        return workflow

    async def run(
        self,
        git_url: str,
        target_user: str | None = None,
    ) -> AgentState:
        """
        ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            git_url: Git ë ˆí¬ì§€í† ë¦¬ URL
            target_user: íŠ¹ì • ìœ ì € ì´ë©”ì¼ (Noneì´ë©´ ì „ì²´ ë¶„ì„)

        Returns:
            AgentState: ìµœì¢… ìƒíƒœ
        """
        logger.info("ğŸš€ Deep Agents ë¶„ì„ ì‹œì‘ (Pydantic ê¸°ë°˜)")
        logger.info(f"   Git URL: {git_url}")
        logger.info(f"   Target User: {target_user if target_user else 'ì „ì²´ ìœ ì €'}")

        # ì´ˆê¸° ìƒíƒœ
        initial_state: AgentState = {
            "task_uuid": str(uuid.uuid4()),
            "git_url": git_url,
            "target_user": target_user,
            "base_path": "",
            "repo_path": None,
            "static_analysis": None,
            "neo4j_ready": False,
            "chromadb_ready": False,
            "todo_list": None,
            "subagent_results": {},
            "final_report_path": None,
            "final_report": None,
            "error_message": None,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "total_commits": 0,
            "total_files": 0,
            "elapsed_time": 0.0,
        }

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        config = {"configurable": {"thread_id": initial_state["task_uuid"]}}
        final_state = await self.app.ainvoke(initial_state, config=config)

        logger.info("âœ… Deep Agents ë¶„ì„ ì™„ë£Œ")
        return final_state

    async def _setup_node(self, state: AgentState) -> dict[str, Any]:
        """
        ì‘ì—… ì´ˆê¸°í™” ë…¸ë“œ

        ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
        Taskë³„ ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        """
        logger.info("âš™ï¸  Setup: ì‘ì—… ì´ˆê¸°í™”")

        task_uuid = state["task_uuid"]
        base_path = self.data_dir / "analyze" / task_uuid
        base_path.mkdir(parents=True, exist_ok=True)

        # Taskë³„ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = base_path / "logs"
        log_dir.mkdir(exist_ok=True)
        
        # Taskë³„ í†µí•© ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        task_log_file = log_dir / "combined.log"
        task_handler = logging.FileHandler(task_log_file, encoding="utf-8")
        task_handler.setFormatter(logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        ))
        task_handler.setLevel(logging.INFO)
        
        # ë£¨íŠ¸ ë¡œê±°ì— í•¸ë“¤ëŸ¬ ì¶”ê°€
        root_logger = logging.getLogger()
        root_logger.addHandler(task_handler)
        
        # Task UUIDë¥¼ í•¸ë“¤ëŸ¬ì— ì €ì¥ (ë‚˜ì¤‘ì— ì œê±°í•˜ê¸° ìœ„í•´)
        task_handler.task_uuid = task_uuid

        logger.info(f"   ì‘ì—… ê²½ë¡œ: {base_path}")
        logger.info(f"   ë¡œê·¸ íŒŒì¼: {task_log_file}")

        return {
            "base_path": str(base_path),
            "updated_at": datetime.now().isoformat(),
        }

    async def _plan_node(self, state: AgentState) -> dict[str, Any]:
        """
        ê³„íš ìƒì„± ë…¸ë“œ

        Plannerë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  TodoList ìƒì„±
        """
        logger.info("ğŸ“‹ Plan: ì‘ì—… ê³„íš ìƒì„±")

        # Planner ì‹¤í–‰
        plan_result = await self.planner.create_plan(state)

        return plan_result

    async def _execute_node(self, state: AgentState) -> dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ ì‹¤í–‰ ë…¸ë“œ (Pydantic ê¸°ë°˜)

        Level 1 ë³‘ë ¬ ì²˜ë¦¬: ë…ë¦½ì ì¸ ì—ì´ì „íŠ¸ë¥¼ ë³‘ë ¬ ì‹¤í–‰
        """
        logger.info("âš¡ Execute: ì—ì´ì „íŠ¸ ì‹¤í–‰ (Pydantic)")

        try:
            task_uuid = state["task_uuid"]
            base_path = Path(state["base_path"])
            git_url = state["git_url"]
            target_user = state.get("target_user")

            # ResultStore ì´ˆê¸°í™”
            store = ResultStore(task_uuid, base_path)

            # Level 1-1: RepoCloner (ìˆœì°¨)
            logger.info("ğŸ“¥ Level 1-1: RepoCloner ì‹¤í–‰")
            repo_cloner = RepoClonerAgent()
            repo_ctx = RepoClonerContext(
                task_uuid=task_uuid,
                git_url=git_url,
                base_path=str(base_path),
                result_store_path=str(store.results_dir),
            )
            repo_response = await repo_cloner.run(repo_ctx)

            if repo_response.status != "success":
                return {
                    "error_message": f"RepoCloner ì‹¤íŒ¨: {repo_response.error}",
                    "updated_at": datetime.now().isoformat(),
                }

            # ResultStoreì— ì €ì¥
            store.save_result("repo_cloner", repo_response)

            repo_path = repo_response.repo_path

            # Level 1-2: ë³‘ë ¬ ì‹¤í–‰ (StaticAnalyzer, CommitAnalyzer, CodeRAGBuilder)
            logger.info("ğŸ“Š Level 1-2: ë³‘ë ¬ ë¶„ì„ ì‹œì‘")

            static_analyzer = StaticAnalyzerAgent()
            commit_analyzer = CommitAnalyzerAgent(
                neo4j_uri=self.neo4j_uri,
                neo4j_user=self.neo4j_user,
                neo4j_password=self.neo4j_password,
            )
            code_rag_builder = CodeRAGBuilderAgent()

            # Pydantic Context ìƒì„±
            static_ctx = StaticAnalyzerContext(
                task_uuid=task_uuid,
                repo_path=repo_path,
                result_store_path=str(store.results_dir),
            )
            commit_ctx = CommitAnalyzerContext(
                task_uuid=task_uuid,
                repo_path=repo_path,
                git_url=git_url,  # Repository Isolationìš©
                target_user=target_user,
                result_store_path=str(store.results_dir),
            )
            # ChromaDB persist ë””ë ‰í† ë¦¬: í™˜ê²½ ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ data_dir/chroma_db
            chromadb_persist_dir = os.getenv(
                "CHROMADB_PERSIST_DIR",
                str(self.data_dir / "chroma_db")
            )

            code_rag_ctx = CodeRAGBuilderContext(
                task_uuid=task_uuid,
                repo_path=repo_path,
                persist_dir=chromadb_persist_dir,
                result_store_path=str(store.results_dir),
            )

            static_response, commit_response, rag_response = await asyncio.gather(
                static_analyzer.run(static_ctx),
                commit_analyzer.run(commit_ctx),
                code_rag_builder.run(code_rag_ctx),
            )

            # ResultStoreì— ì €ì¥
            store.save_result("static_analyzer", static_response)
            store.save_result("commit_analyzer", commit_response)
            store.save_result("code_rag_builder", rag_response)

            # Pydantic Response â†’ dict ë³€í™˜ (ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
            static_result = static_response.model_dump()
            commit_result = commit_response.model_dump()
            rag_result = rag_response.model_dump()

            # Level 1-3: CommitEvaluator (ë³‘ë ¬)
            logger.info("ğŸ“ Level 1-3: CommitEvaluator ì‹¤í–‰")

            if commit_response.status != "success":
                logger.warning("CommitAnalyzer ì‹¤íŒ¨, CommitEvaluator ìŠ¤í‚µ")
                commit_evaluations = []
            else:
                # Neo4jì—ì„œ ìœ ì € ì»¤ë°‹ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                if target_user:
                    # Repository ID ìƒì„± (ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ í•„ìˆ˜)
                    from shared.utils.repo_utils import generate_repo_id
                    repo_id = generate_repo_id(git_url)
                    
                    user_commits = await get_user_commits.ainvoke({
                        "user_email": target_user,
                        "repo_id": repo_id,  # ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ í•„ìˆ˜
                        "limit": 100,
                        "neo4j_uri": self.neo4j_uri,
                        "neo4j_user": self.neo4j_user,
                        "neo4j_password": self.neo4j_password,
                    })
                    logger.info(f"ğŸ” íƒ€ê²Ÿ ìœ ì € {target_user}: {len(user_commits)}ê°œ ì»¤ë°‹")
                else:
                    # ì „ì²´ ìœ ì €ì˜ ê²½ìš°: ëª¨ë“  ìœ ì €ì˜ ìµœê·¼ ì»¤ë°‹ ìƒ˜í”Œë§
                    from shared.tools.neo4j_tools import query_graph
                    from shared.utils.repo_utils import generate_repo_id

                    # Repository ID ìƒì„± (ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ í•„ìˆ˜)
                    repo_id = generate_repo_id(git_url)

                    # 1. ëª¨ë“  ìœ ì € ì´ë©”ì¼ ê°€ì ¸ì˜¤ê¸° (repo_id í•„í„°ë§)
                    all_users_query = f"""
                    MATCH (u:User)-[:COMMITTED]->(c:Commit)
                    WHERE u.repo_id = $repo_id AND c.repo_id = $repo_id
                    RETURN DISTINCT u.email AS email, count(c) AS commit_count
                    ORDER BY commit_count DESC
                    """
                    all_users = await query_graph.ainvoke({
                        "cypher_query": all_users_query,
                        "parameters": {"repo_id": repo_id},
                        "repo_id": repo_id,
                        "neo4j_uri": self.neo4j_uri,
                        "neo4j_user": self.neo4j_user,
                        "neo4j_password": self.neo4j_password,
                    })

                    logger.info(f"ğŸ” ì „ì²´ {len(all_users)}ëª…ì˜ ìœ ì € ë°œê²¬")

                    # 2. ê° ìœ ì €ì˜ ìµœê·¼ ì»¤ë°‹ ìƒ˜í”Œë§ (ìœ ì €ë‹¹ ìµœëŒ€ 20ê°œ)
                    user_commits = []
                    for user_info in all_users:
                        user_email = user_info["email"]
                        user_sample = await get_user_commits.ainvoke({
                            "user_email": user_email,
                            "repo_id": repo_id,  # ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ í•„ìˆ˜
                            "limit": 20,
                            "neo4j_uri": self.neo4j_uri,
                            "neo4j_user": self.neo4j_user,
                            "neo4j_password": self.neo4j_password,
                        })
                        # ê° ì»¤ë°‹ì— author_email ì¶”ê°€
                        for commit in user_sample:
                            commit["author_email"] = user_email
                        user_commits.extend(user_sample)

                    logger.info(f"ğŸ” ì „ì²´ ìƒ˜í”Œë§: {len(user_commits)}ê°œ ì»¤ë°‹ (ìœ ì €ë‹¹ ìµœëŒ€ 20ê°œ)")

                # CommitEvaluator ë³‘ë ¬ ì‹¤í–‰ (ì„¤ì •ì—ì„œ ë°°ì¹˜ í¬ê¸° ê°€ì ¸ì˜¤ê¸°) - Pydantic ê¸°ë°˜
                commit_evaluator = CommitEvaluatorAgent(llm=self.haiku_llm)
                total_evaluated = 0  # í†µê³„ìš© ì¹´ìš´í„°ë§Œ ìœ ì§€

                batch_size = self.config.commit_evaluator_batch_size
                for i in range(0, len(user_commits), batch_size):
                    batch = user_commits[i : i + batch_size]

                    # Pydantic Context ìƒì„±
                    batch_contexts = [
                        CommitEvaluatorContext(
                            task_uuid=task_uuid,
                            commit_hash=commit["hash"],
                            user=target_user if target_user else commit.get("author_email", ""),
                            git_url=git_url,  # Repository Isolationìš©
                            neo4j_uri=self.neo4j_uri,
                            neo4j_user=self.neo4j_user,
                            neo4j_password=self.neo4j_password,
                        )
                        for commit in batch
                    ]

                    batch_responses = await asyncio.gather(*[
                        commit_evaluator.run(ctx) for ctx in batch_contexts
                    ])

                    # ë°°ì¹˜ ê²°ê³¼ë¥¼ ResultStoreì— ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: ì¦‰ì‹œ ì €ì¥)
                    batch_id = i // batch_size
                    store.save_batched_result(
                        "commit_evaluator",
                        batch_id,
                        [resp.model_dump() for resp in batch_responses]
                    )

                    # ë©”ëª¨ë¦¬ í•´ì œ: batch_responsesëŠ” ë” ì´ìƒ í•„ìš” ì—†ìŒ
                    total_evaluated += len(batch_responses)
                    del batch_responses

                    logger.info(f"   {i + len(batch)}/{len(user_commits)} ì»¤ë°‹ í‰ê°€ ì™„ë£Œ (ë°°ì¹˜ {batch_id} ì €ì¥ë¨)")

            # Level 1-4: UserAggregator - Pydantic ê¸°ë°˜ (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)
            logger.info("ğŸ‘¤ Level 1-4: UserAggregator ì‹¤í–‰")

            # CommitEvaluator ë°°ì¹˜ê°€ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
            batched_agents = store.list_batched_agents()
            has_commit_evaluations = "commit_evaluator" in batched_agents

            if has_commit_evaluations:
                user_aggregator = UserAggregatorAgent()
                # UserAggregatorê°€ ResultStoreì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¡œë“œí•˜ë¯€ë¡œ commit_evaluations ì „ë‹¬ ë¶ˆí•„ìš”
                user_agg_ctx = UserAggregatorContext(
                    task_uuid=task_uuid,
                    user=target_user,  # Noneì´ë©´ ì „ì²´ ìœ ì € (validatorì—ì„œ í—ˆìš©)
                    commit_evaluations=None,  # ResultStoreì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ
                    result_store_path=str(store.results_dir),
                )
                user_agg_response = await user_aggregator.run(user_agg_ctx)
                store.save_result("user_aggregator", user_agg_response)
                user_agg_result = user_agg_response.model_dump()
            else:
                user_agg_result = {
                    "status": "failed",
                    "user": target_user if target_user else None,
                    "aggregate_stats": {},
                    "error": "ì»¤ë°‹ í‰ê°€ ê²°ê³¼ ì—†ìŒ",
                }

            # Level 1-4.5: UserSkillProfiler - Pydantic ê¸°ë°˜
            logger.info("ğŸ¯ Level 1-4.5: UserSkillProfiler ì‹¤í–‰")

            # â„¹ï¸ skill_chartsëŠ” ë…ë¦½ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸(server/skill_charts_builder.py)ë¡œ ì‚¬ì „ êµ¬ì¶•ë¨
            # â„¹ï¸ get_skill_chroma_client()ëŠ” ì›ê²© ChromaDB(CHROMADB_HOST)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ persist_dir ë¶ˆí•„ìš”
            if rag_result["status"] == "success":
                logger.info(f"âœ… ì½”ë“œ RAG êµ¬ì¶• ì™„ë£Œ: {rag_result['total_chunks']} chunks")

                # ChromaDB persist ë””ë ‰í† ë¦¬ (ì½”ë“œ ì»¬ë ‰ì…˜ìš©)
                chromadb_persist_dir = os.getenv(
                    "CHROMADB_PERSIST_DIR",
                    str(self.data_dir / "chroma_db")
                )

                # target_userê°€ Noneì´ë©´ "ALL_USERS"ë¡œ ì²˜ë¦¬ (UserAggregatorì™€ ë™ì¼)
                user_for_skill_profiler = target_user if target_user else "ALL_USERS"

                user_skill_profiler = UserSkillProfilerAgent()
                skill_profile_ctx = UserSkillProfilerContext(
                    task_uuid=task_uuid,
                    user=user_for_skill_profiler,
                    # persist_dirëŠ” ê¸°ë³¸ê°’ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ì›ê²© ChromaDB ì‚¬ìš©ìœ¼ë¡œ ë¬´ì‹œë¨)
                    code_persist_dir=chromadb_persist_dir,  # ì½”ë“œ ì»¬ë ‰ì…˜ìš© ë””ë ‰í† ë¦¬
                    result_store_path=str(store.results_dir),
                )
                skill_profile_response = await user_skill_profiler.run(skill_profile_ctx)
                store.save_result("user_skill_profiler", skill_profile_response)
                skill_profile_result = skill_profile_response.model_dump()
            else:
                skill_profile_result = {
                    "status": "skipped",
                    "user": target_user if target_user else "ALL_USERS",
                    "skill_profile": {},
                    "error": "RAG not ready",
                }

            # Level 1-5: Reporter - Pydantic ê¸°ë°˜
            logger.info("ğŸ“ Level 1-5: Reporter ì‹¤í–‰")

            reporter = ReporterAgent(llm=self.sonnet_llm)
            # ReporterëŠ” ResultStoreì—ì„œ ì§ì ‘ ë¡œë“œí•˜ë¯€ë¡œ dict ì „ë‹¬ ë¶ˆí•„ìš” (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹ˆ dict ì „ë‹¬)
            reporter_ctx = ReporterContext(
                task_uuid=task_uuid,
                base_path=str(base_path),
                git_url=git_url,
                static_analysis={},  # ResultStoreì—ì„œ ë¡œë“œí•˜ë¯€ë¡œ ë¹ˆ dict
                user_aggregate={},   # ResultStoreì—ì„œ ë¡œë“œí•˜ë¯€ë¡œ ë¹ˆ dict
                result_store_path=str(store.results_dir),
            )
            report_response = await reporter.run(reporter_ctx)
            store.save_result("reporter", report_response)
            report_result = report_response.model_dump()

            # ìµœì¢… ê²°ê³¼ ë°˜í™˜ (ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ)
            return {
                "repo_path": repo_path,
                "static_analysis": static_result,  # Reporter í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
                "neo4j_ready": commit_response.status == "success",
                "chromadb_ready": rag_result["status"] == "success",  # skill_chartsëŠ” ë…ë¦½ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‚¬ì „ êµ¬ì¶•
                "total_commits": commit_result.get("total_commits", 0),
                "total_files": static_result.get("loc_stats", {}).get("total_files", 0),
                "subagent_results": {
                    "repo_cloner": {"status": repo_response.status, "path": "results/repo_cloner.json"},
                    "static_analyzer": {"status": static_response.status, "path": "results/static_analyzer.json"},
                    "commit_analyzer": {"status": commit_response.status, "path": "results/commit_analyzer.json"},
                    "code_rag_builder": {"status": rag_response.status, "path": "results/code_rag_builder.json"},
                    # skill_charts_rag_builderëŠ” ë…ë¦½ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë¶„ë¦¬ë¨
                    "user_skill_profiler": {"status": skill_profile_result.get("status", "skipped"), "path": "results/user_skill_profiler.json"},
                    "user_aggregator": {"status": user_agg_result.get("status", "failed"), "path": "results/user_aggregator.json"},
                    "reporter": {"status": report_response.status, "path": "results/reporter.json"},
                },
                "final_report_path": report_result.get("report_path"),
                "updated_at": datetime.now().isoformat(),
                "error_message": None,
            }

        except Exception as e:
            logger.error(f"âŒ Execute ë…¸ë“œ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error_message": str(e),
                "updated_at": datetime.now().isoformat(),
            }

    async def _finalize_node(self, state: AgentState) -> dict[str, Any]:
        """
        ìµœì¢… ì²˜ë¦¬ ë…¸ë“œ

        ê²°ê³¼ ì €ì¥ ë° ë¦¬í¬íŠ¸ ìƒì„±
        Taskë³„ ë¡œê·¸ í•¸ë“¤ëŸ¬ ì œê±°
        """
        logger.info("ğŸ‰ Finalize: ì‘ì—… ì™„ë£Œ ì²˜ë¦¬")

        task_uuid = state["task_uuid"]
        base_path = Path(state["base_path"])

        # ì„ì‹œ ë¦¬í¬íŠ¸
        report_content = f"""# ì½”ë“œ ë¶„ì„ ë¦¬í¬íŠ¸ (Pydantic ê¸°ë°˜)

**Task UUID**: {state['task_uuid']}
**Git URL**: {state['git_url']}
**Target User**: {state.get('target_user', 'ì „ì²´ ìœ ì €')}

## ì‹¤í–‰ ê²°ê³¼

TodoList: {len(state.get('todo_list', []))}ê°œ ì‘ì—…
ì„œë¸Œì—ì´ì „íŠ¸ ê²°ê³¼: {state.get('subagent_results', {})}

**ìƒì„± ì‹œê°„**: {datetime.now().isoformat()}
"""

        # ResultStoreë¥¼ í†µí•´ ë¦¬í¬íŠ¸ ì €ì¥ (S3 ë˜ëŠ” ë¡œì»¬)
        try:
            from shared.storage import ResultStore
            store = ResultStore(task_uuid, base_path)
            report_path = store.save_report("final_report.md", report_content)
            logger.info(f"   ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ ResultStore ì €ì¥ ì‹¤íŒ¨, ë¡œì»¬ì— ì €ì¥: {e}")
            # Fallback: ë¡œì»¬ì— ì €ì¥
            report_path = base_path / "final_report.md"
            report_path.write_text(report_content, encoding="utf-8")
            logger.info(f"   ë¦¬í¬íŠ¸ ì €ì¥ (ë¡œì»¬): {report_path}")

        # ë¡œê·¸ íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ (ì‘ì—… ì™„ë£Œ ì‹œ)
        log_dir = base_path / "logs"
        if log_dir.exists():
            try:
                from shared.storage import ResultStore
                store = ResultStore(task_uuid, base_path)
                uploaded_logs = store.upload_log_directory(log_dir)
                if uploaded_logs:
                    logger.info(f"   ë¡œê·¸ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {len(uploaded_logs)}ê°œ íŒŒì¼")
            except Exception as e:
                logger.warning(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ë””ë²„ê·¸ ë¡œê·¸ ë””ë ‰í† ë¦¬ë„ S3ì— ì—…ë¡œë“œ
        debug_dir = base_path / "debug"
        if debug_dir.exists():
            try:
                from shared.storage import ResultStore
                store = ResultStore(task_uuid, base_path)
                # debug ë””ë ‰í† ë¦¬ë¥¼ logs/debug/ ì•„ë˜ì— ì—…ë¡œë“œ
                uploaded_debug = store.upload_log_directory(debug_dir, remote_subdir="debug")
                if uploaded_debug:
                    logger.info(f"   ë””ë²„ê·¸ ë¡œê·¸ ì—…ë¡œë“œ ì™„ë£Œ: {len(uploaded_debug)}ê°œ íŒŒì¼")
            except Exception as e:
                logger.warning(f"âš ï¸ ë””ë²„ê·¸ ë¡œê·¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        # í† í° ì‚¬ìš©ëŸ‰ ì „ì²´ ì§‘ê³„ ì¶œë ¥
        logger.info("")
        TokenTracker.print_summary()

        # Taskë³„ ë¡œê·¸ í•¸ë“¤ëŸ¬ ì œê±° (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        root_logger = logging.getLogger()
        handlers_to_remove = [
            h for h in root_logger.handlers
            if hasattr(h, 'task_uuid') and h.task_uuid == task_uuid
        ]
        for handler in handlers_to_remove:
            handler.close()
            root_logger.removeHandler(handler)
            logger.debug(f"   ë¡œê·¸ í•¸ë“¤ëŸ¬ ì œê±°: {task_uuid}")

        return {
            "final_report_path": str(report_path),
            "final_report": report_content,
            "updated_at": datetime.now().isoformat(),
        }
