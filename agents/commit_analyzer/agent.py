"""
CommitAnalyzer Agent

Git ì»¤ë°‹ì„ ë¶„ì„í•˜ê³  Neo4jì— ì ì¬ (Pydantic ìŠ¤í‚¤ë§ˆ ì‚¬ìš©, MERGEë¡œ ë©±ë“±ì„± ë³´ì¥)
Repository Isolation ì§€ì›: ê° Git repository ë°ì´í„°ë¥¼ ê²©ë¦¬í•˜ì—¬ ì €ì¥
"""

import logging
import asyncio
from pathlib import Path
from typing import Any, Optional

from pydriller import Repository

from shared.graph_db import GraphDBBackend, Neo4jBackend
from .schemas import CommitAnalyzerContext, CommitAnalyzerResponse
from .author_mapper import AuthorMapper

logger = logging.getLogger(__name__)


class CommitAnalyzerAgent:
    """
    Git ì»¤ë°‹ì„ ë¶„ì„í•˜ê³  Neo4jì— ì ì¬í•˜ëŠ” ì„œë¸Œì—ì´ì „íŠ¸

    Level 2 ë³‘ë ¬ ì²˜ë¦¬:
    - ì»¤ë°‹ ë§ˆì´ë‹ (PyDriller)
    - Neo4j ì ì¬ (ë°°ì¹˜ ë‹¨ìœ„, MERGE ì‚¬ìš©)

    Repository Isolation:
    - ê° ë…¸ë“œì— Repo_{repo_id} ë¼ë²¨ ìë™ ì¶”ê°€
    - ì¿¼ë¦¬ ì‹œ repo_idë¡œ í•„í„°ë§í•˜ì—¬ ë‹¤ë¥¸ repository ë°ì´í„°ì™€ ê²©ë¦¬
    """

    def __init__(
        self,
        neo4j_uri: str,
        neo4j_user: str,
        neo4j_password: str,
    ):
        self.neo4j_uri = neo4j_uri
        self.neo4j_user = neo4j_user
        self.neo4j_password = neo4j_password
        self.backend: Optional[GraphDBBackend] = None
        self.author_mapper: Optional[AuthorMapper] = None

    async def run(self, context: CommitAnalyzerContext) -> CommitAnalyzerResponse:
        """
        ì»¤ë°‹ ë¶„ì„ ë° Neo4j ì ì¬ ì‹¤í–‰ (Pydantic ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
        Repository Isolation ì ìš©: repo_idë¡œ ë°ì´í„° ê²©ë¦¬

        Args:
            context: CommitAnalyzerContext (ê²€ì¦ëœ ì…ë ¥, git_url í¬í•¨)

        Returns:
            CommitAnalyzerResponse (íƒ€ì… ì•ˆì „ ì¶œë ¥)
        """
        repo_path = context.repo_path
        target_user = context.target_user
        repo_id = context.repo_id  # Repository Isolationìš© ID
        user_emails = set(context.user_emails) if context.user_emails else set()

        logger.info(f"ğŸ“Š CommitAnalyzer: {repo_path} ë¶„ì„ ì‹œì‘ (repo_id: {repo_id})")

        # ë””ë²„ê¹…: ì…ë ¥ íŒŒë¼ë¯¸í„° ìƒì„¸ ë¡œê·¸
        logger.info(f"ğŸ” Input parameters:")
        logger.info(f"   - target_user: {target_user}")
        logger.info(f"   - user_emails ({len(user_emails)}): {sorted(user_emails) if user_emails else 'None'}")
        logger.info(f"   - author_mapping_rules: {'Enabled' if context.author_mapping_rules else 'Disabled'}")

        if user_emails:
            logger.info(f"ğŸ“§ ì‚¬ìš©ì ì‹ë³„ì {len(user_emails)}ê°œ ì‚¬ìš© (ê°•í™”ëœ ì‚¬ìš©ì íŒë‹¨)")

        try:
            # GraphDBBackend ì´ˆê¸°í™” (Neo4j)
            self.backend = Neo4jBackend()

            # AuthorMapper ì´ˆê¸°í™” (ë§¤í•‘ ê·œì¹™ì´ ìˆëŠ” ê²½ìš°)
            if context.author_mapping_rules:
                mapping_dict = context.author_mapping_rules.to_dict()
                self.author_mapper = AuthorMapper(mapping_dict)
                stats = self.author_mapper.get_mapping_stats()
                logger.info(
                    f"âœ… AuthorMapper enabled: {stats['total_developers']} developers, "
                    f"{stats['total_aliases']} aliases"
                )

                # ë””ë²„ê¹…: ê° ë§¤í•‘ ê·œì¹™ ìƒì„¸ ë¡œê·¸
                for canonical_name, rule in mapping_dict.items():
                    logger.debug(f"  ğŸ“‹ Mapping rule: {canonical_name} (canonical_email: {rule['canonical_email']})")
                    logger.debug(f"     â† {len(rule.get('aliases', []))} aliases")
                    for alias in rule.get('aliases', []):
                        alias_name = alias.get('name', 'N/A')
                        alias_email = alias['email']
                        logger.debug(f"       - {alias_name} <{alias_email}>")
            else:
                self.author_mapper = None
                logger.info("â„¹ï¸ AuthorMapper disabled: No mapping rules provided")

            # Neo4j ì´ˆê¸°í™” (ì¸ë±ìŠ¤, ì œì•½ì¡°ê±´) - Repositoryë³„ë¡œ ê²©ë¦¬
            await self._init_neo4j(repo_id)

            # Level 2-1: PyDrillerë¡œ ì»¤ë°‹ ë§ˆì´ë‹ (ê°•í™”ëœ ì‚¬ìš©ì íŒë‹¨)
            commits_data = await self._mine_commits(repo_path, target_user, user_emails)

            # Level 2-2: Neo4jì— ë°°ì¹˜ ì ì¬ (MERGE ì‚¬ìš©, Repository Isolation ì ìš©)
            stats = await self._load_to_neo4j(commits_data, repo_id)

            logger.info(
                f"âœ… CommitAnalyzer: {stats['total_commits']}ê°œ ì»¤ë°‹, "
                f"{stats['total_users']}ëª… ìœ ì € ì ì¬ ì™„ë£Œ (repo_id: {repo_id})"
            )

            return CommitAnalyzerResponse(
                status="success",
                total_commits=stats["total_commits"],
                total_users=stats["total_users"],
                total_files=stats["total_files"],
                error=None,
            )

        except Exception as e:
            logger.error(f"âŒ CommitAnalyzer: {e}")
            return CommitAnalyzerResponse(
                status="failed",
                total_commits=0,
                total_users=0,
                total_files=0,
                error=str(e),
            )

        finally:
            if self.backend:
                await self.backend.close()

    async def _init_neo4j(self, repo_id: str):
        """
        Neo4j ì¸ë±ìŠ¤ ë° ì œì•½ì¡°ê±´ ìƒì„± (Repository Isolation ì ìš©)

        Args:
            repo_id: Repository ID (ì˜ˆ: github_user_repo)
        """
        # Repository ë¼ë²¨ ìƒì„±
        repo_label = self.backend.get_repo_label(repo_id)

        # ì œì•½ì¡°ê±´ ìƒì„± ì¿¼ë¦¬ (Repositoryë³„ë¡œ ê²©ë¦¬)
        # Neo4j 5.xì—ì„œëŠ” ì—¬ëŸ¬ ë¼ë²¨ì„ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ,
        # ë‹¨ì¼ ë¼ë²¨ + ë³µí•© í‚¤(repo_id ì†ì„± í¬í•¨)ë¡œ ì œì•½ì¡°ê±´ ìƒì„±
        safe_repo_id = repo_id.replace('-', '_').replace('.', '_')
        constraints = [
            # User ë…¸ë“œ: (email, repo_id) ë³µí•© í‚¤ë¡œ repositoryë³„ uniqueness ë³´ì¥
            f"CREATE CONSTRAINT user_email_{safe_repo_id} IF NOT EXISTS "
            f"FOR (u:User) REQUIRE (u.email, u.repo_id) IS UNIQUE",

            # Commit ë…¸ë“œ: (hash, repo_id) ë³µí•© í‚¤ë¡œ repositoryë³„ uniqueness ë³´ì¥
            f"CREATE CONSTRAINT commit_hash_{safe_repo_id} IF NOT EXISTS "
            f"FOR (c:Commit) REQUIRE (c.hash, c.repo_id) IS UNIQUE",

            # File ë…¸ë“œ: (path, repo_id) ë³µí•© í‚¤ë¡œ repositoryë³„ uniqueness ë³´ì¥
            f"CREATE CONSTRAINT file_path_{safe_repo_id} IF NOT EXISTS "
            f"FOR (f:File) REQUIRE (f.path, f.repo_id) IS UNIQUE",
        ]

        for constraint_query in constraints:
            try:
                await self.backend.execute_query(constraint_query, repo_id=repo_id)
            except Exception as e:
                # ì œì•½ì¡°ê±´ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë¬´ì‹œ
                if "already exists" not in str(e).lower():
                    logger.warning(f"âš ï¸ ì œì•½ì¡°ê±´ ìƒì„± ì‹¤íŒ¨: {e}")

        logger.info(f"âœ… Neo4j ì¸ë±ìŠ¤ ë° ì œì•½ì¡°ê±´ ìƒì„± ì™„ë£Œ (repo_id: {repo_id})")

    def _extract_username_from_email(self, email: str) -> str:
        """
        ì´ë©”ì¼ ì•ë¶€ë¶„(username) ì¶”ì¶œ
        """
        prefix = email.split("@")[0] if "@" in email else email
        return prefix.split("+")[1] if "+" in prefix else prefix

    def _is_target_user(
        self, commit, target_user: str, user_emails: set[str]
    ) -> bool:
        """
        ê°•í™”ëœ ì‚¬ìš©ì íŒë‹¨ ë¡œì§

        Args:
            commit: PyDriller commit ê°ì²´
            target_user: íƒ€ê²Ÿ ì‚¬ìš©ì (ì´ë©”ì¼ ë˜ëŠ” ì´ë¦„)
            user_emails: GitHub APIë¡œ ì¡°íšŒí•œ ì‚¬ìš©ì ì´ë©”ì¼/ì‹ë³„ì ëª©ë¡ (ì†Œë¬¸ì)

        Returns:
            target_userì˜ ì»¤ë°‹ì´ë©´ True
        """
        author_email = commit.author.email.lower() if commit.author.email else ""
        author_name = commit.author.name.lower() if commit.author.name else ""
        author_name_from_email = self._extract_username_from_email(commit.author.email) if commit.author.email else ""

        target_lower = target_user.lower()
        lower_target_emails = {email.lower() for email in user_emails}
        target_names_from_email = {self._extract_username_from_email(email) for email in lower_target_emails}

        # 1. ì •í™•í•œ ì´ë©”ì¼/ì´ë¦„ ë§¤ì¹­
        if target_lower == author_email or target_lower == author_name:
            logger.debug(
                f"âœ… Match (exact): {commit.author.name} <{commit.author.email}> "
                f"matches target_user={target_user}"
            )
            return True

        # 2. GitHub APIë¡œ ì¡°íšŒí•œ ì´ë©”ì¼ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
        if author_email in lower_target_emails:
            logger.debug(
                f"âœ… Match (user_emails): {commit.author.name} <{commit.author.email}> "
                f"found in user_emails"
            )
            return True

        # 3. ìœ ì €ì˜ ì´ë©”ì¼ ì•ë¶€ë¶„(username)ê³¼ ì‘ì„±ì ì´ë¦„ ë¹„êµ
        if author_name in target_names_from_email:
            logger.debug(
                f"âœ… Match (username): {commit.author.name} <{commit.author.email}> "
                f"matches target username"
            )
            return True

        # 4. ìœ ì €ì˜ ì´ë©”ì¼ ì•ë¶€ë¶„(username)ê³¼ ì‘ì„±ì ì´ë©”ì¼ ì•ë¶€ë¶„ ë¹„êµ
        if author_name_from_email in target_names_from_email:
            logger.debug(
                f"âœ… Match (email prefix): {commit.author.name} <{commit.author.email}> "
                f"matches target email prefix"
            )
            return True

        # ë§¤ì¹­ ì‹¤íŒ¨ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
        logger.debug(
            f"âŒ No match: {commit.author.name} <{commit.author.email}> "
            f"vs target_user={target_user}, user_emails={user_emails}"
        )
        return False

    async def _mine_commits(
        self, repo_path: str, target_user: Optional[str], user_emails: set[str] = None
    ) -> list[dict[str, Any]]:
        """
        PyDrillerë¡œ ì»¤ë°‹ ë§ˆì´ë‹ (ê°•í™”ëœ ì‚¬ìš©ì íŒë‹¨ ë¡œì§)

        Args:
            repo_path: Git ë ˆí¬ì§€í† ë¦¬ ê²½ë¡œ
            target_user: íŠ¹ì • ìœ ì € ì´ë©”ì¼ ë˜ëŠ” ì´ë¦„ (Noneì´ë©´ ì „ì²´)
            user_emails: GitHub APIë¡œ ì¡°íšŒí•œ ì‚¬ìš©ì ì´ë©”ì¼/ì‹ë³„ì ëª©ë¡ (ì†Œë¬¸ì)

        Returns:
            list of commit data dictionaries
        """
        commits_data = []
        if user_emails is None:
            user_emails = set()

        # PyDrillerëŠ” ë™ê¸° APIì´ë¯€ë¡œ executorì—ì„œ ì‹¤í–‰
        def _mine():
            repo = Repository(repo_path)
            author_mapping_log = {}  # ë””ë²„ê¹…: ì›ë³¸ â†’ ì •ê·œí™” ë§¤í•‘ ì¶”ì 

            for commit in repo.traverse_commits():
                # íŠ¹ì • ìœ ì € í•„í„°ë§ (ê°•í™”ëœ ë¡œì§)
                if target_user:
                    if not self._is_target_user(commit, target_user, user_emails):
                        continue

                # ì €ì ì •ë³´ ì •ê·œí™” (AuthorMapper ì‚¬ìš©)
                original_author_name = commit.author.name
                original_author_email = commit.author.email

                if self.author_mapper:
                    normalized_name, normalized_email = self.author_mapper.normalize_author(
                        original_author_name, original_author_email
                    )

                    # ë””ë²„ê¹…: ë§¤í•‘ì´ ë°œìƒí•œ ê²½ìš° ë¡œê·¸ ê¸°ë¡
                    if (normalized_name, normalized_email) != (original_author_name, original_author_email):
                        original_key = (original_author_name, original_author_email)
                        normalized_key = (normalized_name, normalized_email)

                        if original_key not in author_mapping_log:
                            author_mapping_log[original_key] = normalized_key
                            logger.debug(
                                f"ğŸ”„ Author mapped: {original_author_name} <{original_author_email}> "
                                f"â†’ {normalized_name} <{normalized_email}>"
                            )
                else:
                    normalized_name = original_author_name
                    normalized_email = original_author_email

                commit_data = {
                    "hash": commit.hash,
                    "message": commit.msg,
                    "author_name": normalized_name,  # ì •ê·œí™”ëœ ì´ë¦„
                    "author_email": normalized_email,  # ì •ê·œí™”ëœ ì´ë©”ì¼
                    "original_author_name": original_author_name,  # ì›ë³¸ ì´ë¦„ (ì°¸ê³ ìš©)
                    "original_author_email": original_author_email,  # ì›ë³¸ ì´ë©”ì¼ (ì°¸ê³ ìš©)
                    "author_date": commit.author_date.isoformat(),
                    "committer_name": commit.committer.name,
                    "committer_email": commit.committer.email,
                    "committer_date": commit.committer_date.isoformat(),
                    "lines_added": commit.insertions,
                    "lines_deleted": commit.deletions,
                    "files_changed": commit.files,
                    "modifications": [],
                }

                # íŒŒì¼ ìˆ˜ì • ë‚´ì—­
                for modification in commit.modified_files:
                    # NULL ê²½ë¡œ í•„í„°ë§ (ì‚­ì œëœ íŒŒì¼ ë“±)
                    file_path = modification.new_path or modification.old_path

                    if file_path is None:
                        logger.warning(
                            f"âš ï¸ Commit {commit.hash[:7]}: íŒŒì¼ ê²½ë¡œê°€ Noneì¸ ìˆ˜ì •ì‚¬í•­ ìŠ¤í‚µ "
                            f"(change_type: {modification.change_type.name})"
                        )
                        continue

                    commit_data["modifications"].append({
                        "filename": modification.filename,
                        "old_path": modification.old_path,
                        "new_path": file_path,  # NULL ëŒ€ì‹  ìœ íš¨í•œ ê²½ë¡œ ì‚¬ìš©
                        "change_type": modification.change_type.name,
                        "added_lines": modification.added_lines,
                        "deleted_lines": modification.deleted_lines,
                        "complexity": modification.complexity if modification.complexity else 0,
                    })

                commits_data.append(commit_data)

            return commits_data

        # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        commits_data = await loop.run_in_executor(None, _mine)

        # ë””ë²„ê¹…: ì‘ì„±ì í†µê³„ ì§‘ê³„
        if self.author_mapper:
            original_authors = set()
            normalized_authors = set()

            for commit in commits_data:
                original_authors.add((commit['original_author_name'], commit['original_author_email']))
                normalized_authors.add((commit['author_name'], commit['author_email']))

            logger.info(
                f"ğŸ“Š Author consolidation: {len(original_authors)} original IDs â†’ {len(normalized_authors)} normalized developers"
            )

            # ì •ê·œí™”ëœ ê°œë°œìë³„ ì»¤ë°‹ ìˆ˜ ì§‘ê³„
            normalized_commit_counts = {}
            for commit in commits_data:
                key = (commit['author_name'], commit['author_email'])
                normalized_commit_counts[key] = normalized_commit_counts.get(key, 0) + 1

            logger.info(f"ğŸ“Š Normalized developers ({len(normalized_commit_counts)}):")
            for (name, email), count in sorted(normalized_commit_counts.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"   - {name} <{email}>: {count} commits")
        else:
            # AuthorMapper ë¹„í™œì„±í™” ìƒíƒœì—ì„œë„ ì‘ì„±ì í†µê³„ ì¶œë ¥
            author_commit_counts = {}
            for commit in commits_data:
                key = (commit['author_name'], commit['author_email'])
                author_commit_counts[key] = author_commit_counts.get(key, 0) + 1

            logger.info(f"ğŸ“Š Authors ({len(author_commit_counts)}):")
            for (name, email), count in sorted(author_commit_counts.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"   - {name} <{email}>: {count} commits")

        logger.info(f"ğŸ“Š PyDriller: {len(commits_data)}ê°œ ì»¤ë°‹ ë§ˆì´ë‹ ì™„ë£Œ")
        return commits_data

    async def _load_to_neo4j(
        self, commits_data: list[dict[str, Any]], repo_id: str
    ) -> dict[str, int]:
        """
        ì»¤ë°‹ ë°ì´í„°ë¥¼ Neo4jì— ë°°ì¹˜ ì ì¬ (MERGE ì‚¬ìš©í•˜ì—¬ ë©±ë“±ì„± ë³´ì¥)
        Repository Isolation ì ìš©: repo_idë¡œ ë°ì´í„° ê²©ë¦¬

        Args:
            commits_data: ì»¤ë°‹ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            repo_id: Repository ID (ì˜ˆ: github_user_repo)

        Returns:
            {"total_commits": int, "total_users": int, "total_files": int}
        """
        # Repository ë¼ë²¨ ìƒì„±
        repo_label = self.backend.get_repo_label(repo_id)

        # ë°°ì¹˜ í¬ê¸°
        batch_size = 100
        total_commits = 0
        users_set = set()
        files_set = set()

        for i in range(0, len(commits_data), batch_size):
            batch = commits_data[i : i + batch_size]

            # ë°°ì¹˜ ì²˜ë¦¬ ì¿¼ë¦¬ (MERGE ì‚¬ìš©, Repository Isolation ì ìš©)
            # ì œì•½ì¡°ê±´ì´ ë³µí•© í‚¤ì´ë¯€ë¡œ MERGEë„ ë³µí•© í‚¤ë¡œ ë§¤ì¹­í•˜ë˜, ë¼ë²¨ì€ ì—¬ì „íˆ ì¶”ê°€
            query = f"""
            UNWIND $commits AS commit

            // User ë…¸ë“œ ìƒì„±/ë³‘í•© (ë³µí•© í‚¤: email + repo_id, Repository ë¼ë²¨ í¬í•¨)
            MERGE (u:{repo_label}:User {{email: commit.author_email, repo_id: $repo_id}})
            ON CREATE SET
                u.name = commit.author_name
            ON MATCH SET
                u.name = commit.author_name  // ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ì—…ë°ì´íŠ¸

            // Commit ë…¸ë“œ ìƒì„±/ë³‘í•© (ë³µí•© í‚¤: hash + repo_id, Repository ë¼ë²¨ í¬í•¨, ë©±ë“±ì„± ë³´ì¥)
            MERGE (c:{repo_label}:Commit {{hash: commit.hash, repo_id: $repo_id}})
            ON CREATE SET
                c.message = commit.message,
                c.author_date = datetime(commit.author_date),
                c.committer_date = datetime(commit.committer_date),
                c.lines_added = commit.lines_added,
                c.lines_deleted = commit.lines_deleted,
                c.files_changed = commit.files_changed
            ON MATCH SET
                c.message = commit.message,
                c.author_date = datetime(commit.author_date),
                c.committer_date = datetime(commit.committer_date),
                c.lines_added = commit.lines_added,
                c.lines_deleted = commit.lines_deleted,
                c.files_changed = commit.files_changed

            // User-Commit ê´€ê³„ ìƒì„±/ë³‘í•©
            MERGE (u)-[:COMMITTED]->(c)

            // File ë…¸ë“œ ë° ê´€ê³„
            WITH c, commit
            UNWIND commit.modifications AS mod

            // File ë…¸ë“œ ìƒì„±/ë³‘í•© (ë³µí•© í‚¤: path + repo_id, Repository ë¼ë²¨ í¬í•¨)
            MERGE (f:{repo_label}:File {{path: mod.new_path, repo_id: $repo_id}})
            ON CREATE SET
                f.filename = mod.filename,
                f.old_path = mod.old_path,
                f.new_path = mod.new_path

            // Commit-File ê´€ê³„ ìƒì„±/ë³‘í•© (ë©±ë“±ì„± ë³´ì¥)
            MERGE (c)-[r:MODIFIED]->(f)
            ON CREATE SET
                r.change_type = mod.change_type,
                r.added_lines = mod.added_lines,
                r.deleted_lines = mod.deleted_lines,
                r.complexity = mod.complexity
            ON MATCH SET
                r.change_type = mod.change_type,
                r.added_lines = mod.added_lines,
                r.deleted_lines = mod.deleted_lines,
                r.complexity = mod.complexity
            """

            await self.backend.execute_query(
                query, params={"commits": batch, "repo_id": repo_id}, repo_id=repo_id
            )

            total_commits += len(batch)

            # í†µê³„ ìˆ˜ì§‘
            for commit in batch:
                users_set.add(commit["author_email"])
                for mod in commit["modifications"]:
                    files_set.add(mod["new_path"])

            logger.info(f"ğŸ“Š Neo4j: {total_commits}/{len(commits_data)} ì»¤ë°‹ ì ì¬ ì¤‘...")

        return {
            "total_commits": total_commits,
            "total_users": len(users_set),
            "total_files": len(files_set),
        }
