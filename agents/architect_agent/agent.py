"""ArchitectAgent - ì•„í‚¤í…ì²˜ íŒ¨í„´ ë° ì„¤ê³„ ì›ì¹™ ë¶„ì„ ì—ì´ì „íŠ¸"""

import logging
import json
import re
import os
from typing import Dict, Any, List, Optional
from langchain_aws import ChatBedrockConverse
from langchain_core.messages import SystemMessage, HumanMessage

from .schemas import ArchitectAgentContext, ArchitectAgentResponse, ArchitectureAnalysis
from shared.utils.prompt_loader import PromptLoader
from shared.utils.token_tracker import TokenTracker
from shared.utils.agent_logging import log_agent_execution
from pathlib import Path

logger = logging.getLogger(__name__)


class ArchitectAgent:
    """ì•„í‚¤í…ì²˜ ì „ë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸"""

    def __init__(self, llm: Optional[ChatBedrockConverse] = None):
        # í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: YAML ëª¨ë¸ ìš°ì„ , ì™¸ë¶€ LLM ì „ë‹¬ ì‹œ ì˜¤ë²„ë¼ì´ë“œ
        if llm is None:
            # YAML ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.llm = PromptLoader.get_llm("architect_agent")
            model_id = PromptLoader.get_model("architect_agent")
            logger.info(f"âœ… ArchitectAgent: YAML ëª¨ë¸ ì‚¬ìš© - {model_id}")
        else:
            # ì™¸ë¶€ ì „ë‹¬ëœ LLM ì‚¬ìš© (ì˜¤ë²„ë¼ì´ë“œ)
            self.llm = llm
            logger.info(f"âœ… ArchitectAgent: ì™¸ë¶€ LLM ì‚¬ìš©")
        
        # í•˜ì´ë¸Œë¦¬ë“œ: ìŠ¤í‚¤ë§ˆ ìë™ ì£¼ì…
        self.prompts = PromptLoader.load_with_schema(
            "architect_agent",
            response_schema_class=ArchitectureAnalysis
        )

    @log_agent_execution(agent_name="architect_agent")
    async def run(self, context: ArchitectAgentContext) -> ArchitectAgentResponse:
        logger.info("ğŸ—ï¸  ArchitectAgent: ì•„í‚¤í…ì²˜ ë¶„ì„ ì‹œì‘")

        try:
            static_analysis = context.static_analysis
            user_aggregate = context.user_aggregate
            repo_path = context.repo_path

            # ë°ì´í„° ì¶”ì¶œ
            loc_stats = static_analysis.get("loc_stats", {})
            code_lines = loc_stats.get("code_lines", 0)
            by_language = loc_stats.get("by_language", {})
            total_files = sum(
                lang_stats.get("files", 0) for lang_stats in by_language.values()
            )

            complexity_data = static_analysis.get("complexity", {})
            total_functions = complexity_data.get("total_functions", 0)

            agg_stats = user_aggregate.get("aggregate_stats", {})
            tech_stack = (
                agg_stats.get("tech_stats", {}).get("technology_frequency", {})
            )

            # ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„
            directory_structure = self._analyze_directory_structure(repo_path)

            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¤€ë¹„
            prompt_variables = {
                "total_files": total_files,
                "code_lines": code_lines,
                "total_functions": total_functions,
                "directory_structure": "\n".join(directory_structure[:30]),
                "tech_stack": self._format_tech_stack(tech_stack),
                "avg_lines_per_file": f"{code_lines / total_files if total_files > 0 else 0:.1f}",
                "avg_lines_per_function": f"{code_lines / total_functions if total_functions > 0 else 0:.1f}",
            }

            # í”„ë¡¬í”„íŠ¸ ìƒì„± (json_schema ë³€ìˆ˜ ìë™ ì£¼ì…)
            system_prompt = PromptLoader.format(
                self.prompts["system_prompt"],
                json_schema=self.prompts.get("json_schema", "")
            )
            user_prompt = PromptLoader.format(
                self.prompts["user_template"],
                **prompt_variables
            )

            # LLM í˜¸ì¶œ (í† í° ì¶”ì )
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ]

            # LLM í˜¸ì¶œ ë¡œê¹…ì„ ìœ„í•´ logger ê°€ì ¸ì˜¤ê¸°
            from shared.utils.agent_debug_logger import AgentDebugLogger
            from pathlib import Path
            base_path = Path(f"./data/analyze/{context.task_uuid}")
            debug_logger = AgentDebugLogger.get_logger(context.task_uuid, base_path, "architect_agent")

            with TokenTracker.track("architect_agent"), debug_logger.track_llm_call() as llm_tracker:
                # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
                llm_tracker.log_prompts(
                    template_name="architect_agent",
                    variables=prompt_variables,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                )
                
                # LLM í˜¸ì¶œ
                response = await self.llm.ainvoke(messages)
                TokenTracker.record_usage("architect_agent", response, model_id=PromptLoader.get_model("architect_agent"))
                llm_tracker.set_messages(messages)
                llm_tracker.set_response(response)
                
                # ì‘ë‹µ ì²˜ë¦¬ ë‹¨ê³„ë³„ ë¡œê¹…
                raw_response = response.content
                parsed_json = None
                architecture_analysis = None
                processing_error = None
                
                try:
                    # JSON íŒŒì‹±
                    parsed_json = self._parse_json_response(raw_response)
                    
                    # Pydantic ê²€ì¦
                    architecture_analysis = ArchitectureAnalysis(**parsed_json)
                    
                    # ì„±ê³µ ë¡œê¹…
                    llm_tracker.log_response_stages(
                        raw=raw_response,
                        parsed=parsed_json,
                        validated=architecture_analysis,
                    )
                except Exception as parse_error:
                    processing_error = str(parse_error)
                    # ì—ëŸ¬ ë¡œê¹…
                    llm_tracker.log_response_stages(
                        raw=raw_response,
                        parsed=parsed_json,
                        validated=None,
                        error=processing_error,
                    )
                    raise

            logger.info(
                f"âœ… ArchitectAgent: ì•„í‚¤í…ì²˜ ë¶„ì„ ì™„ë£Œ - ì ìˆ˜ {architecture_analysis.architecture_score}/10"
            )

            response = ArchitectAgentResponse(
                status="success",
                architecture_analysis=architecture_analysis,
                error=None,
            )
            return response

        except Exception as e:
            logger.error(f"âŒ ArchitectAgent: {e}", exc_info=True)
            error_response = ArchitectAgentResponse(
                status="failed",
                architecture_analysis=ArchitectureAnalysis(),
                error=str(e),
            )
            return error_response

    def _analyze_directory_structure(self, repo_path: str) -> List[str]:
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„ (3ë ˆë²¨ê¹Œì§€)"""
        directory_structure = []

        if not os.path.exists(repo_path):
            return ["ë ˆí¬ì§€í† ë¦¬ ê²½ë¡œ ì—†ìŒ"]

        try:
            for root, dirs, files in os.walk(repo_path):
                # .git ë“± ì œì™¸
                dirs[:] = [d for d in dirs if not d.startswith(".")]
                level = root.replace(repo_path, "").count(os.sep)
                if level < 3:  # 3ë ˆë²¨ê¹Œì§€ë§Œ
                    indent = " " * 2 * level
                    directory_structure.append(f"{indent}{os.path.basename(root)}/")
        except Exception as e:
            logger.warning(f"ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return ["ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨"]

        return directory_structure

    def _parse_json_response(self, text: str) -> Dict[str, Any]:
        """LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹± (ì¤‘ê´„í˜¸ ë§¤ì¹­ ë¡œì§)"""
        # 1. ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ ì‹œë„
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
        if json_match:
            logger.info("âœ… ArchitectAgent: JSON ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ ì„±ê³µ")
            return json.loads(json_match.group(1))

        # 2. ì¤‘ê´„í˜¸ ë§¤ì¹­ì„ í†µí•´ ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ ì°¾ê¸°
        try:
            logger.info("âš ï¸  ArchitectAgent: JSON ì½”ë“œ ë¸”ë¡ ì—†ìŒ, ì²« ë²ˆì§¸ JSON ê°ì²´ ì¶”ì¶œ ì‹œë„")

            start_idx = text.find("{")
            if start_idx == -1:
                raise ValueError("JSON ê°ì²´ ì‹œì‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            brace_count = 0
            end_idx = start_idx
            for i in range(start_idx, len(text)):
                if text[i] == "{":
                    brace_count += 1
                elif text[i] == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i + 1
                        break

            if brace_count != 0:
                raise ValueError("JSON ê°ì²´ê°€ ì™„ì „í•˜ì§€ ì•ŠìŒ")

            json_str = text[start_idx:end_idx]
            return json.loads(json_str)

        except (ValueError, json.JSONDecodeError) as e:
            logger.warning(f"âŒ ArchitectAgent: JSON íŒŒì‹± ì‹¤íŒ¨ - {e}")
            logger.warning("âš ï¸  ArchitectAgent: ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©")
            return {
                "structure_patterns": [],
                "design_principles": {},
                "modularity_score": 6.0,
                "scalability_assessment": "ë³´í†µ ìˆ˜ì¤€",
                "architecture_score": 6.0,
                "recommendations": ["ì•„í‚¤í…ì²˜ ê°œì„  ê¶Œì¥"],
                "raw_analysis": text,
            }

    def _format_tech_stack(self, tech_stack: Dict[str, int]) -> str:
        """ê¸°ìˆ  ìŠ¤íƒ í¬ë§·íŒ…"""
        if not tech_stack:
            return "N/A"

        items = []
        for tech, count in sorted(tech_stack.items(), key=lambda x: x[1], reverse=True):
            items.append(f"- {tech}: {count}íšŒ")

        return "\n".join(items) if items else "N/A"
