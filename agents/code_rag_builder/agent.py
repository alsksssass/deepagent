"""CodeRAGBuilder Agent - ì½”ë“œ RAG êµ¬ì¶•"""

import logging
import asyncio
from pathlib import Path
from typing import Any
import hashlib

import chromadb
from sentence_transformers import SentenceTransformer
from shared.tools.chromadb_tools import get_code_chroma_client

from .schemas import CodeRAGBuilderContext, CodeRAGBuilderResponse
from shared.utils.tree_sitter_utils import (
    extract_functions_and_classes,
    get_language_from_extension,
    is_language_supported,
)

logger = logging.getLogger(__name__)


class CodeRAGBuilderAgent:
    """
    ì½”ë“œë¥¼ íŒŒì‹±í•˜ê³  ChromaDBì— ì„ë² ë”©ì„ ì €ì¥í•˜ëŠ” ì—ì´ì „íŠ¸

    Level 2 ë³‘ë ¬ ì²˜ë¦¬:
    - íŒŒì¼ ì½ê¸° ë° íŒŒì‹±
    - ì„ë² ë”© ìƒì„± ë° ì €ì¥
    """

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.client = None

    async def run(self, context: CodeRAGBuilderContext) -> CodeRAGBuilderResponse:
        """
        ì½”ë“œ RAG êµ¬ì¶• ì‹¤í–‰

        Args:
            context: CodeRAGBuilderContext

        Returns:
            CodeRAGBuilderResponse
        """
        repo_path = Path(context.repo_path)
        task_uuid = context.task_uuid
        collection_name = f"code_{task_uuid}"

        logger.info(f"ğŸ”¨ CodeRAGBuilder: {repo_path} RAG êµ¬ì¶• ì‹œì‘")

        try:
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë™ê¸°)
            # device="cpu"ë¥¼ ëª…ì‹œí•˜ì—¬ meta tensor ì˜¤ë¥˜ ë°©ì§€
            loop = asyncio.get_event_loop()
            self.model = await loop.run_in_executor(
                None, lambda: SentenceTransformer(self.model_name, device="cpu")
            )

            # ChromaDB í´ë¼ì´ì–¸íŠ¸ (task_uuidë³„ ë¡œì»¬ ì €ì¥ì†Œ)
            self.client = get_code_chroma_client(task_uuid)

            # ì»¬ë ‰ì…˜ ìƒì„± (ê¸°ì¡´ ê²ƒ ì‚­ì œ)
            try:
                self.client.delete_collection(name=collection_name)
            except Exception:
                pass

            collection = self.client.create_collection(name=collection_name)

            # Level 2-1: ì½”ë“œ íŒŒì¼ ìˆ˜ì§‘
            code_files = await self._collect_code_files(repo_path)

            logger.info(f"ğŸ“‚ {len(code_files)}ê°œ ì½”ë“œ íŒŒì¼ ë°œê²¬")

            # Level 2-2: íŒŒì¼ë³„ íŒŒì‹± ë° ì²­í¬ ìƒì„± (ë°°ì¹˜ ë³‘ë ¬)
            all_chunks = []
            batch_size = 10

            for i in range(0, len(code_files), batch_size):
                batch = code_files[i : i + batch_size]

                # ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬
                batch_chunks_list = await asyncio.gather(
                    *[self._parse_file(file_path) for file_path in batch]
                )

                for chunks in batch_chunks_list:
                    all_chunks.extend(chunks)

                logger.info(f"ğŸ“Š {i + len(batch)}/{len(code_files)} íŒŒì¼ íŒŒì‹± ì™„ë£Œ")

            # Level 2-3: ì„ë² ë”© ìƒì„± ë° ì €ì¥
            total_chunks = await self._store_embeddings(collection, all_chunks)

            logger.info(f"âœ… CodeRAGBuilder: {total_chunks}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")

            return CodeRAGBuilderResponse(
                status="success",
                total_files=len(code_files),
                total_chunks=total_chunks,
                collection_name=collection_name,
            )

        except Exception as e:
            logger.error(f"âŒ CodeRAGBuilder: {e}")
            return CodeRAGBuilderResponse(
                status="failed",
                total_files=0,
                total_chunks=0,
                collection_name="",
                error=str(e),
            )

    async def _collect_code_files(self, repo_path: Path) -> list[Path]:
        """
        ì½”ë“œ íŒŒì¼ ìˆ˜ì§‘ (.py, .js, .ts, .tsx, .jsx, .java, .go, .rs ë“±)
        """
        code_extensions = {
            ".py",
            ".js",
            ".ts",
            ".tsx",
            ".jsx",
            ".java",
            ".go",
            ".rs",
            ".cpp",
            ".c",
            ".h",
            ".hpp",
            ".cs",
            ".rb",
            ".php",
            ".swift",
            ".kt",
        }

        def _collect():
            files = []
            for ext in code_extensions:
                files.extend(repo_path.rglob(f"*{ext}"))
            return files

        loop = asyncio.get_event_loop()
        files = await loop.run_in_executor(None, _collect)

        # í…ŒìŠ¤íŠ¸, ë¹Œë“œ, node_modules ë“± ì œì™¸
        exclude_patterns = [
            "test",
            "tests",
            "__pycache__",
            "node_modules",
            "venv",
            ".venv",
            "build",
            "dist",
            ".git",
        ]

        filtered_files = [
            f
            for f in files
            if not any(pattern in str(f) for pattern in exclude_patterns)
        ]

        return filtered_files

    async def _parse_file(self, file_path: Path) -> list[dict[str, Any]]:
        """
        íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• 

        í†µí•© íŒŒì„œ ì „ëµ:
        1. Tree-sitter ì§€ì› ì–¸ì–´: Tree-sitter ê¸°ë°˜ í•¨ìˆ˜/í´ë˜ìŠ¤ ë‹¨ìœ„ ë¶„í•  (Python í¬í•¨)
        2. ê¸°íƒ€: ë¹ˆ ì¤„ 2ê°œ ì´ìƒ ê¸°ì¤€ ë¶„í•  (í´ë°±)
        3. ìµœëŒ€ ì²­í¬ í¬ê¸° ì œí•œ (200ì¤„)

        Returns:
            list of {"file": str, "chunk_id": str, "code": str, "type": str, "line_start": int, "line_end": int}
        """

        def _parse():
            try:
                content = file_path.read_text(encoding="utf-8")
            except Exception as e:
                logger.warning(f"âš ï¸  {file_path} ì½ê¸° ì‹¤íŒ¨: {e}")
                return []

            lines = content.split("\n")
            chunks = []
            parser_type = self._select_parser(file_path)
            
            # Tree-sitter: êµ¬ì¡°ì  íŒŒì‹± (ì‹¤ì œ AST ê¸°ë°˜)
            if parser_type == "tree-sitter":
                language = get_language_from_extension(file_path.suffix)
                if language:
                    tree_sitter_chunks = extract_functions_and_classes(
                        content, language, max_chunk_lines=200
                    )
                    if tree_sitter_chunks:
                        chunks.extend(tree_sitter_chunks)
                        logger.debug(f"âœ… {file_path.name}: Tree-sitter ê¸°ë°˜ {len(tree_sitter_chunks)}ê°œ ì²­í¬ ìƒì„± ({language})")
                    else:
                        # Tree-sitter íŒŒì‹± ì‹¤íŒ¨ ì‹œ í´ë°±
                        logger.warning(f"âš ï¸  {file_path.name} Tree-sitter íŒŒì‹± ì‹¤íŒ¨, ë¹ˆ ì¤„ ê¸°ì¤€ìœ¼ë¡œ í´ë°±")
                        parser_type = "blank-line"
                else:
                    parser_type = "blank-line"
            
            # ë¹ˆ ì¤„ ê¸°ì¤€ ë¶„í•  (í´ë°±)
            if parser_type == "blank-line" or not chunks:
                blank_line_chunks = self._extract_blank_line_chunks(content, lines)
                if blank_line_chunks:
                    # ê¸°ì¡´ ì²­í¬ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ë¹ˆ ì¤„ ì²­í¬ë§Œ ì¶”ê°€
                    if chunks:
                        existing_ranges = {(c["line_start"], c["line_end"]) for c in chunks}
                        for blank_chunk in blank_line_chunks:
                            # ê¸°ì¡´ ì²­í¬ì™€ ê²¹ì¹˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
                            overlaps = False
                            for start, end in existing_ranges:
                                if not (blank_chunk["line_end"] < start or blank_chunk["line_start"] > end):
                                    overlaps = True
                                    break
                            if not overlaps:
                                chunks.append(blank_chunk)
                    else:
                        chunks.extend(blank_line_chunks)
            
            # ìµœëŒ€ ì²­í¬ í¬ê¸° ì œí•œ (200ì¤„ ì´ˆê³¼ ì‹œ ë¶„í• )
            final_chunks = []
            for chunk in chunks:
                chunk_lines = chunk["line_end"] - chunk["line_start"] + 1
                if chunk_lines > 200:
                    # í° ì²­í¬ë¥¼ ì—¬ëŸ¬ ê°œë¡œ ë¶„í• 
                    split_chunks = self._split_large_chunk(chunk, lines, max_size=200)
                    final_chunks.extend(split_chunks)
                else:
                    final_chunks.append(chunk)
            
            # ì²­í¬ ID ìƒì„± (ê³ ìœ ì„± ë³´ì¥: file_path + line_start + line_end + type + code)
            seen_ids = set()
            unique_chunks = []
            for chunk in final_chunks:
                chunk_code = chunk["code"]
                line_start = chunk.get("line_start", 0)
                line_end = chunk.get("line_end", 0)
                chunk_type = chunk.get("type", "unknown")
                
                # ê³ ìœ í•œ chunk_id ìƒì„± (ìœ„ì¹˜ ì •ë³´ í¬í•¨)
                chunk_id = hashlib.md5(
                    (
                        str(file_path) + 
                        str(line_start) + 
                        str(line_end) + 
                        chunk_type + 
                        chunk_code
                    ).encode()
                ).hexdigest()
                
                # ì¤‘ë³µ ì²´í¬: ê°™ì€ IDê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
                if chunk_id not in seen_ids:
                    seen_ids.add(chunk_id)
                    chunk["chunk_id"] = chunk_id
                    chunk["file"] = str(file_path)
                    unique_chunks.append(chunk)
                else:
                    logger.debug(
                        f"âš ï¸ ì¤‘ë³µ ì²­í¬ ìŠ¤í‚µ: {file_path.name}:{line_start}-{line_end} "
                        f"(type={chunk_type})"
                    )
            
            return unique_chunks

        loop = asyncio.get_event_loop()
        chunks = await loop.run_in_executor(None, _parse)

        return chunks
    
    def _select_parser(self, file_path: Path) -> str:
        """
        íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ìµœì  íŒŒì„œ ì„ íƒ

        ìš°ì„ ìˆœìœ„:
        1. Tree-sitter ì§€ì› ì–¸ì–´: Tree-sitter (ì‹¤ì œ AST ê¸°ë°˜)
        2. ê¸°íƒ€: ë¹ˆ ì¤„ ê¸°ì¤€ (í´ë°±)

        Returns:
            "tree-sitter" ë˜ëŠ” "blank-line"
        """
        # Tree-sitter ì§€ì› ì–¸ì–´ í™•ì¸
        if is_language_supported(file_path.suffix):
            return "tree-sitter"

        # ê¸°íƒ€ëŠ” ë¹ˆ ì¤„ ê¸°ì¤€
        return "blank-line"
    
    def _extract_blank_line_chunks(
        self, content: str, lines: list[str]
    ) -> list[dict[str, Any]]:
        """
        ë¹ˆ ì¤„ 2ê°œ ì´ìƒ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)
        """
        chunks = []
        current_chunk = []
        blank_count = 0
        chunk_start_line = 1
        
        for line_idx, line in enumerate(lines, start=1):
            if line.strip() == "":
                blank_count += 1
            else:
                if blank_count >= 2 and current_chunk:
                    chunk_code = "\n".join(current_chunk)
                    chunk_end_line = line_idx - blank_count - 1
                    
                    chunks.append({
                        "code": chunk_code,
                        "type": "code_block",
                        "line_start": chunk_start_line,
                        "line_end": chunk_end_line,
                    })
                    
                    current_chunk = []
                    chunk_start_line = line_idx
                
                current_chunk.append(line)
                blank_count = 0
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunk_code = "\n".join(current_chunk)
            chunks.append({
                "code": chunk_code,
                "type": "code_block",
                "line_start": chunk_start_line,
                "line_end": len(lines),
            })
        
        return chunks
    
    def _split_large_chunk(
        self, chunk: dict[str, Any], lines: list[str], max_size: int = 200
    ) -> list[dict[str, Any]]:
        """
        í° ì²­í¬ë¥¼ ìµœëŒ€ í¬ê¸°ë¡œ ë¶„í• 
        """
        chunk_start = chunk["line_start"]
        chunk_end = chunk["line_end"]
        chunk_lines = chunk_end - chunk_start + 1
        
        if chunk_lines <= max_size:
            return [chunk]
        
        split_chunks = []
        current_start = chunk_start
        
        while current_start <= chunk_end:
            current_end = min(current_start + max_size - 1, chunk_end)
            chunk_code = "\n".join(lines[current_start - 1 : current_end])
            
            split_chunks.append({
                "code": chunk_code,
                "type": chunk.get("type", "code_block"),
                "line_start": current_start,
                "line_end": current_end,
                "name": chunk.get("name", ""),
            })
            
            current_start = current_end + 1
        
        return split_chunks

    async def _store_embeddings(
        self, collection, chunks: list[dict[str, Any]]
    ) -> int:
        """
        ì²­í¬ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ChromaDBì— ì €ì¥

        Args:
            collection: ChromaDB collection
            chunks: ì½”ë“œ ì²­í¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì €ì¥ëœ ì²­í¬ ìˆ˜
        """
        if not chunks:
            return 0

        # ì„ë² ë”© ìƒì„± (ë™ê¸°)
        def _embed():
            texts = [chunk["code"] for chunk in chunks]
            embeddings = self.model.encode(texts, show_progress_bar=True)
            return embeddings.tolist()

        loop = asyncio.get_event_loop()
        embeddings = await loop.run_in_executor(None, _embed)

        # ChromaDBì— ë°°ì¹˜ ì €ì¥ (ì¤‘ë³µ ID ì²´í¬)
        batch_size = 1000
        saved_count = 0

        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i : i + batch_size]
            batch_embeddings = embeddings[i : i + batch_size]
            
            # ì¤‘ë³µ ID í•„í„°ë§ (ChromaDB ì €ì¥ ì „ ìµœì¢… ì²´í¬)
            unique_batch = []
            unique_embeddings = []
            seen_batch_ids = set()
            
            for idx, chunk in enumerate(batch_chunks):
                chunk_id = chunk["chunk_id"]
                if chunk_id not in seen_batch_ids:
                    seen_batch_ids.add(chunk_id)
                    unique_batch.append(chunk)
                    unique_embeddings.append(batch_embeddings[idx])
            
            if not unique_batch:
                continue

            try:
                collection.add(
                    ids=[chunk["chunk_id"] for chunk in unique_batch],
                    embeddings=unique_embeddings,
                    documents=[chunk["code"] for chunk in unique_batch],
                    metadatas=[
                        {
                            "file": chunk["file"],
                            "type": chunk["type"],
                            "line_start": chunk.get("line_start", 0),
                            "line_end": chunk.get("line_end", 0),
                        }
                        for chunk in unique_batch
                    ],
                )
                saved_count += len(unique_batch)
                logger.info(f"ğŸ“Š {saved_count}/{len(chunks)} ì²­í¬ ì €ì¥ ì¤‘...")
            except Exception as e:
                # ì¤‘ë³µ ID ì˜¤ë¥˜ ë°œìƒ ì‹œ ê°œë³„ ì €ì¥ìœ¼ë¡œ ì¬ì‹œë„
                logger.warning(f"âš ï¸ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨, ê°œë³„ ì €ì¥ ì‹œë„: {str(e)}")
                for chunk, embedding in zip(unique_batch, unique_embeddings):
                    try:
                        collection.add(
                            ids=[chunk["chunk_id"]],
                            embeddings=[embedding],
                            documents=[chunk["code"]],
                            metadatas=[{
                                "file": chunk["file"],
                                "type": chunk["type"],
                                "line_start": chunk.get("line_start", 0),
                                "line_end": chunk.get("line_end", 0),
                            }],
                        )
                        saved_count += 1
                    except Exception as e2:
                        logger.warning(
                            f"âš ï¸ ì²­í¬ ì €ì¥ ì‹¤íŒ¨ (ID: {chunk['chunk_id'][:8]}...): {str(e2)}"
                        )

        return saved_count
