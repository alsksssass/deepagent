"""RepoSynthesizerAgent - ì—¬ëŸ¬ ë ˆí¬ì§€í† ë¦¬ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ëŠ” ì—ì´ì „íŠ¸"""

import logging
import asyncio
import json
import os
from pathlib import Path
from typing import Any, Dict, List, Optional
from datetime import datetime

from langchain_core.messages import SystemMessage, HumanMessage

from shared.storage import ResultStore
from shared.utils.prompt_loader import PromptLoader
from shared.utils.token_tracker import TokenTracker
from shared.utils.skill_level_calculator import SkillLevelCalculator
from .schemas import (
    RepoSynthesizerContext,
    RepoSynthesizerResponse,
    UserAnalysisResult,
    LanguageInfo,
    LLMAnalysisResult,
)

# Response í´ë˜ìŠ¤ import (load_resultì— í•„ìš”)
from agents.static_analyzer.schemas import StaticAnalyzerResponse
from agents.user_aggregator.schemas import UserAggregatorResponse
from agents.user_skill_profiler.schemas import UserSkillProfilerResponse

logger = logging.getLogger(__name__)


class RepoSynthesizerAgent:
    """
    ì—¬ëŸ¬ ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ëŠ” ì—ì´ì „íŠ¸

    ì£¼ìš” ê¸°ëŠ¥:
    1. ê° ë ˆí¬ì§€í† ë¦¬ ê²°ê³¼ ë¡œë“œ ë° ê²€ì¦
    2. í†µê³„ ì§‘ê³„ (ì´ ì»¤ë°‹ ìˆ˜, ì´ íŒŒì¼ ìˆ˜ ë“±)
    3. ë ˆí¬ì§€í† ë¦¬ ê°„ ë¹„êµ ë¶„ì„
    4. LLMì„ ì´ìš©í•œ ì¢…í•© ë¶„ì„ ë° ê°œì„  ë°©í–¥ ì œì‹œ
    5. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    """

    def __init__(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        # PromptLoaderë¡œ LLM ë¡œë“œ
        self.llm = PromptLoader.get_llm("repo_synthesizer")
        model_id = PromptLoader.get_model("repo_synthesizer")
        
        # í•˜ì´ë¸Œë¦¬ë“œ: ìŠ¤í‚¤ë§ˆ ìë™ ì£¼ì…
        self.prompts = PromptLoader.load_with_schema(
            "repo_synthesizer",
            response_schema_class=LLMAnalysisResult
        )
        
        logger.info(f"âœ… RepoSynthesizer: LLM ì´ˆê¸°í™” ì™„ë£Œ - {model_id}")

    async def run(self, context: RepoSynthesizerContext) -> RepoSynthesizerResponse:
        """
        ì—¬ëŸ¬ ë ˆí¬ì§€í† ë¦¬ ê²°ê³¼ ì¢…í•©

        Args:
            context: RepoSynthesizerContext

        Returns:
            RepoSynthesizerResponse
        """
        logger.info(f"ğŸ”¬ RepoSynthesizer: {len(context.repo_results)}ê°œ ë ˆí¬ì§€í† ë¦¬ ì¢…í•© ì‹œì‘")

        try:
            # 1. ê° ë ˆí¬ ê²°ê³¼ ìš”ì•½ ì¶”ì¶œ
            repo_summaries = await self._extract_repo_summaries(context.repo_results)

            # 2. í†µê³„ ì§‘ê³„
            total_commits = sum(s.get("total_commits", 0) for s in repo_summaries)
            total_files = sum(s.get("total_files", 0) for s in repo_summaries)
            successful = sum(1 for s in repo_summaries if s.get("status") == "success")
            failed = len(repo_summaries) - successful

            logger.info(f"   ì´ ì»¤ë°‹: {total_commits}, ì´ íŒŒì¼: {total_files}")
            logger.info(f"   ì„±ê³µ: {successful}ê°œ, ì‹¤íŒ¨: {failed}ê°œ")

            # 3. UserAnalysisResult ìƒì„±
            user_analysis_result = await self._generate_user_analysis_result(
                context.repo_results,
                context.main_task_uuid,
                context.main_base_path,
            )
            context.user_analysis_result = user_analysis_result

            # 4. LLM ì¢…í•© ë¶„ì„ ë° ê°œì„  ë°©í–¥ ì œì‹œ
            llm_analysis = await self._generate_llm_analysis(
                repo_summaries=repo_summaries,
                total_commits=total_commits,
                total_files=total_files,
                successful=successful,
                failed=failed,
                target_user=context.target_user,
                user_analysis_result=user_analysis_result,
            )

            # 5. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
            report_content = self._generate_synthesis_report(
                repo_summaries=repo_summaries,
                total_commits=total_commits,
                total_files=total_files,
                successful=successful,
                failed=failed,
                target_user=context.target_user,
                user_analysis_result=user_analysis_result,
                llm_analysis=llm_analysis,
            )

            # 6. UserAnalysisResultì˜ markdown, ì–¸ì–´ë³„ ì •ë³´ ì—…ë°ì´íŠ¸
            if user_analysis_result:
                user_analysis_result.markdown = report_content
                
                # LLMì´ ìƒì„±í•œ ì–¸ì–´ë³„ ì •ë³´ë¥¼ UserAnalysisResultì— ë™ì  í•„ë“œë¡œ ì‚½ì…
                if llm_analysis:
                    for attr_name in dir(llm_analysis):
                        if not attr_name.startswith('_') and attr_name not in [
                            'overall_assessment',
                            'strengths',
                            'improvement_recommendations',
                            'role_suitability',
                            'model_config',
                            'model_fields',
                            'model_computed_fields',
                            'model_dump',
                            'model_dump_json',
                            'model_validate',
                            'model_validate_json',
                            'model_copy',
                            'model_post_init',
                            'model_json_schema',
                            'model_parametrized_name',
                            'model_rebuild',
                            'model_fields_set'
                        ]:
                            attr_value = getattr(llm_analysis, attr_name, None)
                            # LanguageInfo íƒ€ì…ì¸ì§€ í™•ì¸
                            if isinstance(attr_value, dict) and all(
                                k in attr_value
                                for k in ['stack', 'level', 'exp']
                            ):
                                lang_info = LanguageInfo(**attr_value)
                                setattr(
                                    user_analysis_result,
                                    attr_name,
                                    lang_info
                                )
                                logger.info(
                                    f"   UserAnalysisResult.{attr_name} "
                                    f"ì—…ë°ì´íŠ¸ ì™„ë£Œ"
                                )
                            elif isinstance(attr_value, LanguageInfo):
                                setattr(
                                    user_analysis_result,
                                    attr_name,
                                    attr_value
                                )
                                logger.info(
                                    f"   UserAnalysisResult.{attr_name} "
                                    f"ì—…ë°ì´íŠ¸ ì™„ë£Œ"
                                )
                
                logger.info(
                    "   UserAnalysisResult.markdownì— "
                    "ì „ì²´ ë¦¬í¬íŠ¸ ë‚´ìš© ì—…ë°ì´íŠ¸ ì™„ë£Œ"
                )

            # 7. ë¦¬í¬íŠ¸ ì €ì¥
            main_store = ResultStore(context.main_task_uuid, Path(context.main_base_path))
            report_path = main_store.save_report("synthesis_report.md", report_content)

            logger.info(f"âœ… RepoSynthesizer: ì¢…í•© ì™„ë£Œ")
            logger.info(f"   ë¦¬í¬íŠ¸: {report_path}")

            return RepoSynthesizerResponse(
                status="success",
                total_repos=len(repo_summaries),
                successful_repos=successful,
                failed_repos=failed,
                total_commits=total_commits,
                total_files=total_files,
                synthesis_report_path=str(report_path),
                synthesis_report_markdown=report_content,
                repo_summaries=repo_summaries,
                user_analysis_result=user_analysis_result,
                llm_analysis=llm_analysis,
            )

        except Exception as e:
            logger.error(f"âŒ RepoSynthesizer ì‹¤íŒ¨: {e}", exc_info=True)
            import traceback
            logger.error(f"ìƒì„¸ Traceback:\n{traceback.format_exc()}")
            return RepoSynthesizerResponse(
                status="failed",
                error=str(e),
            )

    async def _extract_repo_summaries(
        self, repo_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ê° ë ˆí¬ì§€í† ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶”ì¶œ"""
        summaries = []

        for result in repo_results:
            try:
                # ì—ëŸ¬ ë°œìƒí•œ ë ˆí¬ ì²˜ë¦¬
                if result.get("error_message"):
                    summaries.append({
                        "git_url": result.get("git_url", "unknown"),
                        "task_uuid": result.get("task_uuid", ""),
                        "status": "failed",
                        "error": result.get("error_message"),
                        "total_commits": 0,
                        "total_files": 0,
                    })
                    continue

                # ì„±ê³µí•œ ë ˆí¬ ìš”ì•½
                task_uuid = result.get("task_uuid", "")
                base_path = result.get("base_path", "")

                # ResultStoreì—ì„œ ì¶”ê°€ ì •ë³´ ë¡œë“œ ì‹œë„
                try:
                    if task_uuid and base_path:
                        store = ResultStore(task_uuid, Path(base_path))
                        
                        # UserAggregator ê²°ê³¼ ë¡œë“œ (í’ˆì§ˆ ì ìˆ˜ ë“±)
                        user_agg_response = store.load_result("user_aggregator", UserAggregatorResponse)
                        user_agg = user_agg_response.model_dump() if user_agg_response else None
                        quality_score = None
                        if user_agg and user_agg.get("aggregate_stats"):
                            quality_stats = user_agg["aggregate_stats"].get("quality_stats", {})
                            quality_score = quality_stats.get("mean_score")

                        summaries.append({
                            "git_url": result.get("git_url", ""),
                            "task_uuid": task_uuid,
                            "base_path": base_path,
                            "status": "success",
                            "total_commits": result.get("total_commits", 0),
                            "total_files": result.get("total_files", 0),
                            "final_report_path": result.get("final_report_path"),
                            "quality_score": quality_score,
                        })
                    else:
                        summaries.append({
                            "git_url": result.get("git_url", ""),
                            "task_uuid": task_uuid,
                            "base_path": base_path,
                            "status": "success",
                            "total_commits": result.get("total_commits", 0),
                            "total_files": result.get("total_files", 0),
                        })
                except Exception as e:
                    logger.warning(f"âš ï¸ ResultStore ë¡œë“œ ì‹¤íŒ¨: {e}")
                    summaries.append({
                        "git_url": result.get("git_url", ""),
                        "task_uuid": task_uuid,
                        "base_path": base_path,
                        "status": "success",
                        "total_commits": result.get("total_commits", 0),
                        "total_files": result.get("total_files", 0),
                    })

            except Exception as e:
                logger.warning(f"âš ï¸ ë ˆí¬ ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                summaries.append({
                    "git_url": result.get("git_url", "unknown"),
                    "status": "failed",
                    "error": str(e),
                })

        return summaries


    async def _generate_llm_analysis(
        self,
        repo_summaries: List[Dict[str, Any]],
        total_commits: int,
        total_files: int,
        successful: int,
        failed: int,
        target_user: str | None,
        user_analysis_result: Optional[UserAnalysisResult],
    ) -> Optional[LLMAnalysisResult]:
        """
        LLMì„ ì´ìš©í•œ ì¢…í•© ë¶„ì„ ë° ê°œì„  ë°©í–¥ ì œì‹œ
        
        Returns:
            LLMAnalysisResult ë˜ëŠ” None
        """
        try:
            # ë ˆí¬ì§€í† ë¦¬ ìš”ì•½ í¬ë§·íŒ…
            repo_summaries_text = self._format_repo_summaries(repo_summaries)
            
            # ê° repoì˜ ìƒì„¸ JSON ë°ì´í„° ìˆ˜ì§‘
            repo_json_data = await self._collect_repo_json_data(repo_summaries)
            
            # ìœ ì € ë¶„ì„ ê²°ê³¼ í¬ë§·íŒ…
            user_analysis_text = ""
            if user_analysis_result:
                user_analysis_text = self._format_user_analysis_result(user_analysis_result)
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¤€ë¹„
            prompt_variables = {
                "total_repos": len(repo_summaries),
                "successful_repos": successful,
                "failed_repos": failed,
                "total_commits": total_commits,
                "total_files": total_files,
                "target_user": target_user if target_user else "ì „ì²´ ìœ ì €",
                "repo_summaries": repo_summaries_text,
                "repo_json_data": repo_json_data,
                "user_analysis_result": user_analysis_text if user_analysis_text else "ì—†ìŒ",
                
            }
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± (json_schema ë³€ìˆ˜ ìë™ ì£¼ì…)
            system_prompt = PromptLoader.format(
                self.prompts["system_prompt"],
                json_schema=self.prompts.get("json_schema", "")
            )
            user_prompt = PromptLoader.format(
                self.prompts["user_template"],
                **prompt_variables
            )
            
            # LLM í˜¸ì¶œ
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ]
            
            logger.info("ğŸ¤– LLM ì¢…í•© ë¶„ì„ ì‹œì‘...")
            response = await self.llm.ainvoke(messages)
            TokenTracker.record_usage(
                "repo_synthesizer",
                response,
                model_id=PromptLoader.get_model("repo_synthesizer")
            )
            
            # JSON íŒŒì‹±
            content = response.content
            try:
                if "```json" in content:
                    json_str = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    json_str = content.split("```")[1].split("```")[0].strip()
                else:
                    json_str = content.strip()
                
                analysis_data = json.loads(json_str)
                
                # ëˆ„ë½ëœ í•„ë“œ ë³´ì™„ (category í•„ë“œê°€ ì—†ëŠ” ê²½ìš°)
                if "improvement_recommendations" in analysis_data:
                    for rec in analysis_data["improvement_recommendations"]:
                        if "category" not in rec or not rec.get("category"):
                            # titleì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ë¡ í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
                            rec["category"] = "ì¼ë°˜"
                
                # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
                try:
                    llm_result = LLMAnalysisResult(**analysis_data)
                    logger.info("âœ… LLM ì¢…í•© ë¶„ì„ ì™„ë£Œ")
                    return llm_result
                except Exception as validation_error:
                    # Pydantic ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë” ìì„¸í•œ ë¡œê¹…
                    from pydantic import ValidationError
                    if isinstance(validation_error, ValidationError):
                        error_count = len(validation_error.errors())
                        logger.warning(f"âš ï¸ LLM ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨: {error_count} validation errors for LLMAnalysisResult")
                        for err in validation_error.errors():
                            logger.warning(f"  - Field: {'.'.join(str(loc) for loc in err['loc'])}, Type: {err['type']}, Msg: {err['msg']}")
                    else:
                        logger.warning(f"âš ï¸ LLM ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨: {validation_error}")
                    logger.debug(f"ì‘ë‹µ ë°ì´í„°: {json.dumps(analysis_data, indent=2, ensure_ascii=False)[:1000]}")
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ ì¬ì‹œë„
                    try:
                        # í•„ìˆ˜ í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                        if "overall_assessment" not in analysis_data:
                            analysis_data["overall_assessment"] = "ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        if "strengths" not in analysis_data:
                            analysis_data["strengths"] = []
                        if "improvement_recommendations" not in analysis_data:
                            analysis_data["improvement_recommendations"] = []

                        # role_suitability í•„ìˆ˜ 5ê°œ ì—­í•  í™•ì¸
                        if "role_suitability" not in analysis_data:
                            analysis_data["role_suitability"] = {}
                        required_roles = ["Backend", "Frontend", "DevOps", "Data Science", "Fullstack"]
                        for role in required_roles:
                            if role not in analysis_data["role_suitability"]:
                                analysis_data["role_suitability"][role] = f"{role} (í‰ê°€ ë¶ˆê°€): ë°ì´í„° ë¶€ì¡±"

                        # hiring_decision í•„ìˆ˜ í•„ë“œ í™•ì¸
                        if "hiring_decision" not in analysis_data:
                            analysis_data["hiring_decision"] = {}
                        hiring = analysis_data["hiring_decision"]

                        if "immediate_readiness" not in hiring:
                            hiring["immediate_readiness"] = "í‰ê°€ ë¶ˆê°€"
                        if "onboarding_period" not in hiring:
                            hiring["onboarding_period"] = "ë¯¸ì •"
                        if "hiring_recommendation" not in hiring:
                            hiring["hiring_recommendation"] = "ì‹ ì¤‘ ê²€í† "
                        if "hiring_decision_reason" not in hiring:
                            hiring["hiring_decision_reason"] = "ë¶„ì„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ì •í™•í•œ í‰ê°€ê°€ ì–´ë µìŠµë‹ˆë‹¤."
                        if "salary_recommendation" not in hiring:
                            hiring["salary_recommendation"] = "ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í‰ê°€ ë¶ˆê°€"
                        if "estimated_salary_range" not in hiring:
                            hiring["estimated_salary_range"] = "í‰ê°€ ë¶ˆê°€"
                        if "technical_risks" not in hiring:
                            hiring["technical_risks"] = []
                        if "expected_contributions" not in hiring:
                            hiring["expected_contributions"] = []

                        llm_result = LLMAnalysisResult(**analysis_data)
                        logger.info("âœ… LLM ì¢…í•© ë¶„ì„ ì™„ë£Œ (ê¸°ë³¸ê°’ ë³´ì™„)")
                        return llm_result
                    except Exception as e2:
                        logger.warning(f"âš ï¸ LLM ì‘ë‹µ ë³µêµ¬ ì‹¤íŒ¨: {e2}")
                        return None
                
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.debug(f"ì‘ë‹µ ë‚´ìš©: {content[:500]}")
                return None
            except Exception as e:
                logger.warning(f"âš ï¸ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.debug(f"ì‘ë‹µ ë‚´ìš©: {content[:500]}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ LLM ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
            return None

    async def _collect_repo_json_data(self, repo_summaries: List[Dict[str, Any]]) -> str:
        """
        ê° ë ˆí¬ì§€í† ë¦¬ì˜ ìƒì„¸ JSON ë°ì´í„° ìˆ˜ì§‘
        
        Returns:
            JSON í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…ëœ ë¬¸ìì—´
        """
        repo_json_list = []
        
        for summary in repo_summaries:
            if summary.get("status") != "success":
                continue
            
            task_uuid = summary.get("task_uuid", "")
            base_path = summary.get("base_path")
            git_url = summary.get("git_url", "")
            
            if not task_uuid or not base_path:
                continue
            
            try:
                store = ResultStore(task_uuid, Path(base_path))
                
                # ì£¼ìš” ë¶„ì„ ê²°ê³¼ ë¡œë“œ
                repo_data = {
                    "git_url": git_url,
                    "task_uuid": task_uuid,
                }
                
                # StaticAnalyzer ê²°ê³¼ (í•µì‹¬ ì •ë³´ë§Œ)
                try:
                    static_response = store.load_result("static_analyzer", StaticAnalyzerResponse)
                    if static_response:
                        static_dict = static_response.model_dump()
                        # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í•„ë“œ)
                        repo_data["static_analysis"] = {
                            "loc_stats": static_dict.get("loc_stats", {}),
                            "complexity": static_dict.get("complexity", {}),
                            "type_check": static_dict.get("type_check", {}),
                        }
                except Exception as e:
                    logger.debug(f"Static analyzer ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                # UserAggregator ê²°ê³¼ (ì „ì²´ í†µê³„)
                try:
                    user_agg_response = store.load_result("user_aggregator", UserAggregatorResponse)
                    if user_agg_response:
                        agg_dict = user_agg_response.model_dump()
                        # aggregate_stats ì „ì²´ í¬í•¨ (í’ˆì§ˆ, ê¸°ìˆ , ë³µì¡ë„ í†µê³„)
                        repo_data["user_aggregator"] = {
                            "aggregate_stats": agg_dict.get("aggregate_stats", {})
                        }
                except Exception as e:
                    logger.debug(f"User aggregator ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                # UserSkillProfiler ê²°ê³¼ (ë¶„ì„ì— í•µì‹¬ì ì¸ í•„ë“œë§Œ)
                try:
                    skill_profile_response = store.load_result("user_skill_profiler", UserSkillProfilerResponse)
                    if skill_profile_response:
                        skill_dict = skill_profile_response.model_dump()
                        skill_profile_data = skill_dict.get("skill_profile", {})
                        
                        # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í•„ë“œ)
                        repo_data["skill_profile"] = {
                            "total_skills": skill_profile_data.get("total_skills", 0),
                            "skills_by_level": skill_profile_data.get("skills_by_level", {}),
                            "skills_by_category": skill_profile_data.get("skills_by_category", {}),
                            "top_skills": skill_profile_data.get("top_skills", [])[:10],  # ìƒìœ„ 10ê°œë§Œ
                            "total_experience": skill_profile_data.get("total_experience", 0),
                            "level": skill_profile_data.get("level", {}),
                            "developer_type_coverage": skill_profile_data.get("developer_type_coverage", {}),
                            "developer_type_levels": skill_profile_data.get("developer_type_levels", {}),
                            "category_coverage": skill_profile_data.get("category_coverage", {}),
                            "total_coverage": skill_profile_data.get("total_coverage", 0),
                        }
                except Exception as e:
                    logger.debug(f"Skill profiler ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                repo_json_list.append(repo_data)
                
            except Exception as e:
                logger.warning(f"âš ï¸ ë ˆí¬ì§€í† ë¦¬ {git_url} JSON ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        if not repo_json_list:
            logger.warning("   ë ˆí¬ì§€í† ë¦¬ JSON ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return "ë ˆí¬ì§€í† ë¦¬ JSON ë°ì´í„° ì—†ìŒ"
        
        logger.info(f"   ìˆ˜ì§‘ëœ JSON ë°ì´í„°: {len(repo_json_list)}ê°œ ë ˆí¬ì§€í† ë¦¬")
        
        # JSON í¬ë§·íŒ… (ê°€ë…ì„±ì„ ìœ„í•´ ë“¤ì—¬ì“°ê¸°)
        json_str = json.dumps(repo_json_list, indent=2, ensure_ascii=False)
        logger.info(f"   JSON ë°ì´í„° í¬ê¸°: {len(json_str):,} ë¬¸ì")
        
        return json_str

    def _format_repo_summaries(self, repo_summaries: List[Dict[str, Any]]) -> str:
        """ë ˆí¬ì§€í† ë¦¬ ìš”ì•½ í¬ë§·íŒ…"""
        formatted = []
        for i, summary in enumerate(repo_summaries, 1):
            status_emoji = "âœ…" if summary.get("status") == "success" else "âŒ"
            git_url = summary.get("git_url", "unknown")
            
            repo_text = f"\n{i}. {status_emoji} {git_url}\n"
            if summary.get("status") == "success":
                repo_text += f"   - ì»¤ë°‹ ìˆ˜: {summary.get('total_commits', 0):,}ê°œ\n"
                repo_text += f"   - íŒŒì¼ ìˆ˜: {summary.get('total_files', 0):,}ê°œ\n"
                if summary.get("quality_score") is not None:
                    repo_text += f"   - í’ˆì§ˆ ì ìˆ˜: {summary.get('quality_score'):.2f}/10\n"
            else:
                repo_text += f"   - ì—ëŸ¬: {summary.get('error', 'Unknown error')}\n"
            
            formatted.append(repo_text)
        
        return "\n".join(formatted)

    def _format_user_analysis_result(self, user_analysis_result: UserAnalysisResult) -> str:
        """ìœ ì € ë¶„ì„ ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        
        formatted.append(f"ì½”ë“œ í’ˆì§ˆ ì ìˆ˜: {user_analysis_result.clean_code:.2f}/10")
        
        if user_analysis_result.role:
            formatted.append(f"\nì—­í• ë³„ ê¸°ìˆ ìŠ¤íƒ ë³´ìœ ìœ¨:")
            for role, percentage in sorted(user_analysis_result.role.items(), key=lambda x: x[1], reverse=True):
                formatted.append(f"  - {role}: {percentage}%")
        
        if hasattr(user_analysis_result, 'python') and user_analysis_result.python:
            python = user_analysis_result.python
            formatted.append(f"\nPython ë¶„ì„:")
            formatted.append(f"  - ìˆ™ë ¨ë„ ë ˆë²¨: {python.level}")
            formatted.append(f"  - ê²½í—˜ì¹˜: {python.exp:,}")
            if python.stack:
                formatted.append(f"  - ê¸°ìˆ  ìŠ¤íƒ: {', '.join(python.stack)}")
        
        return "\n".join(formatted)

    async def _generate_user_analysis_result(
        self,
        repo_results: List[Dict[str, Any]],
        main_task_uuid: str,
        main_base_path: str,
    ) -> Optional[UserAnalysisResult]:
        """
        target_userì˜ ì¢…í•© ë¶„ì„ ê²°ê³¼ ìƒì„±
        
        Returns:
            UserAnalysisResult ë˜ëŠ” None
        """
        try:
            # ëª¨ë“  ë ˆí¬ì§€í† ë¦¬ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
            all_quality_scores = []  # í’ˆì§ˆ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
            all_skills = []  # ëª¨ë“  ë ˆí¬ì˜ ìŠ¤í‚¬ ë°ì´í„° (ì¤‘ë³µ í¬í•¨)
            all_tech_stack = set()  # ì „ì²´ ê¸°ìˆ  ìŠ¤íƒ (ì¤‘ë³µ ì œê±°ìš©)
            
            for result in repo_results:
                if result.get("error_message"):
                    continue
                    
                task_uuid = result.get("task_uuid", "")
                base_path = result.get("base_path", "")
                
                if not task_uuid or not base_path:
                    continue
                
                try:
                    store = ResultStore(task_uuid, Path(base_path))
                    logger.info(f"ğŸ“‚ RepoSynthesizer ë°ì´í„° ë¡œë“œ ì‹œì‘: task_uuid={task_uuid}")
                    logger.info(f"   base_path: {base_path}")
                    logger.info(f"   ResultStore results_dir: {store.results_dir}")
                    
                    # total_skill.json ë¡œë“œ (ì¼ë°˜ JSON íŒŒì¼)
                    try:
                        import json
                        logger.info(f"   ğŸ“¥ total_skill.json ë¡œë“œ ì‹œë„: {base_path}/total_skill.json")
                        total_skill_content = store.load_debug_file("total_skill.json")
                        total_skill_data = json.loads(total_skill_content)
                        if isinstance(total_skill_data, list):
                            all_skills += total_skill_data
                            logger.info(f"   âœ… total_skill.json ë¡œë“œ ì„±ê³µ: {len(total_skill_data)}ê°œ ìŠ¤í‚¬")
                        else:
                            logger.debug(f"total_skill.jsonì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹˜: {type(total_skill_data)}")
                    except FileNotFoundError:
                        logger.warning(f"   âš ï¸ total_skill.json íŒŒì¼ ì—†ìŒ: task_uuid={task_uuid}, base_path={base_path}")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ total_skill.json ë¡œë“œ ì‹¤íŒ¨: {e}, base_path={base_path}")
                    
                    
                    # 1. UserAggregator ê²°ê³¼ì—ì„œ í’ˆì§ˆ ì ìˆ˜ ìˆ˜ì§‘
                    try:
                        logger.info(f"   ğŸ“¥ user_aggregator.json ë¡œë“œ ì‹œë„: {store.results_dir}/user_aggregator.json")
                        user_agg_response = store.load_result("user_aggregator", UserAggregatorResponse)
                        user_agg = user_agg_response.model_dump() if user_agg_response else None
                        if user_agg and user_agg.get("aggregate_stats"):
                            quality_stats = user_agg["aggregate_stats"].get("quality_stats", {})
                            avg_score = quality_stats.get("average_score")
                            if avg_score is not None:
                                all_quality_scores.append(avg_score)
                                logger.info(f"   âœ… user_aggregator.json ë¡œë“œ ì„±ê³µ: í’ˆì§ˆ ì ìˆ˜={avg_score}")
                        else:
                            logger.warning(f"   âš ï¸ user_aggregator ê²°ê³¼ì— aggregate_stats ì—†ìŒ")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ user_aggregator.json ë¡œë“œ ì‹¤íŒ¨: {e}")
                    
                    # 2. UserSkillProfiler ê²°ê³¼ì—ì„œ ìŠ¤í‚¬ ë°ì´í„° ìˆ˜ì§‘
                    try:
                        logger.info(f"   ğŸ“¥ user_skill_profiler.json ë¡œë“œ ì‹œë„: {store.results_dir}/user_skill_profiler.json")
                        skill_profile_response = store.load_result("user_skill_profiler", UserSkillProfilerResponse)
                        skill_profile = skill_profile_response.model_dump() if skill_profile_response else None
                        if skill_profile:
                            logger.info(f"   âœ… user_skill_profiler.json ë¡œë“œ ì„±ê³µ")
                        else:
                            logger.warning(f"   âš ï¸ user_skill_profiler ê²°ê³¼ê°€ None")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ user_skill_profiler.json ë¡œë“œ ì‹¤íŒ¨: {e}")
                        skill_profile = None
                    
                    if skill_profile and skill_profile.get("skill_profile"):
                        # top_skillsì—ì„œ ìŠ¤í‚¬ ì •ë³´ ì¶”ì¶œ
                        top_skills = skill_profile["skill_profile"].get("top_skills", [])
                        logger.info(f"   ğŸ“Š top_skills ìˆ˜ì§‘: {len(top_skills)}ê°œ")
                        for skill in top_skills:
                            # all_skillsì— ì¶”ê°€ (ë ˆë²¨ ê³„ì‚°ìš©)
                            # top_skillsëŠ” ì´ë¯¸ base_scoreë¥¼ í¬í•¨í•œ ìŠ¤í‚¬ ê°ì²´
                            all_skills.append(skill)
                            
                            # ê¸°ìˆ  ìŠ¤íƒ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                            skill_category = skill.get("category", "")
                            if skill_category:
                                all_tech_stack.add(skill_category)
                        logger.info(f"   âœ… top_skillsë¥¼ all_skillsì— ì¶”ê°€ ì™„ë£Œ: {len(top_skills)}ê°œ")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ë ˆí¬ì§€í† ë¦¬ {task_uuid} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    continue
            
            # ë°ì´í„° ì§‘ê³„
            logger.info(f"   í’ˆì§ˆ ì ìˆ˜: {len(all_quality_scores)}ê°œ")
            logger.info(f"   ìˆ˜ì§‘ëœ ìŠ¤í‚¬: {len(all_skills)}ê°œ (ì¤‘ë³µ í¬í•¨)")
            logger.info(f"   ê³ ìœ  ê¸°ìˆ  ìŠ¤íƒ: {len(all_tech_stack)}ê°œ")
            
            # 1. clean_code ì ìˆ˜ ê³„ì‚° (í‰ê· )
            clean_code_score = 0.0
            if all_quality_scores:
                clean_code_score = sum(all_quality_scores) / len(all_quality_scores)
            
            # 2. SkillLevelCalculatorë¡œ ì •í™•í•œ ë ˆë²¨ ê³„ì‚°
            total_experience = SkillLevelCalculator.calculate_total_experience(all_skills)
            logger.info(f"   ëª¨ë“  ìŠ¤í‚¬: {all_skills}")
            level_info = SkillLevelCalculator.calculate_level(total_experience)
            
            logger.info(f"   ì´ ê²½í—˜ì¹˜: {total_experience:,} EXP")
            logger.info(f"   ë ˆë²¨: {level_info['level']} ({level_info['level_name']})")
            
            # 3. ê°œë°œì íƒ€ì…ë³„ ì»¤ë²„ë¦¬ì§€ ë° ë ˆë²¨ ê³„ì‚°
            chromadb_persist_dir = os.getenv(
                "CHROMADB_PERSIST_DIR", str(Path(main_base_path).parent.parent / "chroma_db_skill_charts")
            )
            developer_type_coverage = await SkillLevelCalculator.calculate_developer_type_coverage(
                all_skills, chromadb_persist_dir
            )
            
            # developer_type_coverageê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš° ì²˜ë¦¬
            if developer_type_coverage is None:
                developer_type_coverage = {}
                logger.warning("âš ï¸ ê°œë°œì íƒ€ì…ë³„ ì»¤ë²„ë¦¬ì§€ ê³„ì‚° ì‹¤íŒ¨, ë¹ˆ dict ì‚¬ìš©")
            
            # 4. role í¼ì„¼íŠ¸ ê³„ì‚°
            role_percentages = {}
            for role, coverage_data in developer_type_coverage.items():
                percentage = coverage_data.get("percentage", 0)
                role_percentages[role] = float(percentage)
            
            logger.info(f"   ì—­í• ë³„ ì»¤ë²„ë¦¬ì§€: {list(role_percentages.keys())}")
            
            # UserAnalysisResult ìƒì„±
            result = UserAnalysisResult(
                python=LanguageInfo(),  # ë¹ˆ ì´ˆê¸°ê°’ (ì–¸ì–´ë³„ ì •ë³´ëŠ” LLMì´ ì±„ì›€)
                clean_code=round(clean_code_score, 2),
                role=role_percentages,
                markdown="",  # ë‚˜ì¤‘ì— ì „ì²´ ë¦¬í¬íŠ¸ë¡œ ì±„ì›€
                level=level_info,  # ì •í™•í•œ ë ˆë²¨ ì •ë³´
                tech_stack=sorted(list(all_tech_stack)) if all_tech_stack else [],  # ì „ì²´ ê¸°ìˆ  ìŠ¤íƒ
            )
            
            logger.info(f"âœ… UserAnalysisResult ìƒì„± ì™„ë£Œ (ì •í™•í•œ ë ˆë²¨ ê³„ì‚°)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ UserAnalysisResult ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return None


    def _generate_synthesis_report(
        self,
        repo_summaries: List[Dict[str, Any]],
        total_commits: int,
        total_files: int,
        successful: int,
        failed: int,
        target_user: str | None,
        user_analysis_result: Optional[UserAnalysisResult] = None,
        llm_analysis: Optional[LLMAnalysisResult] = None,
    ) -> str:
        """ì¢…í•© ë¦¬í¬íŠ¸ ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
        
        is_single = len(repo_summaries) == 1
        title = "Repository Analysis - Synthesis Report" if is_single else "Multi-Repository Analysis - Synthesis Report"
        
        report = f"""# {title}

**ìƒì„± ì‹œê°„**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**ë¶„ì„ ëŒ€ìƒ ìœ ì €**: {target_user if target_user else "ì „ì²´ ìœ ì €"}

---

## ğŸ“Š Executive Summary

- **ì´ ë ˆí¬ì§€í† ë¦¬ ìˆ˜**: {len(repo_summaries)}ê°œ
- **ì„±ê³µ**: {successful}ê°œ
- **ì‹¤íŒ¨**: {failed}ê°œ
- **ì´ ë¶„ì„ ì»¤ë°‹ ìˆ˜**: {total_commits:,}ê°œ
- **ì´ ë¶„ì„ íŒŒì¼ ìˆ˜**: {total_files:,}ê°œ

---

"""

        # target_userê°€ ìˆê³  user_analysis_resultê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if user_analysis_result:
            # ë ˆë²¨ ì •ë³´ ë¨¼ì € í‘œì‹œ (UserAnalysisResultì—ì„œ ê°€ì ¸ì˜´)
            if user_analysis_result.level:
                level_info = user_analysis_result.level
                report += "## ğŸ¯ ê°œë°œì ë ˆë²¨\n\n"
                report += f"**ë ˆë²¨**: {level_info.get('level', 0)}\n"
                report += (
                    f"**ì´ ê²½í—˜ì¹˜**: "
                    f"{level_info.get('experience', 0):,}\n"
                )
                report += (
                    f"**í˜„ì¬ ë ˆë²¨ ê²½í—˜ì¹˜**: "
                    f"{level_info.get('current_level_exp', 0):,} / "
                    f"{level_info.get('next_level_exp', 0):,}\n"
                )
                report += (
                    f"**ì§„í–‰ë¥ **: "
                    f"{level_info.get('progress_percentage', 0):.1f}%\n\n"
                )
            
            # ê¸°ìˆ  ìŠ¤íƒ í‘œì‹œ (UserAnalysisResultì—ì„œ ê°€ì ¸ì˜´)
            if user_analysis_result.tech_stack and len(user_analysis_result.tech_stack) > 0:
                report += "ê¸°ìˆ  ìŠ¤íƒ\n\n"
                # 5ê°œì”© ì¤„ë°”ê¾¸ì–´ í‘œì‹œ
                for i in range(0, len(user_analysis_result.tech_stack), 5):
                    chunk = user_analysis_result.tech_stack[i:i+5]
                    report += f"`{'` Â· `'.join(chunk)}`\n"
                report += "\n"
            
            report += user_analysis_result.markdown
            report += "\n---\n\n"

        # LLM ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        if llm_analysis:
            report += "## ğŸ¤– LLM ì¢…í•© ë¶„ì„ ë° ê°œì„  ë°©í–¥\n\n"
            
            report += f"### ì¢…í•© í‰ê°€\n\n{llm_analysis.overall_assessment}\n\n"
            
            if llm_analysis.strengths:
                report += "### ê°•ì  ë¶„ì„\n\n"
                for strength in llm_analysis.strengths:
                    report += f"- {strength}\n"
                report += "\n"
            
            if llm_analysis.improvement_recommendations:
                report += "### ê°œì„  ë°©í–¥\n\n"
                for rec in llm_analysis.improvement_recommendations:
                    report += f"#### {rec.priority} - {rec.title}\n\n"
                    report += f"**ì¹´í…Œê³ ë¦¬**: {rec.category}\n\n"
                    report += f"{rec.description}\n\n"
                    if rec.action_items:
                        report += "**ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜**:\n"
                        for action in rec.action_items:
                            report += f"- {action}\n"
                    report += "\n"
            
            if llm_analysis.role_suitability:
                report += "### ì—­í•  ì í•©ì„± í‰ê°€\n\n"
                for role, assessment in llm_analysis.role_suitability.items():
                    report += f"- **{role}**: {assessment}\n"
                report += "\n"

        # LLM ë¶„ì„ì´ ì—†ëŠ” ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€
        if not llm_analysis:
            report += "## ğŸ“ Notes\n\n"
            report += "LLM ë¶„ì„ì´ ì‹¤íŒ¨í•˜ì—¬ ìƒì„¸ í‰ê°€ì™€ ê°œì„  ë°©í–¥ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"

        return report

