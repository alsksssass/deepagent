"""SecurityAgent - ë³´ì•ˆ ì·¨ì•½ì  ë° ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ì—ì´ì „íŠ¸"""

import logging
import json
import re
from pathlib import Path
from typing import Dict, Any, Optional
from langchain_aws import ChatBedrockConverse
from langchain_core.messages import SystemMessage, HumanMessage

from .schemas import (
    SecurityAgentContext,
    SecurityAgentResponse,
    SecurityAnalysis,
    VulnerabilityRisk,
)
from shared.utils.prompt_loader import PromptLoader
from shared.utils.token_tracker import TokenTracker
from shared.utils.agent_debug_logger import AgentDebugLogger

logger = logging.getLogger(__name__)


class SecurityAgent:
    """
    ë³´ì•ˆ ì „ë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸

    ë¶„ì„ ì˜ì—­:
    - íƒ€ì… ì•ˆì •ì„± ê´€ë ¨ ë³´ì•ˆ ì´ìŠˆ
    - ì¸ì¦/ì¸ê°€ íŒ¨í„´ ê²€ì‚¬
    - ì…ë ¥ ê²€ì¦ ë° ì·¨ì•½ì  ë¶„ì„
    - ì „ë°˜ì ì¸ ë³´ì•ˆ ìœ„í—˜ë„ í‰ê°€
    """

    def __init__(self, llm: Optional[ChatBedrockConverse] = None):
        # í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: YAML ëª¨ë¸ ìš°ì„ , ì™¸ë¶€ LLM ì „ë‹¬ ì‹œ ì˜¤ë²„ë¼ì´ë“œ
        if llm is None:
            # YAML ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.llm = PromptLoader.get_llm("security_agent")
            model_id = PromptLoader.get_model("security_agent")
            logger.info(f"âœ… SecurityAgent: YAML ëª¨ë¸ ì‚¬ìš© - {model_id}")
        else:
            # ì™¸ë¶€ ì „ë‹¬ëœ LLM ì‚¬ìš© (ì˜¤ë²„ë¼ì´ë“œ)
            self.llm = llm
            logger.info(f"âœ… SecurityAgent: ì™¸ë¶€ LLM ì‚¬ìš©")
        
        # í•˜ì´ë¸Œë¦¬ë“œ: ìŠ¤í‚¤ë§ˆ ìë™ ì£¼ì…
        self.prompts = PromptLoader.load_with_schema(
            "security_agent",
            response_schema_class=SecurityAnalysis
        )

    async def run(self, context: SecurityAgentContext) -> SecurityAgentResponse:
        """
        ë³´ì•ˆ ë¶„ì„ ì‹¤í–‰

        Args:
            context: SecurityAgentContext (static_analysis, user_aggregate, git_url)

        Returns:
            SecurityAgentResponse (status, security_analysis, error)
        """
        logger.info("ğŸ›¡ï¸  SecurityAgent: ë³´ì•ˆ ë¶„ì„ ì‹œì‘")

        # ë””ë²„ê¹… ë¡œê±° ì´ˆê¸°í™”
        base_path = Path(f"./data/analyze/{context.task_uuid}")
        debug_logger = AgentDebugLogger.get_logger(context.task_uuid, base_path, "security_agent")
        
        with debug_logger.track_execution():
            # ìš”ì²­ ë¡œê¹…
            debug_logger.log_request(context)
            
            try:
                static_analysis = context.static_analysis
                user_aggregate = context.user_aggregate

                # ë¶„ì„ ë°ì´í„° ì¶”ì¶œ
                type_check = static_analysis.get("type_check", {})
                type_errors = type_check.get("total_errors", 0)
                type_warnings = type_check.get("total_warnings", 0)

                complexity_data = static_analysis.get("complexity", {})
                complexity_summary = complexity_data.get("summary", {})

                tech_stack = (
                    user_aggregate.get("aggregate_stats", {})
                    .get("tech_stats", {})
                    .get("technology_frequency", {})
                )

                # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¤€ë¹„
                prompt_variables = {
                    "type_errors": type_errors,
                    "type_warnings": type_warnings,
                    "complexity_a": complexity_summary.get("A", 0),
                    "complexity_b": complexity_summary.get("B", 0),
                    "complexity_c": complexity_summary.get("C", 0),
                    "complexity_d": complexity_summary.get("D", 0),
                    "complexity_f": complexity_summary.get("F", 0),
                    "tech_stack": self._format_tech_stack(tech_stack),
                }

                # í”„ë¡¬í”„íŠ¸ ìƒì„± (json_schema ë³€ìˆ˜ ìë™ ì£¼ì…)
                system_prompt = PromptLoader.format(
                    self.prompts["system_prompt"],
                    json_schema=self.prompts.get("json_schema", "")
                )
                user_prompt = PromptLoader.format(
                    self.prompts["user_template"],
                    **prompt_variables
                )

                # LLM í˜¸ì¶œ (í† í° ì¶”ì  + ê°œì„ ëœ ë””ë²„ê¹… ë¡œê¹…)
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_prompt),
                ]

                with TokenTracker.track("security_agent"), debug_logger.track_llm_call() as llm_tracker:
                    # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
                    llm_tracker.log_prompts(
                        template_name="security_agent",
                        variables=prompt_variables,
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                    )
                    
                    # LLM í˜¸ì¶œ
                    response = await self.llm.ainvoke(messages)
                    TokenTracker.record_usage("security_agent", response, model_id=PromptLoader.get_model("security_agent"))
                    llm_tracker.set_messages(messages)
                    llm_tracker.set_response(response)
                    
                    # ì‘ë‹µ ì²˜ë¦¬ ë‹¨ê³„ë³„ ë¡œê¹…
                    raw_response = response.content
                    parsed_json = None
                    security_analysis = None
                    processing_error = None
                    
                    try:
                        # JSON íŒŒì‹±
                        parsed_json = self._parse_json_response(raw_response)
                        
                        # Pydantic ê²€ì¦
                        security_analysis = SecurityAnalysis(**parsed_json)
                        
                        # ì„±ê³µ ë¡œê¹…
                        llm_tracker.log_response_stages(
                            raw=raw_response,
                            parsed=parsed_json,
                            validated=security_analysis,
                        )
                    except Exception as parse_error:
                        processing_error = str(parse_error)
                        # ì—ëŸ¬ ë¡œê¹…
                        llm_tracker.log_response_stages(
                            raw=raw_response,
                            parsed=parsed_json,
                            validated=None,
                            error=processing_error,
                        )
                        raise

                logger.info(
                    f"âœ… SecurityAgent: ë³´ì•ˆ ë¶„ì„ ì™„ë£Œ - ì ìˆ˜ {security_analysis.security_score}/10"
                )

                response = SecurityAgentResponse(
                    status="success",
                    security_analysis=security_analysis,
                    error=None,
                )
                
                # ìµœì¢… ì‘ë‹µ ë¡œê¹…
                debug_logger.log_response(response)
                return response

            except Exception as e:
                logger.error(f"âŒ SecurityAgent: {e}", exc_info=True)
                error_response = SecurityAgentResponse(
                    status="failed",
                    security_analysis=SecurityAnalysis(),
                    error=str(e),
                )
                debug_logger.log_response(error_response)
                return error_response

    def _parse_json_response(self, text: str) -> Dict[str, Any]:
        """LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹±"""
        # 1. ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ ì‹œë„
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
        if json_match:
            logger.info("âœ… SecurityAgent: JSON ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ ì„±ê³µ")
            return json.loads(json_match.group(1))

        # 2. ì§ì ‘ JSON íŒŒì‹± ì‹œë„
        try:
            logger.info("âš ï¸  SecurityAgent: JSON ì½”ë“œ ë¸”ë¡ ì—†ìŒ, ì§ì ‘ íŒŒì‹± ì‹œë„")
            return json.loads(text)
        except json.JSONDecodeError as e:
            logger.warning(f"âŒ SecurityAgent: JSON íŒŒì‹± ì‹¤íŒ¨ - {e}")
            logger.warning("âš ï¸  SecurityAgent: ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©")
            return {
                "type_safety_issues": ["JSON íŒŒì‹± ì‹¤íŒ¨"],
                "auth_patterns": [],
                "vulnerability_risks": [],
                "security_score": 5.0,
                "recommendations": ["ìˆ˜ë™ ê²€í†  í•„ìš”"],
                "raw_analysis": text,
            }

    def _format_tech_stack(self, tech_stack: Dict[str, int]) -> str:
        """ê¸°ìˆ  ìŠ¤íƒ í¬ë§·íŒ…"""
        if not tech_stack:
            return "N/A"

        items = []
        for tech, count in sorted(tech_stack.items(), key=lambda x: x[1], reverse=True):
            items.append(f"- {tech}: {count}íšŒ")

        return "\n".join(items) if items else "N/A"
