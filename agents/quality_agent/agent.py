"""QualityAgent - ì½”ë“œ í’ˆì§ˆ ë° ìœ ì§€ë³´ìˆ˜ì„± ë¶„ì„ ì—ì´ì „íŠ¸"""

import logging
import json
import re
from pathlib import Path
from typing import Dict, Any, Optional
from langchain_aws import ChatBedrockConverse
from langchain_core.messages import SystemMessage, HumanMessage

from .schemas import (
    QualityAgentContext,
    QualityAgentResponse,
    QualityAnalysis,
    CodeSmell,
)
from shared.utils.prompt_loader import PromptLoader
from shared.utils.token_tracker import TokenTracker
from shared.utils.agent_logging import log_agent_execution

logger = logging.getLogger(__name__)


class QualityAgent:
    """
    í’ˆì§ˆ ì „ë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸

    ë¶„ì„ ì˜ì—­:
    - ì½”ë“œ ë³µìž¡ë„ ìƒì„¸ ë¶„ì„
    - íƒ€ìž… ì•ˆì •ì„± í‰ê°€
    - ì£¼ì„/ë¬¸ì„œí™” ìˆ˜ì¤€ í‰ê°€
    - ì½”ë“œ ìŠ¤ë©œ ì‹ë³„
    """

    def __init__(self, llm: Optional[ChatBedrockConverse] = None):
        # í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: YAML ëª¨ë¸ ìš°ì„ , ì™¸ë¶€ LLM ì „ë‹¬ ì‹œ ì˜¤ë²„ë¼ì´ë“œ
        if llm is None:
            # YAML ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.llm = PromptLoader.get_llm("quality_agent")
            model_id = PromptLoader.get_model("quality_agent")
            logger.info(f"âœ… QualityAgent: YAML ëª¨ë¸ ì‚¬ìš© - {model_id}")
        else:
            # ì™¸ë¶€ ì „ë‹¬ëœ LLM ì‚¬ìš© (ì˜¤ë²„ë¼ì´ë“œ)
            self.llm = llm
            logger.info(f"âœ… QualityAgent: ì™¸ë¶€ LLM ì‚¬ìš©")
        
        # í•˜ì´ë¸Œë¦¬ë“œ: ìŠ¤í‚¤ë§ˆ ìžë™ ì£¼ìž…
        self.prompts = PromptLoader.load_with_schema(
            "quality_agent",
            response_schema_class=QualityAnalysis
        )

    @log_agent_execution(agent_name="quality_agent")
    async def run(self, context: QualityAgentContext) -> QualityAgentResponse:
        """
        í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰

        Args:
            context: QualityAgentContext (static_analysis, user_aggregate)

        Returns:
            QualityAgentResponse (status, quality_analysis, error)
        """
        logger.info("ðŸ“Š QualityAgent: í’ˆì§ˆ ë¶„ì„ ì‹œìž‘")

        try:
            static_analysis = context.static_analysis
            user_aggregate = context.user_aggregate

            # ë¶„ì„ ë°ì´í„° ì¶”ì¶œ
            loc_stats = static_analysis.get("loc_stats", {})
            code_lines = loc_stats.get("code_lines", 0)
            comment_lines = loc_stats.get("comment_lines", 0)
            total_lines = loc_stats.get("total_lines", 1)

            type_check = static_analysis.get("type_check", {})
            type_errors = type_check.get("total_errors", 0)
            type_warnings = type_check.get("total_warnings", 0)
            files_analyzed = type_check.get("files_analyzed", 0)

            complexity_data = static_analysis.get("complexity", {})
            avg_complexity = complexity_data.get("average_complexity", 0)

            agg_stats = user_aggregate.get("aggregate_stats", {})
            avg_quality_score = (
                agg_stats.get("quality_stats", {}).get("average_score", 0)
            )

            # ì£¼ì„ ë¹„ìœ¨ ê³„ì‚°
            comment_ratio = (comment_lines / total_lines * 100) if total_lines > 0 else 0
            # íƒ€ìž… ì—ëŸ¬ ë¹„ìœ¨
            type_error_ratio = (type_errors / files_analyzed) if files_analyzed > 0 else 0

            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¤€ë¹„
            prompt_variables = {
                "total_lines": total_lines,
                "code_lines": code_lines,
                "comment_lines": comment_lines,
                "comment_ratio": f"{comment_ratio:.1f}",
                "files_analyzed": files_analyzed,
                "type_errors": type_errors,
                "type_warnings": type_warnings,
                "type_error_ratio": f"{type_error_ratio:.2f}",
                "avg_complexity": f"{avg_complexity:.2f}",
                "avg_quality_score": f"{avg_quality_score:.2f}",
            }

            # í”„ë¡¬í”„íŠ¸ ìƒì„± (json_schema ë³€ìˆ˜ ìžë™ ì£¼ìž…)
            system_prompt = PromptLoader.format(
                self.prompts["system_prompt"],
                json_schema=self.prompts.get("json_schema", "")
            )
            user_prompt = PromptLoader.format(
                self.prompts["user_template"],
                **prompt_variables
            )

            # LLM í˜¸ì¶œ (í† í° ì¶”ì )
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ]

            # LLM í˜¸ì¶œ ë¡œê¹…ì„ ìœ„í•´ logger ê°€ì ¸ì˜¤ê¸°
            from shared.utils.agent_debug_logger import AgentDebugLogger
            from pathlib import Path
            base_path = Path(f"./data/analyze/{context.task_uuid}")
            debug_logger = AgentDebugLogger.get_logger(context.task_uuid, base_path, "quality_agent")

            with TokenTracker.track("quality_agent"), debug_logger.track_llm_call() as llm_tracker:
                # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
                llm_tracker.log_prompts(
                    template_name="quality_agent",
                    variables=prompt_variables,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                )
                
                # LLM í˜¸ì¶œ
                response = await self.llm.ainvoke(messages)
                TokenTracker.record_usage("quality_agent", response, model_id=PromptLoader.get_model("quality_agent"))
                llm_tracker.set_messages(messages)
                llm_tracker.set_response(response)
                
                # ì‘ë‹µ ì²˜ë¦¬ ë‹¨ê³„ë³„ ë¡œê¹…
                raw_response = response.content
                parsed_json = None
                quality_analysis = None
                processing_error = None
                
                try:
                    # JSON íŒŒì‹±
                    parsed_json = self._parse_json_response(
                        raw_response, comment_ratio, avg_quality_score
                    )
                    
                    # Pydantic ê²€ì¦
                    quality_analysis = QualityAnalysis(**parsed_json)
                    
                    # ì„±ê³µ ë¡œê¹…
                    llm_tracker.log_response_stages(
                        raw=raw_response,
                        parsed=parsed_json,
                        validated=quality_analysis,
                    )
                except Exception as parse_error:
                    processing_error = str(parse_error)
                    # ì—ëŸ¬ ë¡œê¹…
                    llm_tracker.log_response_stages(
                        raw=raw_response,
                        parsed=parsed_json,
                        validated=None,
                        error=processing_error,
                    )
                    raise

            logger.info(
                f"âœ… QualityAgent: í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ - ì ìˆ˜ {quality_analysis.quality_score}/10"
            )

            response = QualityAgentResponse(
                status="success",
                quality_analysis=quality_analysis,
                error=None,
            )
            return response

        except Exception as e:
            logger.error(f"âŒ QualityAgent: {e}", exc_info=True)
            error_response = QualityAgentResponse(
                status="failed",
                quality_analysis=QualityAnalysis(),
                error=str(e),
            )
            return error_response

    def _parse_json_response(
        self, text: str, comment_ratio: float, avg_quality_score: float
    ) -> Dict[str, Any]:
        """LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹±"""
        # 1. ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ ì‹œë„
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
        if json_match:
            logger.info("âœ… QualityAgent: JSON ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ ì„±ê³µ")
            return json.loads(json_match.group(1))

        # 2. ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ë§Œ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        try:
            logger.info("âš ï¸  QualityAgent: JSON ì½”ë“œ ë¸”ë¡ ì—†ìŒ, ì²« ë²ˆì§¸ JSON ê°ì²´ ì¶”ì¶œ ì‹œë„")

            # ì¤‘ê´„í˜¸ ë§¤ì¹­ì„ í†µí•´ ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ ì°¾ê¸°
            start_idx = text.find("{")
            if start_idx == -1:
                raise ValueError("JSON ê°ì²´ ì‹œìž‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            brace_count = 0
            end_idx = start_idx
            for i in range(start_idx, len(text)):
                if text[i] == "{":
                    brace_count += 1
                elif text[i] == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i + 1
                        break

            if brace_count != 0:
                raise ValueError("JSON ê°ì²´ê°€ ì™„ì „í•˜ì§€ ì•ŠìŒ")

            json_str = text[start_idx:end_idx]
            return json.loads(json_str)

        except (ValueError, json.JSONDecodeError) as e:
            logger.warning(f"âŒ QualityAgent: JSON íŒŒì‹± ì‹¤íŒ¨ - {e}")
            logger.warning("âš ï¸  QualityAgent: ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©")
            return {
                "maintainability_index": 50.0,
                "documentation_coverage": comment_ratio,
                "type_safety_level": "Fair",
                "code_smells": [],
                "quality_score": avg_quality_score if avg_quality_score > 0 else 5.0,
                "recommendations": ["í’ˆì§ˆ ê°œì„  í•„ìš”"],
                "raw_analysis": text,
            }
