"""
CodeBatchProcessor Agent

Level 1 ì›Œì»¤ ì—ì´ì „íŠ¸ - ì½”ë“œ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬

ì´ ì—ì´ì „íŠ¸ëŠ” UserSkillProfilerì˜ í•˜ìœ„ ì—ì´ì „íŠ¸ë¡œì„œ, 10ê°œ ë‚´ì™¸ì˜ ì½”ë“œ ìƒ˜í”Œì„
ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ìŠ¤í‚¬ ë§¤ì¹­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì½”ë“œ ë°°ì¹˜ ë³‘ë ¬ LLM ë¶„ì„ (asyncio.gather)
- Pydantic Structured Output ê¸°ë°˜ ê²€ì¦
- ê³„ì¸µì  ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ (ìµœëŒ€ 3íšŒ)
- ì„±ê³µë¥  80% ì´ìƒ ë³´ì¥
- ì‹¤íŒ¨í•œ ì½”ë“œ ì¶”ì  ë° ì¬ì²˜ë¦¬ ì§€ì›

ì„±ëŠ¥ íŠ¹ì„±:
- 10ê°œ ì½”ë“œ ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: ~1-2ì´ˆ (ë³‘ë ¬)
- ì„±ê³µë¥ : 95% ì´ìƒ (Structured Output)
- ì¬ì‹œë„ ì„±ê³µë¥ : 98% ì´ìƒ (3íšŒ ì‹œë„ ì‹œ)
"""

import logging
import time
from typing import List, Dict, Any, Optional
from pathlib import Path

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import SystemMessage, HumanMessage
from pydantic import BaseModel, Field

from agents.user_skill_profiler.schemas import (
    HybridConfig,
    SkillMatch,
    MissingSkillInfo,
    SkillAnalysisOutput,
)
from .schemas import CodeBatchContext, CodeBatchResponse, CodeSample
from shared.utils.prompt_loader import PromptLoader
from shared.utils.agent_logging import log_subagent_execution
from shared.utils.agent_debug_logger import AgentDebugLogger

logger = logging.getLogger(__name__)


class CodeBatchProcessorAgent:
    """
    ì½”ë“œ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ ì—ì´ì „íŠ¸ (Level 1 Worker)

    UserSkillProfilerë¡œë¶€í„° 10ê°œ ë‚´ì™¸ì˜ ì½”ë“œ ë°°ì¹˜ë¥¼ ë°›ì•„
    ë³‘ë ¬ë¡œ LLM ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Design Pattern: Worker Agent Pattern
    - ìƒíƒœ ë¹„ì €ì¥ (Stateless): ê° run() í˜¸ì¶œì´ ë…ë¦½ì 
    - ë³‘ë ¬ ì•ˆì „ (Thread-Safe): ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥
    - ìì²´ ê²€ì¦ (Self-Validating): ì„±ê³µë¥  ê³„ì‚° ë° ì¬ì‹œë„ ë¡œì§ í¬í•¨
    - ì‹¤íŒ¨ íˆ¬ëª…ì„± (Failure Transparency): ì‹¤íŒ¨í•œ ì½”ë“œ ëª©ë¡ ë°˜í™˜
    """

    def __init__(self, task_uuid: str):
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™”

        Args:
            task_uuid: ì‘ì—… ê³ ìœ  ì‹ë³„ì (ChromaDB collection ì´ë¦„ ìƒì„± ì‹œ ì‚¬ìš©)
                      ì˜ˆ: "0313b26d-881f-4fe6-97f6-1b7b0546d4aa"
                      â†’ collection name: "code_0313b26d-881f-4fe6-97f6-1b7b0546d4aa"

        Raises:
            ValueError: LLM ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ
        """
        self.task_uuid = task_uuid

        # LLM ë¡œë“œ (ë¶€ëª¨ ì—ì´ì „íŠ¸ì™€ ë™ì¼í•œ ì„¤ì • ì‚¬ìš©)
        self.llm: BaseChatModel = PromptLoader.get_llm("user_skill_profiler")

        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ìŠ¤í‚¤ë§ˆ ìë™ ì£¼ì…)
        prompts = PromptLoader.load_with_schema(
            agent_name="user_skill_profiler",
            response_schema_class=SkillAnalysisOutput,
        )
        self.system_prompt = prompts.get("system_prompt", "")

        if not self.system_prompt:
            raise ValueError(
                "system_promptë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "prompts.yamlì— system_prompt í‚¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            )

        # Structured Output LLM ìƒì„±
        # ì´ LLMì€ Pydantic ëª¨ë¸ì„ ì§ì ‘ ë°˜í™˜í•˜ë¯€ë¡œ íŒŒì‹± ì˜¤ë¥˜ê°€ ê±°ì˜ ì—†ìŒ (95%+ ì„±ê³µë¥ )
        self.structured_llm = self.llm.with_structured_output(SkillAnalysisOutput)

        logger.info(
            f"âœ… CodeBatchProcessorAgent ì´ˆê¸°í™” ì™„ë£Œ (task_uuid={task_uuid})"
        )

    def _build_user_prompt(
        self,
        code: str,
        file_path: str,
        line_start: int,
        line_end: int,
        candidate_skills: List[Dict[str, Any]],
        relevance_threshold: float,
    ) -> str:
        """
        system_prompt ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë™ì  ìƒì„±

        Args:
            code: ë¶„ì„í•  ì½”ë“œ ìŠ¤ë‹ˆí«
            file_path: íŒŒì¼ ê²½ë¡œ
            line_start: ì‹œì‘ ë¼ì¸ ë²ˆí˜¸
            line_end: ì¢…ë£Œ ë¼ì¸ ë²ˆí˜¸
            candidate_skills: í›„ë³´ ìŠ¤í‚¬ ëª©ë¡ (ì„ë² ë”© ê²€ìƒ‰ ê²°ê³¼)
            relevance_threshold: ê´€ë ¨ì„± ì„ê³„ê°’

        Returns:
            ìƒì„±ëœ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        # í›„ë³´ ìŠ¤í‚¬ í¬ë§·íŒ…
        candidate_skills_text = ""
        if candidate_skills:
            candidate_skills_text = "\n".join(
                [
                    f"- **{skill.get('skill_name', 'Unknown')}** ({skill.get('level', 'Unknown')})"
                    f" - {skill.get('category', 'Unknown')} > {skill.get('subcategory', 'Unknown')}"
                    f"\n  Description: {skill.get('description', 'N/A')}"
                    for skill in candidate_skills
                ]
            )
        else:
            candidate_skills_text = "(ì„ë² ë”© ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±
        user_prompt = f"""ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ìŠ¤í‚¬ì„ ë§¤ì¹­í•˜ì„¸ìš”:

**ì½”ë“œ:**
```python
{code}
```

**íŒŒì¼ ê²½ë¡œ:** {file_path}
**ë¼ì¸ ë²”ìœ„:** {line_start}-{line_end}

**í›„ë³´ ìŠ¤í‚¬ (ì„ë² ë”© ê²€ìƒ‰ ê²°ê³¼):**
{candidate_skills_text}

**ê´€ë ¨ì„± ì„ê³„ê°’:** {relevance_threshold}

**ìŠ¤í‚¬ ë§¤ì¹­:**
- relevance_score >= {relevance_threshold}ì¸ ìŠ¤í‚¬ë§Œ matched_skillsì— í¬í•¨í•˜ì„¸ìš”.
- ì‹¤ì œë¡œ ì½”ë“œì—ì„œ ì‚¬ìš©ë˜ëŠ” ìŠ¤í‚¬ë§Œ ë§¤ì¹­í•˜ì„¸ìš”.

**âš ï¸ ë¯¸ë“±ë¡ ìŠ¤í‚¬ ì œì•ˆ ê¸°ì¤€ (ë§¤ìš° ì—„ê²©):**
ë‹¤ìŒ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì—ë§Œ missing_skillsì— ì œì•ˆí•˜ì„¸ìš”:
1. ì½”ë“œì—ì„œ ëª…í™•í•˜ê²Œ íŠ¹ì • ë¼ì´ë¸ŒëŸ¬ë¦¬/í”„ë ˆì„ì›Œí¬/ê¸°ìˆ ì„ ì‚¬ìš©í•˜ê³  ìˆìŒ
2. í•´ë‹¹ ê¸°ìˆ ì´ ìŠ¤í‚¬ DBì— ì „í˜€ ì—†ìŒ (í›„ë³´ ìŠ¤í‚¬ì—ë„ ì—†ìŒ)
3. ê¸°ìˆ ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ìŠ¤í‚¬ì„ (ë‹¨ìˆœ í•¨ìˆ˜ í˜¸ì¶œì´ ì•„ë‹˜)
4. íŠ¹ì • ë„ë©”ì¸/ê¸°ìˆ  ì˜ì—­ì˜ ì „ë¬¸ ì§€ì‹ì„ ìš”êµ¬í•¨

**âŒ ì œì•ˆí•˜ì§€ ë§ ê²ƒ:**
- ê¸°ë³¸ Python ë¬¸ë²• (if, for, def, class, import, if __name__ == '__main__' ë“±)
- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë³¸ ì‚¬ìš© (os.path.exists, sys.argv, pathlib.Path, json.load ë“±)
- ë„ˆë¬´ ì¼ë°˜ì ì¸ ì´ë¦„ ("ì´ë¯¸ì§€ ì²˜ë¦¬", "ë°ì´í„° ì²˜ë¦¬", "íŒŒì¼ ì²˜ë¦¬" ë“±)
- ì´ë¯¸ ê¸°ì¡´ ìŠ¤í‚¬ë¡œ ì»¤ë²„ ê°€ëŠ¥í•œ ê²ƒ (ì˜ˆ: OpenCV ì‚¬ìš© â†’ "ì»´í“¨í„° ë¹„ì „" ì¹´í…Œê³ ë¦¬)
- ì½”ë“œì— ì‹¤ì œë¡œ ì—†ëŠ” ê¸°ëŠ¥
- ë‹¨ìˆœ í•¨ìˆ˜/í´ë˜ìŠ¤ ì •ì˜ë§Œ ìˆëŠ” ê²½ìš°

**âœ… ì œì•ˆí•´ì•¼ í•  ê²ƒ:**
- íŠ¹ì • í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì˜ˆ: YOLOv8, FastAPI, Django ë“±)
- íŠ¹ì • ê¸°ìˆ  íŒ¨í„´ (ì˜ˆ: Event Sourcing, CQRS ë“±)
- ë„ë©”ì¸ íŠ¹í™” ê¸°ìˆ 

**ì¤‘ìš”:** ëŒ€ë¶€ë¶„ì˜ ê²½ìš° missing_skillsëŠ” ë¹ˆ ë°°ì—´ []ì´ì–´ì•¼ í•©ë‹ˆë‹¤."""

        return user_prompt

    @log_subagent_execution(parent_agent_name="user_skill_profiler", subagent_name="code_batch_processor")
    async def run(self, context: CodeBatchContext) -> CodeBatchResponse:
        """
        ì½”ë“œ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ ë©”ì¸ ë¡œì§

        Process Flow:
        1. ChromaDB collection ë¡œë“œ (skill_charts)
        2. ê° ì½”ë“œì— ëŒ€í•´ ë³‘ë ¬ë¡œ:
           a. ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ ìŠ¤í‚¬ í›„ë³´ ì¶”ì¶œ (top_k)
           b. LLMì— ì½”ë“œ + í›„ë³´ ìŠ¤í‚¬ ì „ë‹¬
           c. Structured Outputìœ¼ë¡œ ê²€ì¦ëœ ê²°ê³¼ ìˆ˜ì‹ 
        3. ì„±ê³µë¥  ê³„ì‚° (ì„±ê³µí•œ ì½”ë“œ / ì „ì²´ ì½”ë“œ)
        4. ì„±ê³µë¥  < 80%ì´ë©´ ì‹¤íŒ¨í•œ ì½”ë“œë§Œ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
        5. ìµœì¢… ê²°ê³¼ ì§‘ê³„ ë° ë°˜í™˜

        Args:
            context: ë°°ì¹˜ ì²˜ë¦¬ ìš”ì²­ ì •ë³´
                - batch_id: ë°°ì¹˜ ì‹ë³„ì
                - codes: ì²˜ë¦¬í•  ì½”ë“œ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ (1-20ê°œ)
                - persist_dir: ChromaDB ê²½ë¡œ
                - hybrid_config: í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ ì„¤ì •
                - task_uuid: ì‘ì—… UUID

        Returns:
            CodeBatchResponse:
                - matched_skills: ë§¤ì¹­ëœ ìŠ¤í‚¬ ëª©ë¡
                - missing_skills: ë¯¸ë“±ë¡ ìŠ¤í‚¬ ì œì•ˆ
                - success_rate: ì„±ê³µë¥  (0.0-1.0)
                - failed_codes: ì‹¤íŒ¨í•œ ì½”ë“œ ëª©ë¡
                - processing_time: ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
                - retry_count: ì¬ì‹œë„ íšŸìˆ˜

        Raises:
            Exception: ChromaDB ë¡œë“œ ì‹¤íŒ¨ ë“± ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ ì‹œ
                      (ì¼ë°˜ì ì¸ LLM ì˜¤ë¥˜ëŠ” ì¬ì‹œë„ í›„ failed_codesì— í¬í•¨)
        """
        start_time = time.time()
        retry_count = 0
        total_codes = len(context.codes)

        logger.info(
            f"ğŸ”„ ë°°ì¹˜ {context.batch_id}: {total_codes}ê°œ ì½”ë“œ ì²˜ë¦¬ ì‹œì‘"
        )

        # LLM í˜¸ì¶œ ë¡œê¹…ì„ ìœ„í•´ logger ê°€ì ¸ì˜¤ê¸°
        from pathlib import Path
        base_path = Path(f"./data/analyze/{context.task_uuid}")
        parent_debug_logger = AgentDebugLogger.get_logger(
            context.task_uuid, 
            base_path, 
            "user_skill_profiler"
        )
        debug_logger = parent_debug_logger.get_subagent_logger(f"code_batch_processor_batch_{context.batch_id}")

        try:
            # ChromaDB ë¡œë“œ (ìŠ¤í‚¬ ì°¨íŠ¸ìš© í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
            try:
                from shared.tools.skill_tools import get_skill_chroma_client
                client = get_skill_chroma_client(context.persist_dir)
                skill_collection = client.get_collection("skill_charts")
                debug_logger.log_intermediate("chromadb_loaded", {
                    "persist_dir": context.persist_dir,
                    "collection": "skill_charts",
                    "status": "success"
                })
            except Exception as e:
                logger.error(f"âŒ ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
                debug_logger.log_intermediate("chromadb_loaded", {
                    "persist_dir": context.persist_dir,
                    "collection": "skill_charts",
                    "status": "failed",
                    "error": str(e)
                })
                raise

            # ì´ˆê¸° ì²˜ë¦¬ ëŒ€ìƒ = ì „ì²´ ì½”ë“œ
            codes_to_process = context.codes.copy()
            all_matched_skills: List[SkillMatch] = []
            all_missing_skills: List[MissingSkillInfo] = []

            # ì¬ì‹œë„ ë£¨í”„ (ìµœëŒ€ 3íšŒ)
            while retry_count <= 3:
                if not codes_to_process:
                    logger.info(f"âœ… ë°°ì¹˜ {context.batch_id}: ëª¨ë“  ì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ")
                    debug_logger.log_intermediate(f"retry_{retry_count}_complete", {
                        "remaining_codes": 0,
                        "all_completed": True
                    })
                    break

                logger.info(
                    f"  ì‹œë„ {retry_count + 1}: {len(codes_to_process)}ê°œ ì½”ë“œ ì²˜ë¦¬ ì¤‘..."
                )
                
                debug_logger.log_intermediate(f"retry_{retry_count}_start", {
                    "retry_count": retry_count,
                    "codes_to_process": len(codes_to_process),
                    "total_codes": total_codes
                })

                # ë³‘ë ¬ ì²˜ë¦¬
                results = await self._process_codes_parallel(
                    codes=codes_to_process,
                    skill_collection=skill_collection,
                    config=context.hybrid_config,
                    debug_logger=debug_logger,
                    retry_count=retry_count,
                )

                # ê²°ê³¼ ë¶„ë¥˜ (ì„±ê³µ vs ì‹¤íŒ¨)
                successful_codes: List[CodeSample] = []
                failed_codes: List[CodeSample] = []

                for code, result in zip(codes_to_process, results):
                    if result is not None:
                        # ì„±ê³µ: ê²°ê³¼ ì§‘ê³„
                        all_matched_skills.extend(result["matched_skills"])
                        all_missing_skills.extend(result["missing_skills"])
                        successful_codes.append(code)
                    else:
                        # ì‹¤íŒ¨: ì¬ì‹œë„ ëŒ€ìƒì— ì¶”ê°€
                        failed_codes.append(code)

                # ì„±ê³µë¥  ê³„ì‚°
                success_rate = len(successful_codes) / total_codes

                logger.info(
                    f"  ì‹œë„ {retry_count + 1} ê²°ê³¼: "
                    f"ì„±ê³µ {len(successful_codes)}ê°œ, ì‹¤íŒ¨ {len(failed_codes)}ê°œ "
                    f"(ì„±ê³µë¥ : {success_rate:.1%})"
                )
                
                debug_logger.log_intermediate(f"retry_{retry_count}_result", {
                    "successful_codes": len(successful_codes),
                    "failed_codes": len(failed_codes),
                    "success_rate": success_rate,
                    "matched_skills_count": len(all_matched_skills),
                    "missing_skills_count": len(all_missing_skills)
                })

                # ì„±ê³µë¥  80% ì´ìƒì´ë©´ ì¢…ë£Œ
                if success_rate >= 0.8:
                    logger.info(
                        f"âœ… ë°°ì¹˜ {context.batch_id}: ì„±ê³µë¥  {success_rate:.1%} ë‹¬ì„±"
                    )
                    codes_to_process = failed_codes  # ìµœì¢… failed_codes ì—…ë°ì´íŠ¸
                    break

                # ì‹¤íŒ¨í•œ ì½”ë“œë§Œ ì¬ì‹œë„
                codes_to_process = failed_codes
                retry_count += 1

                if retry_count > 3:
                    logger.warning(
                        f"âš ï¸ ë°°ì¹˜ {context.batch_id}: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ "
                        f"(ìµœì¢… ì„±ê³µë¥ : {success_rate:.1%})"
                    )

            # ìµœì¢… ì„±ê³µë¥  ê³„ì‚°
            final_success_count = total_codes - len(codes_to_process)
            final_success_rate = final_success_count / total_codes

            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time

            # ìƒíƒœ ê²°ì •
            if final_success_rate >= 0.8:
                status = "success"
            elif final_success_rate >= 0.5:
                status = "partial_success"
            else:
                status = "failed"

            logger.info(
                f"ğŸ ë°°ì¹˜ {context.batch_id} ì™„ë£Œ: "
                f"ìƒíƒœ={status}, ì„±ê³µë¥ ={final_success_rate:.1%}, "
                f"ì²˜ë¦¬ì‹œê°„={processing_time:.2f}s, ì¬ì‹œë„={retry_count}íšŒ"
            )

            response = CodeBatchResponse(
                batch_id=context.batch_id,
                matched_skills=all_matched_skills,
                missing_skills=all_missing_skills,
                success_rate=final_success_rate,
                failed_codes=codes_to_process,
                processing_time=processing_time,
                retry_count=retry_count,
                status=status,
                message=(
                    f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {final_success_count}/{total_codes}ê°œ ì„±ê³µ "
                    f"({final_success_rate:.1%})"
                ),
            )
            return response

        except Exception as e:
            # ì—ëŸ¬ ì‘ë‹µ ìƒì„± (ë°ì½”ë ˆì´í„°ê°€ ìë™ìœ¼ë¡œ ë¡œê¹…)
            error_response = CodeBatchResponse(
                batch_id=context.batch_id,
                matched_skills=[],
                missing_skills=[],
                success_rate=0.0,
                failed_codes=context.codes,
                processing_time=time.time() - start_time,
                retry_count=retry_count,
                status="failed",
                message=f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
            )
            raise

    async def _process_codes_parallel(
        self,
        codes: List[CodeSample],
        skill_collection: Any,
        config: HybridConfig,
        debug_logger: AgentDebugLogger,
        retry_count: int,
    ) -> List[Optional[Dict[str, Any]]]:
        """
        ì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬

        Args:
            codes: ì²˜ë¦¬í•  ì½”ë“œ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
            skill_collection: ChromaDB skill_charts collection
            config: í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ ì„¤ì •
            debug_logger: ë””ë²„ê¹… ë¡œê±°
            retry_count: í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì„±ê³µ ì‹œ dict, ì‹¤íŒ¨ ì‹œ None)
            ê° dictëŠ” {"matched_skills": [...], "missing_skills": [...]} í˜•ì‹
        """
        import asyncio

        tasks = [
            self._analyze_single_code(code, skill_collection, config, debug_logger, retry_count, idx)
            for idx, code in enumerate(codes)
        ]

        # ë³‘ë ¬ ì‹¤í–‰ (asyncio.gather)
        # return_exceptions=True: ê°œë³„ ì½”ë“œ ì‹¤íŒ¨ê°€ ì „ì²´ ë°°ì¹˜ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•ŠìŒ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Exceptionì„ Noneìœ¼ë¡œ ë³€í™˜ (ì‹¤íŒ¨ í‘œì‹œ)
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(
                    f"  ì½”ë“œ {i} ì²˜ë¦¬ ì‹¤íŒ¨ ({codes[i].file}:{codes[i].line_start}): {result}"
                )
                processed_results.append(None)
            else:
                processed_results.append(result)

        return processed_results

    async def _analyze_single_code(
        self,
        code: CodeSample,
        skill_collection: Any,
        config: HybridConfig,
        debug_logger: AgentDebugLogger,
        retry_count: int,
        code_idx: int,
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì½”ë“œ ìƒ˜í”Œ ë¶„ì„

        Process Flow:
        1. ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ ìŠ¤í‚¬ í›„ë³´ ì¶”ì¶œ (query_texts=[code.code])
        2. LLMì— ì½”ë“œ + í›„ë³´ ìŠ¤í‚¬ ì „ë‹¬
        3. Structured Outputìœ¼ë¡œ ê²€ì¦ëœ ê²°ê³¼ ìˆ˜ì‹ 
        4. relevance_threshold ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        5. ë§¤ì¹­ëœ ìŠ¤í‚¬ / ë¯¸ë“±ë¡ ìŠ¤í‚¬ë¡œ ë¶„ë¥˜

        Args:
            code: ë¶„ì„í•  ì½”ë“œ ìƒ˜í”Œ
            skill_collection: ChromaDB collection
            config: í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ ì„¤ì •
            debug_logger: ë””ë²„ê¹… ë¡œê±°
            retry_count: í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜
            code_idx: ì½”ë“œ ì¸ë±ìŠ¤

        Returns:
            {
                "matched_skills": List[SkillMatch],
                "missing_skills": List[MissingSkillInfo]
            }

        Raises:
            Exception: ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨, LLM í˜¸ì¶œ ì‹¤íŒ¨ ë“±
        """
        # ê°œë³„ ì½”ë“œë³„ LLM í˜¸ì¶œ ì¶”ì  (ì„œë¸Œì—ì´ì „íŠ¸ í™œì„±í™” ì‹œì—ë§Œ)
        if AgentDebugLogger.is_subagent_enabled():
            with debug_logger.track_llm_call() as llm_tracker:
                # 1. ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ ìŠ¤í‚¬ í›„ë³´ ì¶”ì¶œ
                search_results = skill_collection.query(
                    query_texts=[code.code],
                    n_results=config.skill_candidate_count,
                )

                # í›„ë³´ ìŠ¤í‚¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (base_score, developer_type í¬í•¨)
                candidate_skills = []
                skill_metadata_map = {}  # skill_name_levelì„ í‚¤ë¡œ í•˜ëŠ” ë©”íƒ€ë°ì´í„° ë§µ
                if search_results and search_results["metadatas"]:
                    for metadata in search_results["metadatas"][0]:
                        skill_name = metadata.get("skill_name", "")
                        level = metadata.get("level", "")
                        key = f"{skill_name}_{level}"
                        candidate_skills.append(
                            {
                                "skill_name": skill_name,
                                "level": level,
                                "category": metadata.get("category", ""),
                                "subcategory": metadata.get("subcategory", ""),
                                "description": metadata.get("description", ""),
                            }
                        )
                        # base_scoreì™€ developer_type ì €ì¥ (ë‚˜ì¤‘ì— SkillMatch ìƒì„± ì‹œ ì‚¬ìš©)
                        skill_metadata_map[key] = {
                            "base_score": int(metadata.get("base_score", 0)),
                            "developer_type": metadata.get("developer_type", "All"),
                        }

                # 2. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë™ì  ìƒì„± (system_prompt ê¸°ë°˜)
                user_prompt = self._build_user_prompt(
                    code=code.code,
                    file_path=code.file,
                    line_start=code.line_start,
                    line_end=code.line_end,
                    candidate_skills=candidate_skills,
                    relevance_threshold=config.relevance_threshold,
                )
                
                # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¤€ë¹„ (ë¡œê¹…ìš©)
                llm_input = {
                    "code": code.code,
                    "file_path": code.file,
                    "line_range": f"{code.line_start}-{code.line_end}",
                    "candidate_skills": candidate_skills,
                    "relevance_threshold": config.relevance_threshold,
                }
                
                llm_tracker.log_prompts(
                    template_name=f"code_batch_processor_code_{code_idx}",
                    variables=llm_input,
                    system_prompt=self.system_prompt,
                    user_prompt=user_prompt,
                )

                # Structured Output LLM í˜¸ì¶œ (SystemMessage + HumanMessage)
                messages = [
                    SystemMessage(content=self.system_prompt),
                    HumanMessage(content=user_prompt),
                ]
                analysis_result: SkillAnalysisOutput = await self.structured_llm.ainvoke(messages)
                
                # ì‘ë‹µ ë¡œê¹… (Structured Outputì´ë¯€ë¡œ ì´ë¯¸ ê²€ì¦ëœ Pydantic ê°ì²´)
                llm_tracker.log_response_stages(
                    raw=str(analysis_result),
                    parsed=analysis_result.model_dump() if hasattr(analysis_result, 'model_dump') else None,
                    validated=analysis_result,
                )
                
                # LLM ì¶”ì  ì •ë³´ ì„¤ì • (ë©”íƒ€ë°ì´í„°ìš©)
                llm_tracker.set_messages(messages)
                llm_tracker.set_response(analysis_result)

                # 3. relevance_threshold í•„í„°ë§ ë° SkillMatchItem â†’ SkillMatch ë³€í™˜
                # base_scoreì™€ developer_typeì„ ChromaDB ë©”íƒ€ë°ì´í„°ì—ì„œ ë¡œë“œ
                matched_skills = []
                for item in analysis_result.matched_skills:
                    if item.relevance_score >= config.relevance_threshold:
                        key = f"{item.skill_name}_{item.level}"
                        metadata = skill_metadata_map.get(key, {})
                        matched_skills.append(
                            SkillMatch(
                                skill_name=item.skill_name,
                                level=item.level,
                                category=item.category,
                                subcategory=item.subcategory,
                                relevance_score=item.relevance_score,
                                reasoning=item.reasoning,
                                base_score=metadata.get("base_score", 0),
                                # weighted_score, occurrence_countëŠ” ê¸°ë³¸ê°’ 0 ì‚¬ìš©
                            )
                        )

                # 4. ë¯¸ë“±ë¡ ìŠ¤í‚¬ ì •ë³´ ì¶”ê°€ (MissingSkillItem â†’ MissingSkillInfo ë³€í™˜)
                missing_skills = []
                for missing in analysis_result.missing_skills:
                    missing_skills.append(
                        MissingSkillInfo(
                            code_snippet=code.code,
                            file_path=code.file,
                            line_number=code.line_start,
                            suggested_skill_name=missing.suggested_skill_name,
                            suggested_level=missing.suggested_level,
                            suggested_category=missing.suggested_category,
                            suggested_subcategory=missing.suggested_subcategory,
                            description=missing.description,
                            evidence_examples=missing.evidence_examples,
                            # developer_typeì€ ê¸°ë³¸ê°’ "All"ì´ë¯€ë¡œ ìƒëµ ê°€ëŠ¥
                        )
                    )

                return {
                    "matched_skills": matched_skills,
                    "missing_skills": missing_skills,
                }
        else:
            # ë””ë²„ê¹… ë¹„í™œì„±í™” ì‹œ ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ
            # 1. ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ ìŠ¤í‚¬ í›„ë³´ ì¶”ì¶œ
            search_results = skill_collection.query(
                query_texts=[code.code],
                n_results=config.skill_candidate_count,
            )

            # í›„ë³´ ìŠ¤í‚¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (base_score, developer_type í¬í•¨)
            candidate_skills = []
            skill_metadata_map = {}  # skill_name_levelì„ í‚¤ë¡œ í•˜ëŠ” ë©”íƒ€ë°ì´í„° ë§µ
            if search_results and search_results["metadatas"]:
                for metadata in search_results["metadatas"][0]:
                    skill_name = metadata.get("skill_name", "")
                    level = metadata.get("level", "")
                    key = f"{skill_name}_{level}"
                    candidate_skills.append(
                        {
                            "skill_name": skill_name,
                            "level": level,
                            "category": metadata.get("category", ""),
                            "subcategory": metadata.get("subcategory", ""),
                            "description": metadata.get("description", ""),
                        }
                    )
                    # base_scoreì™€ developer_type ì €ì¥ (ë‚˜ì¤‘ì— SkillMatch ìƒì„± ì‹œ ì‚¬ìš©)
                    skill_metadata_map[key] = {
                        "base_score": int(metadata.get("base_score", 0)),
                        "developer_type": metadata.get("developer_type", "All"),
                    }

            # 2. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë™ì  ìƒì„± (system_prompt ê¸°ë°˜)
            user_prompt = self._build_user_prompt(
                code=code.code,
                file_path=code.file,
                line_start=code.line_start,
                line_end=code.line_end,
                candidate_skills=candidate_skills,
                relevance_threshold=config.relevance_threshold,
            )

            # Structured Output LLM í˜¸ì¶œ (SystemMessage + HumanMessage)
            messages = [
                SystemMessage(content=self.system_prompt),
                HumanMessage(content=user_prompt),
            ]
            analysis_result: SkillAnalysisOutput = await self.structured_llm.ainvoke(messages)

            # 3. relevance_threshold í•„í„°ë§ ë° SkillMatchItem â†’ SkillMatch ë³€í™˜
            # base_scoreì™€ developer_typeì„ ChromaDB ë©”íƒ€ë°ì´í„°ì—ì„œ ë¡œë“œ
            matched_skills = []
            for item in analysis_result.matched_skills:
                if item.relevance_score >= config.relevance_threshold:
                    key = f"{item.skill_name}_{item.level}"
                    metadata = skill_metadata_map.get(key, {})
                    matched_skills.append(
                        SkillMatch(
                            skill_name=item.skill_name,
                            level=item.level,
                            category=item.category,
                            subcategory=item.subcategory,
                            relevance_score=item.relevance_score,
                            reasoning=item.reasoning,
                            base_score=metadata.get("base_score", 0),
                            # weighted_score, occurrence_countëŠ” ê¸°ë³¸ê°’ 0 ì‚¬ìš©
                        )
                    )

            # 4. ë¯¸ë“±ë¡ ìŠ¤í‚¬ ì •ë³´ ì¶”ê°€ (MissingSkillItem â†’ MissingSkillInfo ë³€í™˜)
            missing_skills = []
            for missing in analysis_result.missing_skills:
                missing_skills.append(
                    MissingSkillInfo(
                        code_snippet=code.code,
                        file_path=code.file,
                        line_number=code.line_start,
                        suggested_skill_name=missing.suggested_skill_name,
                        suggested_level=missing.suggested_level,
                        suggested_category=missing.suggested_category,
                        suggested_subcategory=missing.suggested_subcategory,
                        description=missing.description,
                        evidence_examples=missing.evidence_examples,
                        # developer_typeì€ ê¸°ë³¸ê°’ "All"ì´ë¯€ë¡œ ìƒëµ ê°€ëŠ¥
                    )
                )

            return {
                "matched_skills": matched_skills,
                "missing_skills": missing_skills,
            }
