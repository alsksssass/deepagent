"""UserSkillProfiler Agent - ê°œë°œì ìŠ¤í‚¬ í”„ë¡œíŒŒì¼ë§"""

import logging
import asyncio
import json
from typing import Any, Tuple, List
from collections import defaultdict
import chromadb

from pathlib import Path
from shared.tools.skill_tools import search_skills_by_code, calculate_category_coverage
from shared.tools.chromadb_tools import get_chroma_client
from shared.utils.prompt_loader import PromptLoader
from shared.utils.agent_logging import log_agent_execution
from shared.utils.agent_debug_logger import AgentDebugLogger
from shared.utils.skill_level_calculator import SkillLevelCalculator
from shared.storage import ResultStore

from .schemas import (
    UserSkillProfilerContext,
    UserSkillProfilerResponse,
    SkillProfileData,
    HybridConfig,
    SkillMatch,
    MissingSkillInfo,
    SkillAnalysisOutput,
)

logger = logging.getLogger(__name__)


class UserSkillProfilerAgent:
    """
    ì‚¬ìš©ìì˜ ì»¤ë°‹ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ Skill Profileì„ ìƒì„±í•˜ëŠ” ì—ì´ì „íŠ¸

    Level 2 ë³‘ë ¬ ì²˜ë¦¬:
    - ì½”ë“œ ì„ë² ë”© ê²€ìƒ‰ (ChromaDB code collection)
    - ìŠ¤í‚¬ ë§¤ì¹­ (ChromaDB skill_charts collection)
    - ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì§‘ê³„

    Dynamic 2-Tier ì•„í‚¤í…ì²˜:
    - Level 0 (Coordinator): UserSkillProfiler - ë°°ì¹˜ ìƒì„± ë° ê²°ê³¼ ì§‘ê³„
    - Level 1 (Worker): CodeBatchProcessorAgent - 10ê°œ ì½”ë“œ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬
    - SmartBatcher: ê· ë“± ë¶€í•˜ ë¶„ì‚° (ìµœëŒ€ ì°¨ì´ â‰¤ 1)
    - ê³„ì¸µì  ì¬ì‹œë„: Level 1 (3íšŒ) + Level 0 (ì‹¤íŒ¨ ë°°ì¹˜ ì¬ì²˜ë¦¬)
    """

    def __init__(self, task_uuid: str = None):
        # Task UUID ì €ì¥ (CodeBatchProcessorì—ê²Œ ì „ë‹¬)
        self.task_uuid = task_uuid

        # PromptLoaderë¡œ LLM ë¡œë“œ
        self.llm = PromptLoader.get_llm("user_skill_profiler")
        model_id = PromptLoader.get_model("user_skill_profiler")

        # í•˜ì´ë¸Œë¦¬ë“œ: ìŠ¤í‚¤ë§ˆ ìë™ ì£¼ì…
        self.prompts = PromptLoader.load_with_schema(
            "user_skill_profiler",
            response_schema_class=SkillAnalysisOutput
        )

        # Tool binding
        from shared.tools.skill_tools import (
            search_skills_by_code,
            get_skill_by_name,
            get_skills_by_category,
        )

        self.llm_with_tools = self.llm.bind_tools(
            [
                search_skills_by_code,
                get_skill_by_name,
                get_skills_by_category,
            ]
        )

        # Structured Output: Force Pydantic schema compliance for 95% success rate
        self.llm_structured = self.llm_with_tools.with_structured_output(
            SkillAnalysisOutput, method="function_calling"
        )

        logger.info(f"âœ… UserSkillProfiler: LLM with structured output ì´ˆê¸°í™” ì™„ë£Œ - {model_id}")

    @log_agent_execution(agent_name="user_skill_profiler")
    async def run(self, context: UserSkillProfilerContext) -> UserSkillProfilerResponse:
        """
        ì‚¬ìš©ì ìŠ¤í‚¬ í”„ë¡œíŒŒì¼ ìƒì„±

        Args:
            context: UserSkillProfilerContext

        Returns:
            UserSkillProfilerResponse
        """
        user = context.user
        task_uuid = context.task_uuid
        persist_dir = context.persist_dir  # ìŠ¤í‚¬ ì°¨íŠ¸ìš©
        code_persist_dir = context.code_persist_dir or persist_dir  # ì½”ë“œ ì»¬ë ‰ì…˜ìš©

        # task_uuidë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ì €ì¥ (CodeBatchProcessorì—ê²Œ ì „ë‹¬ìš©)
        if not self.task_uuid:
            self.task_uuid = task_uuid

        logger.info(f"ğŸ¯ UserSkillProfiler: {user} ìŠ¤í‚¬ í”„ë¡œíŒŒì¼ ìƒì„± ì‹œì‘")

        # ResultStore ì´ˆê¸°í™” (ë°°ì¹˜ ê²°ê³¼ ì €ì¥ìš©)
        base_path = Path(context.result_store_path).parent if context.result_store_path else Path(f"./data/analyze/{task_uuid}")
        result_store = ResultStore(task_uuid, base_path)
        
        # ì¤‘ê°„ ë‹¨ê³„ ë¡œê¹…ì„ ìœ„í•´ logger ê°€ì ¸ì˜¤ê¸°
        debug_logger = AgentDebugLogger.get_logger(task_uuid, base_path, "user_skill_profiler")

        # í™˜ê²½ë³€ìˆ˜ì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • ë¡œë“œ
        if context.enable_hybrid and context.hybrid_config is None:
            context.hybrid_config = HybridConfig.from_env()
            logger.info(
                f"âš™ï¸ í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • ë¡œë“œ: "
                f"concurrent={context.hybrid_config.llm_max_concurrent}, "
                f"batch={context.hybrid_config.llm_batch_size}, "
                f"candidates={context.hybrid_config.skill_candidate_count}"
            )

        try:
            # Level 2-1: ìœ ì € ì½”ë“œ ìˆ˜ì§‘ (ChromaDB code collection)
            user_code_samples = await self._collect_user_code(task_uuid, code_persist_dir)
            
            # ì¤‘ê°„ ë‹¨ê³„ ë¡œê¹…
            debug_logger.log_intermediate("code_collection", {
                "sample_count": len(user_code_samples) if user_code_samples else 0,
                "samples_preview": user_code_samples[:3] if user_code_samples else []  # ìƒ˜í”Œë§Œ
            })

            if not user_code_samples:
                logger.warning(f"âš ï¸ {user}: ì½”ë“œ ìƒ˜í”Œ ì—†ìŒ")
                response = UserSkillProfilerResponse(
                    status="failed",
                    user=user,
                    skill_profile=SkillProfileData(),
                    error="No code samples found",
                )
                debug_logger.log_response(response)
                return response

            # Level 2-2: ì½”ë“œ â†’ ìŠ¤í‚¬ ë§¤ì¹­
            detected_skills = []
            missing_skills = []

            if context.enable_hybrid:
                # í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­: ì„ë² ë”© í›„ë³´ + LLM íŒë‹¨
                detected_skills, missing_skills = await self._hybrid_match_parallel(
                    user_code_samples,
                    persist_dir,  # ìŠ¤í‚¬ ì°¨íŠ¸ìš©
                    context.hybrid_config,
                    result_store=result_store,
                )
            else:
                # ê¸°ì¡´ ì„ë² ë”© ë§¤ì¹­
                detected_skills = await self._match_skills_parallel(user_code_samples, persist_dir)  # ìŠ¤í‚¬ ì°¨íŠ¸ìš©

            # Level 2-2.5: ë¯¸ë“±ë¡ ìŠ¤í‚¬ ë¡œê¹…
            missing_log_path = None
            if missing_skills and context.result_store_path:
                from .missing_skills_logger import MissingSkillsLogger

                logger_instance = MissingSkillsLogger(context.result_store_path)
                missing_log_path = logger_instance.save_missing_skills(
                    missing_skills,
                    task_uuid,
                )
                logger.info(f"ğŸ“ ë¯¸ë“±ë¡ ìŠ¤í‚¬ {len(missing_skills)}ê°œ ë¡œê·¸ ì €ì¥: {missing_log_path}")

            # Level 2-3: ìŠ¤í‚¬ í†µê³„ ì§‘ê³„
            skill_profile_data = await self._aggregate_skill_profile(detected_skills, persist_dir)
            
            # ì¤‘ê°„ ë‹¨ê³„ ë¡œê¹…
            debug_logger.log_intermediate("skill_matching", {
                "detected_skills_count": len(detected_skills),
                "missing_skills_count": len(missing_skills),
            })
            debug_logger.log_intermediate("aggregation", {
                "total_skills": skill_profile_data.get("total_skills", 0),
                "total_coverage": skill_profile_data.get("total_coverage", 0),
            })

            # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            skill_profile = SkillProfileData(**skill_profile_data)

            logger.info(
                f"âœ… UserSkillProfiler: {user} - "
                f"{skill_profile.total_skills}ê°œ ìŠ¤í‚¬ í”„ë¡œíŒŒì¼ ì™„ë£Œ "
                f"(ë¯¸ë“±ë¡: {len(missing_skills)}ê°œ)"
            )

            response = UserSkillProfilerResponse(
                status="success",
                user=user,
                skill_profile=skill_profile,
                missing_skills_log_path=missing_log_path,
                hybrid_stats=(
                    {
                        "total_analyzed": len(user_code_samples),
                        "skills_found": len(detected_skills),
                        "missing_skills": len(missing_skills),
                        "hybrid_enabled": context.enable_hybrid,
                    }
                    if context.enable_hybrid
                    else None
                ),
            )
            
            # ìµœì¢… ì‘ë‹µ ë¡œê¹… (ë°ì½”ë ˆì´í„°ê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ë§Œ, ì¤‘ê°„ ë‹¨ê³„ ë¡œê¹…ì„ ìœ„í•´ ìœ ì§€)
            debug_logger.log_response(response)
            return response

        except Exception as e:
            logger.error(f"âŒ UserSkillProfiler: {e}", exc_info=True)
            error_response = UserSkillProfilerResponse(
                status="failed",
                user=user,
                skill_profile=SkillProfileData(),
                error=str(e),
            )
            debug_logger.log_response(error_response)
            return error_response

    async def _collect_user_code(self, task_uuid: str, persist_dir: str) -> list[dict[str, Any]]:
        """
        ChromaDB code collectionì—ì„œ ìœ ì € ì½”ë“œ ìƒ˜í”Œ ìˆ˜ì§‘

        Returns:
            ì½”ë“œ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ (ê° ìƒ˜í”Œì€ {"code": str, "file": str, "line_start": int, "line_end": int})
        """
        try:
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ (ì‹±ê¸€í†¤ ì‚¬ìš©)
            client = get_chroma_client(persist_dir)

            collection_name = f"code_{task_uuid}"
            collection = client.get_collection(name=collection_name)

            # ì „ì²´ ì½”ë“œì™€ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            results = collection.get(include=["documents", "metadatas"])
            documents = results["documents"]
            metadatas = results["metadatas"]

            # ì½”ë“œ + ë©”íƒ€ë°ì´í„° ê²°í•©
            code_samples = []
            for i, doc in enumerate(documents):
                metadata = metadatas[i] if i < len(metadatas) else {}
                code_samples.append(
                    {
                        "code": doc,
                        "file": metadata.get("file", "unknown"),
                        "line_start": metadata.get("line_start", 0),
                        "line_end": metadata.get("line_end", 0),
                    }
                )

            logger.info(f"ğŸ“‚ {len(code_samples)}ê°œ ì½”ë“œ ìƒ˜í”Œ ìˆ˜ì§‘ (íŒŒì¼ ê²½ë¡œ/ë¼ì¸ ë²ˆí˜¸ í¬í•¨)")
            return code_samples

        except Exception as e:
            logger.error(f"âŒ ì½”ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def _match_skills_parallel(
        self, code_samples: list[dict[str, Any]], persist_dir: str
    ) -> list[dict[str, Any]]:
        """
        ì½”ë“œ ìƒ˜í”Œë“¤ì„ ë³‘ë ¬ë¡œ ìŠ¤í‚¬ ë§¤ì¹­

        Returns:
            ë§¤ì¹­ëœ ìŠ¤í‚¬ ë¦¬ìŠ¤íŠ¸
        """
        # ë°°ì¹˜ í¬ê¸° (ë„ˆë¬´ ë§ìœ¼ë©´ ë³‘ë ¬ ì²˜ë¦¬ ë¶€ë‹´)
        batch_size = 10
        all_skills = []

        for i in range(0, len(code_samples), batch_size):
            batch = code_samples[i : i + batch_size]

            # ë³‘ë ¬ ìŠ¤í‚¬ ê²€ìƒ‰
            batch_results = await asyncio.gather(
                *[
                    search_skills_by_code.ainvoke(
                        {
                            "code_snippet": sample["code"],
                            "n_results": 5,  # ê° ì½”ë“œë‹¹ ìƒìœ„ 5ê°œ ìŠ¤í‚¬
                            "persist_dir": persist_dir,
                        }
                    )
                    for sample in batch
                ]
            )

            # ê²°ê³¼ ë³‘í•©
            for skills in batch_results:
                all_skills.extend(skills)

            logger.info(f"ğŸ” {i + len(batch)}/{len(code_samples)} ì½”ë“œ ìŠ¤í‚¬ ë§¤ì¹­ ì™„ë£Œ")

        # ì¤‘ë³µ ì œê±° ë° ì‹ ë¢°ë„ í•„í„°ë§
        unique_skills = self._deduplicate_skills(all_skills)

        logger.info(f"âœ… ì´ {len(unique_skills)}ê°œ ê³ ìœ  ìŠ¤í‚¬ ë°œê²¬")
        return unique_skills

    async def _hybrid_match_parallel(
        self,
        code_samples: List[dict[str, Any]],
        persist_dir: str,
        config: HybridConfig,
        result_store: ResultStore = None,
    ) -> Tuple[List[dict[str, Any]], List[MissingSkillInfo]]:
        """
        Dynamic 2-Tier í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­

        Level 0 (Coordinator): ë°°ì¹˜ ìƒì„± ë° ê²°ê³¼ ì§‘ê³„
        Level 1 (Worker): CodeBatchProcessorAgentë¡œ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬

        Args:
            code_samples: ì½”ë“œ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ (ê° ìƒ˜í”Œì€ {"code": str, "file": str, "line_start": int, "line_end": int})
            persist_dir: ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬
            config: í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì •

        Returns:
            (ë§¤ì¹­ëœ ìŠ¤í‚¬ ë¦¬ìŠ¤íŠ¸, ë¯¸ë“±ë¡ ìŠ¤í‚¬ ë¦¬ìŠ¤íŠ¸)

        Performance:
            - ê¸°ì¡´: 95ì´ˆ (88ê°œ ì½”ë“œ, ìˆœì°¨ì  ë°°ì¹˜ ì²˜ë¦¬)
            - ê°œì„ : ~10ì´ˆ (88ê°œ ì½”ë“œ, 9ê°œ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬)
            - í–¥ìƒ: 90% (9.5ë°° ë¹ ë¦„)
        """
        total_codes = len(code_samples)
        logger.info(f"ğŸš€ Dynamic 2-Tier ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {total_codes}ê°œ ì½”ë“œ")

        # Lazy import to avoid circular dependency
        from .sub_agents.code_batch_processor import (
            CodeBatchProcessorAgent,
            CodeBatchContext,
            CodeBatchResponse,
            SmartBatcher,
        )

        # 1. SmartBatcherë¡œ ê· ë“± ë°°ì¹˜ ìƒì„±
        batches = SmartBatcher.create_balanced_batches(
            code_samples=code_samples,
            max_agents=config.llm_max_concurrent,
            target_batch_size=config.llm_batch_size,
        )

        num_batches = len(batches)
        logger.info(
            f"ğŸ“¦ {num_batches}ê°œ ë°°ì¹˜ ìƒì„± ì™„ë£Œ "
            f"(í¬ê¸°: {[len(b) for b in batches]})"
        )

        # 2. ë°°ì¹˜ë³„ CodeBatchProcessorAgent ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
        async def process_batch(batch_id: int, batch_codes: List):
            """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬ (Level 1 Worker í˜¸ì¶œ)"""
            try:
                # CodeBatchProcessorAgent ìƒì„±
                processor = CodeBatchProcessorAgent(task_uuid=self.task_uuid)

                # Context ìƒì„±
                batch_context = CodeBatchContext(
                    batch_id=batch_id,
                    codes=batch_codes,
                    persist_dir=persist_dir,
                    hybrid_config=config,
                    task_uuid=self.task_uuid,
                )

                # Level 1 Worker ì‹¤í–‰
                response = await processor.run(batch_context)

                logger.info(
                    f"  ë°°ì¹˜ {batch_id}: {response.status} "
                    f"(ì„±ê³µë¥  {response.success_rate:.1%}, "
                    f"{response.processing_time:.2f}s)"
                )

                # ë°°ì¹˜ ê²°ê³¼ ì €ì¥ (ResultStore ì‚¬ìš©)
                if result_store:
                    try:
                        result_store.save_batched_result(
                            agent_name="code_batch_processor",
                            batch_id=batch_id,
                            result=response,
                        )
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë°°ì¹˜ {batch_id} ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

                return response

            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ë¹ˆ ì‘ë‹µ ë°˜í™˜ (Level 0 ì¬ì‹œë„ ëŒ€ìƒ)
                return CodeBatchResponse(
                    batch_id=batch_id,
                    matched_skills=[],
                    missing_skills=[],
                    success_rate=0.0,
                    failed_codes=batch_codes,
                    processing_time=0.0,
                    retry_count=0,
                    status="failed",
                    message=f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}",
                )

        # 3. ëª¨ë“  ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰
        logger.info(f"âš¡ {num_batches}ê°œ ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘...")
        batch_responses = await asyncio.gather(
            *[process_batch(i, batch) for i, batch in enumerate(batches)]
        )

        # 4. Level 0 ì¬ì‹œë„: ì„±ê³µë¥  < 80%ì¸ ë°°ì¹˜ë§Œ ì¬ì²˜ë¦¬
        retry_batches = []
        for response in batch_responses:
            if response.success_rate < 0.8 and response.failed_codes:
                retry_batches.append(response)

        if retry_batches:
            logger.warning(
                f"âš ï¸ {len(retry_batches)}ê°œ ë°°ì¹˜ ì¬ì‹œë„ í•„ìš” "
                f"(ì„±ê³µë¥  < 80%)"
            )

            retry_responses = await asyncio.gather(
                *[
                    process_batch(
                        resp.batch_id,
                        resp.failed_codes,
                    )
                    for resp in retry_batches
                ]
            )

            # ì¬ì‹œë„ ê²°ê³¼ë¡œ ì›ë³¸ ì‘ë‹µ êµì²´ ë° ì €ì¥
            for i, orig_resp in enumerate(batch_responses):
                for retry_resp in retry_responses:
                    if orig_resp.batch_id == retry_resp.batch_id:
                        batch_responses[i] = retry_resp
                        # ì¬ì‹œë„ ê²°ê³¼ë„ ì €ì¥
                        if result_store:
                            try:
                                result_store.save_batched_result(
                                    agent_name="code_batch_processor",
                                    batch_id=retry_resp.batch_id,
                                    result=retry_resp,
                                )
                            except Exception as e:
                                logger.warning(f"âš ï¸ ì¬ì‹œë„ ë°°ì¹˜ {retry_resp.batch_id} ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                        break

        # 5. ê²°ê³¼ ì§‘ê³„
        all_matched_skills = []
        all_missing_skills = []
        total_failed = 0

        for response in batch_responses:
            # SkillMatchë¥¼ dictë¡œ ë³€í™˜ (base_score í¬í•¨)
            for skill in response.matched_skills:
                all_matched_skills.append(
                    {
                        "skill_name": skill.skill_name,
                        "level": skill.level,
                        "category": skill.category,
                        "subcategory": skill.subcategory,
                        "relevance_score": skill.relevance_score,
                        "reasoning": skill.reasoning,
                        "base_score": skill.base_score,  # âœ… base_score ì¶”ê°€
                    }
                )

            # MissingSkillInfoëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
            all_missing_skills.extend(response.missing_skills)

            # ì‹¤íŒ¨ ì½”ë“œ ì¹´ìš´íŠ¸
            total_failed += len(response.failed_codes)

        # 6. ì¤‘ë³µ ì œê±°
        unique_matched = self._deduplicate_skills(all_matched_skills)

        # 7. ìµœì¢… í†µê³„
        final_success_rate = (total_codes - total_failed) / total_codes
        logger.info(
            f"âœ… Dynamic 2-Tier ë§¤ì¹­ ì™„ë£Œ: "
            f"{len(unique_matched)}ê°œ ìŠ¤í‚¬, {len(all_missing_skills)}ê°œ ë¯¸ë“±ë¡, "
            f"ì„±ê³µë¥  {final_success_rate:.1%}"
        )

        return unique_matched, all_missing_skills

    def _deduplicate_skills(self, skills: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """
        ì¤‘ë³µ ìŠ¤í‚¬ ì œê±° ë° ì‹ ë¢°ë„ ì§‘ê³„

        ë™ì¼ ìŠ¤í‚¬ì´ ì—¬ëŸ¬ ë²ˆ ë§¤ì¹­ë˜ë©´ í‰ê·  relevance_score ì‚¬ìš©
        """
        skill_dict = defaultdict(list)

        for skill in skills:
            key = f"{skill['skill_name']}_{skill['level']}"
            skill_dict[key].append(skill)

        unique_skills = []
        for key, skill_list in skill_dict.items():
            # í‰ê·  relevance_score
            avg_score = sum(s["relevance_score"] for s in skill_list) / len(skill_list)

            # ì‹ ë¢°ë„ í•„í„°ë§ (0.3 ì´ìƒë§Œ)
            if avg_score >= 0.3:
                skill = skill_list[0].copy()
                skill["relevance_score"] = round(avg_score, 3)
                skill["occurrence_count"] = len(skill_list)
                # base_scoreëŠ” ì²« ë²ˆì§¸ ê°’ ì‚¬ìš© (ì¤‘ë³µ ì œê±° ì‹œ ë™ì¼ ìŠ¤í‚¬ì´ë¯€ë¡œ ê°™ìŒ)
                skill["base_score"] = skill.get("base_score", 0)
                unique_skills.append(skill)

        # relevance_score ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        unique_skills.sort(key=lambda x: x["relevance_score"], reverse=True)

        return unique_skills

    async def _aggregate_skill_profile(
        self, skills: list[dict[str, Any]], persist_dir: str
    ) -> dict[str, Any]:
        """
        ìŠ¤í‚¬ í”„ë¡œíŒŒì¼ ì§‘ê³„

        Returns:
            {
                "total_skills": int,
                "skills_by_category": {...},
                "skills_by_level": {...},
                "category_coverage": {...},
                "top_skills": [...],
            }
        """
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        skills_by_category = defaultdict(list)
        for skill in skills:
            skills_by_category[skill["category"]].append(skill)

        # ë ˆë²¨ë³„ ë¶„ë¥˜
        skills_by_level = defaultdict(list)
        for skill in skills:
            skills_by_level[skill["level"]].append(skill)

        # ì¹´í…Œê³ ë¦¬ë³„ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
        coverage = await calculate_category_coverage.ainvoke(
            {"user_skills": skills, "persist_dir": persist_dir}
        )

        # ìƒìœ„ ìŠ¤í‚¬ (relevance_score ê¸°ì¤€ Top 10)
        top_skills = skills[:10]

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for cat, cat_skills in skills_by_category.items():
            category_stats[cat] = {
                "count": len(cat_skills),
                "levels": {
                    "Basic": len([s for s in cat_skills if s["level"] == "Basic"]),
                    "Intermediate": len([s for s in cat_skills if s["level"] == "Intermediate"]),
                    "Advanced": len([s for s in cat_skills if s["level"] == "Advanced"]),
                },
                "avg_score": round(
                    sum(s["relevance_score"] for s in cat_skills) / len(cat_skills), 2
                ),
            }

        # category_coverage ë‚´ë¶€ì˜ percentageë¥¼ intë¡œ ë³€í™˜ (0.0-100.0 â†’ 0-100)
        category_coverage_converted = {}
        for cat, cat_data in coverage["category_coverage"].items():
            category_coverage_converted[cat] = {
                "count": cat_data["count"],
                "total": cat_data["total"],
                "percentage": int(cat_data["percentage"]),  # float â†’ int ë³€í™˜
            }

        # ë ˆë²¨ë§ ì‹œìŠ¤í…œ ê³„ì‚°
        total_experience = SkillLevelCalculator.calculate_total_experience(skills)
        level_info = SkillLevelCalculator.calculate_level(total_experience)

        # ê°œë°œì íƒ€ì…ë³„ ë³´ìœ ìœ¨ ê³„ì‚°
        developer_type_coverage = await SkillLevelCalculator.calculate_developer_type_coverage(
            skills, persist_dir
        )
        developer_type_levels = SkillLevelCalculator.get_developer_type_levels(
            developer_type_coverage
        )

        logger.info(
            f"ğŸ“Š ë ˆë²¨ë§ ê³„ì‚° ì™„ë£Œ: {total_experience} EXP â†’ {level_info['level_name']} (Lv.{level_info['level']})"
        )
        logger.info(
            f"ğŸ“Š ê°œë°œì íƒ€ì…ë³„ ë³´ìœ ìœ¨: {len(developer_type_coverage)}ê°œ íƒ€ì…"
        )

        return {
            "total_skills": len(skills),
            "skills_by_category": category_stats,
            "skills_by_level": {
                "Basic": len(skills_by_level["Basic"]),
                "Intermediate": len(skills_by_level["Intermediate"]),
                "Advanced": len(skills_by_level["Advanced"]),
            },
            "category_coverage": category_coverage_converted,
            "total_coverage": int(
                coverage["total_coverage"]
            ),  # calculate_category_coverage()ê°€ ì´ë¯¸ ë°±ë¶„ìœ¨ë¡œ ë°˜í™˜ (6.5% â†’ 6)
            "top_skills": [
                {
                    "skill_name": s["skill_name"],
                    "level": s["level"],
                    "category": s["category"],
                    "relevance_score": s["relevance_score"],
                    "occurrence_count": s.get("occurrence_count", 1),
                }
                for s in top_skills
            ],
            # ë ˆë²¨ë§ ì‹œìŠ¤í…œ í•„ë“œ ì¶”ê°€
            "total_experience": total_experience,
            "level": level_info,
            # ê°œë°œì íƒ€ì…ë³„ í†µê³„ í•„ë“œ ì¶”ê°€
            "developer_type_coverage": developer_type_coverage,
            "developer_type_levels": developer_type_levels,
        }
